{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=Workbook()\n",
    "tt_s=tt.active\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>381</td>\n",
       "      <td>Given the fast development of analysis techniq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>382</td>\n",
       "      <td>To increase trust in artificial intelligence s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>383</td>\n",
       "      <td>By introducing a small set of additional param...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>384</td>\n",
       "      <td>Language models keep track of complex informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>385</td>\n",
       "      <td>In the Transformer model, “self-attention” com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>386</td>\n",
       "      <td>With the growing popularity of deep-learning b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a                                                  b\n",
       "0  381  Given the fast development of analysis techniq...\n",
       "1  382  To increase trust in artificial intelligence s...\n",
       "2  383  By introducing a small set of additional param...\n",
       "3  384  Language models keep track of complex informat...\n",
       "4  385  In the Transformer model, “self-attention” com...\n",
       "5  386  With the growing popularity of deep-learning b..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_excel('t.xlsx')\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 420 entries, 0 to 419\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   a       420 non-null    int64 \n",
      " 1   b       420 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 6.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method', 'As a step in this direction we study the case of representations of phonology in neural network models of spoken language', 'We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences', 'We manipulate two factors that can affect the outcome of analysis', 'First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models', 'Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance', 'We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.'], ['To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions', 'In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations', 'We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations', 'Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks', 'Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions', 'Our framework shows that this model is capable of generating a significant number of inconsistent explanations.'], ['By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings)', 'The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge', 'However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself', 'Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT)', 'Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process', 'Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines', 'We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.'], ['Language models keep track of complex information about the preceding context – including, e.g., syntactic relations in a sentence', 'We investigate whether they also capture information beneficial for resolving pronominal anaphora in English', 'We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus', 'The Transformer outperforms the LSTM in all analyses', 'Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world', 'However, we find traces of the latter aspect, too.'], ['In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer', 'Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed', 'This makes attention weights unreliable as explanations probes', 'In this paper, we consider the problem of quantifying this flow of information through self-attention', 'We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens', 'We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.'], ['With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems', 'But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research', 'We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria', 'We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is “defined” by the community', 'We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted', 'Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful', 'We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.'], ['Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model’s predictions', 'Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model’s prediction', 'They can be considered a plausible explanation if they provide a human-understandable justification for the model’s predictions', 'In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model’s predictions', 'We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model’s predictions', 'Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model’s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions', 'To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse', 'We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model’s predictions (iii) correlate better with gradient-based attribution methods', 'Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model’s predictions', 'Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention'], ['Multi-task Learning methods have achieved great progress in text classification', 'However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications', 'To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption', 'The extensive experiments back up our theoretical analysis and validate the superiority of our proposals.'], ['This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology', 'Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation', 'The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.'], ['Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training', 'However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading', 'In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances)', 'We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills', 'We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.'], ['This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC)', 'The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC', 'For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods', 'Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM', 'The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks', 'Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.'], ['With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents', 'Most existing methods usually learn the representations of users and news from news contents for recommendation', 'However, they seldom consider high-order connectivity underlying the user-news interactions', 'Moreover, existing methods failed to disentangle a user’s latent preference factors which cause her clicks on different news', 'In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD', 'Our model can encode high-order relationships into user and news representations by information propagation along the graph', 'Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability', 'A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations', 'Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.'], ['In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case', 'We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story', 'We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework', 'Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants’ impacts in a complex case.'], ['The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online', 'Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection', 'While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language', 'The latter is, however, inextricably linked to abusive behaviour', 'In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other', 'Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.'], ['The key to effortless end-user programming is natural language', 'We examine how to teach intelligent systems new functions, expressed in natural language', 'As a first step, we collected 3168 samples of teaching efforts in plain English', 'Then we built fuSE, a novel system that translates English function descriptions into code', 'Our approach is three-tiered and each task is evaluated separately', 'We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT)', 'Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM)', 'Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods)', 'In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.'], ['Moderation is crucial to promoting healthy online discussions', 'Although several ‘toxicity’ detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently', 'We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems? We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title', 'We find that context can both amplify or mitigate the perceived toxicity of posts', 'Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context', 'Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware', 'This points to the need for larger datasets of comments annotated in context', 'We make our code and data publicly available.'], ['Abstract Meaning Representations (AMRs) capture sentence-level semantics structural representations to broad-coverage natural sentences', 'We investigate parsing AMR with explicit dependency structures and interpretable latent structures', 'We generate the latent soft structure without additional annotations, and fuse both dependency and latent structure via an extended graph neural networks', 'The fused structural information helps our experiments results to achieve the best reported results on both AMR 2.0 (77.5% Smatch F1 on LDC2017T10) and AMR 1.0 ((71.8% Smatch F1 on LDC2014T12).'], ['Answering natural language questions over tables is usually seen as a semantic parsing task', 'To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms', 'However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation', 'In this paper, we present TaPas, an approach to question answering over tables without generating logical forms', 'TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection', 'TaPas extends BERT’s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end', 'We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture', 'We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.'], ['In argumentation, people state premises to reason towards a conclusion', 'The conclusion conveys a stance towards some target, such as a concept or statement', 'Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons', 'However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation', 'We thus study the question to what extent an argument’s conclusion can be reconstructed from its premises', 'In particular, we argue here that a decisive step is to infer a conclusion’s target, and we hypothesize that this target is related to the premises’ targets', 'We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network', 'Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines', 'According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.'], ['Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality', 'Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities', 'Equally treating all modalities may encode too much useless information from less important modalities', 'In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT', 'The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images', 'Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.'], ['In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario', 'We, at first, manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes, both implicit and explicit', 'For multi-tasking, we propose two attention mechanisms, viz', 'Inter-segment Inter-modal Attention (Ie-Attention) and Intra-segment Inter-modal Attention (Ia-Attention)', 'The main motivation of Ie-Attention is to learn the relationship between the different segments of the sentence across the modalities', 'In contrast, Ia-Attention focuses within the same segment of the sentence across the modalities', 'Finally, representations from both the attentions are concatenated and shared across the five classes (i.e., sarcasm, implicit sentiment, explicit sentiment, implicit emotion, explicit emotion) for multi-tasking', 'Experimental results on the extended version of the MUStARD dataset show the efficacy of our proposed approach for sarcasm detection over the existing state-of-the-art systems', 'The evaluation also shows that the proposed multi-task framework yields better performance for the primary task, i.e., sarcasm detection, with the help of two secondary tasks, emotion and sentiment analysis.'], ['The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively', 'But these studies limit themselves to text', 'Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task', 'Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions', 'Hence, the effect of emotion too on automatic identification of DAs needs to be studied', 'In this work, we address the role of both multi-modality and emotion recognition (ER) in DAC', 'DAC and ER help each other by way of multi-task learning', 'One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets', 'To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions', 'We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.'], ['Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts', 'In this paper, we present the first computational study of parody', 'We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts', 'We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries', 'Our results show that political parody tweets can be predicted with an accuracy up to 90%', 'Finally, we identify the markers of parody through a linguistic analysis', 'Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.'], ['A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties', 'We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better', 'We propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias', 'We find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting.'], ['Social biases are encoded in word embeddings', 'This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications', 'Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods', 'We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning', 'However, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g', 'gender) than others (e.g', 'race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.'], ['In an era where generating content and publishing it is so easy, we are bombarded with information and are exposed to all kinds of claims, some of which do not always rank high on the truth scale', 'This paper suggests that the key to a longer-term, holistic, and systematic approach to navigating this information pollution is capturing the provenance of claims', 'To do that, we develop a formal definition of provenance graph for a given natural language claim, aiming to understand where the claim may come from and how it has evolved', 'To construct the graph, we model provenance inference, formulated mainly as an information extraction task and addressed via a textual entailment model', 'We evaluate our approach using two benchmark datasets, showing initial success in capturing the notion of provenance and its effectiveness on the application of claim verification.'], ['Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality', 'In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality', 'Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts', 'Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize', 'Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents', 'We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.'], ['State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions', 'This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs', 'But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress', 'We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP', 'This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected', 'We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions)', 'Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems', 'The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/'], ['In many settings it is important for one to be able to understand why a model made a particular prediction', 'In NLP this often entails extracting snippets of an input text ‘responsible for’ corresponding model output; when such a snippet comprises tokens that indeed informed the model’s prediction, it is a faithful explanation', 'In some settings, faithfulness may be critical to ensure transparency', 'Lei et al', '(2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules', 'However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning', 'We propose a simpler variant of this approach that provides faithful explanations by construction', 'In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict', 'An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex', 'In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to ‘end-to-end’ approaches, while being more general and easier to train', 'Code is available at https://github.com/successar/FRESH.'], ['Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets', 'In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation', 'Recently, Pampari et al', '(EMNLP’18) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes', 'In this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (CliniRC) task', 'From our qualitative analysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA questions are often answerable without using domain knowledge', 'From our quantitative experiments, surprising results include that (iii) using a small sampled subset (5%-20%), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert’s performance, and (v) BERT models do not beat the best performing base model', 'Following our analysis of the emrQA, we further explore two desired aspects of CliniRC systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts', 'We argue that both should be considered when creating future datasets.'], ['Transformer-based QA models use input-wide self-attention – i.e', 'across both the question and the input passage – at all layers, causing them to be slow and memory-intensive', 'It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers', 'We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers', 'This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically', 'Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset', 'We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy', 'We open source the code at https://github.com/StonyBrookNLP/deformer.'], ['Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges', 'Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG', 'Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer', 'KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA', 'Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn’t always readily available', 'In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction', 'Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far', 'We fill this gap in this paper and propose EmbedKGQA', 'EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs', 'EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods', 'Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA’s effectiveness over other state-of-the-art baselines.'], ['Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows', 'A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset', 'This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data', 'We propose an unsupervised approach to training QA models with generated pseudo-training data', 'We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships', 'Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving state-of-the-art performance on SQuAD for unsupervised QA.'], ['Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method', 'We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications', 'Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC', 'When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.'], ['A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions', 'We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants', 'Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages', 'However, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available', 'We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings', 'Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io.'], ['Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length', 'Evaluating the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance', 'DRSs are typically visualized as boxes which are not straightforward to process automatically', 'Counter transforms DRSs to clauses and measures clause overlap by searching for variable mappings between two DRSs', 'However, this metric is computationally costly (with respect to memory and CPU time) and does not scale with longer texts', 'We introduce Dscorer, an efficient new metric which converts box-style DRSs to graphs and then measures the overlap of n-grams', 'Experiments show that Dscorer computes accuracy scores that are correlated with Counter at a fraction of the time.'], ['We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software', 'We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering', 'We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.'], ['Correctly resolving textual mentions of people fundamentally entails making inferences about those people', 'Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders', 'To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems', 'Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.'], ['Motivated by human attention, computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data', 'While attention mechanisms are claimed to achieve interpretability, little is known about the actual relationships between machine and human attention', 'In this work, we conduct the first quantitative assessment of human versus computational attention mechanisms for the text classification task', 'To achieve this, we design and conduct a large-scale crowd-sourcing study to collect human attention maps that encode the parts of a text that humans focus on when conducting text classification', 'Based on this new resource of human attention dataset for text classification, YELP-HAT, collected on the publicly available YELP dataset, we perform a quantitative comparative analysis of machine attention maps created by deep learning models and human attention maps', 'Our analysis offers insights into the relationships between human versus machine attention maps along three dimensions: overlap in word selections, distribution over lexical categories, and context-dependency of sentiment polarity', 'Our findings open promising future research opportunities ranging from supervised attention to the design of human-centric attention-based explanations.'], ['The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually “know” about natural language', 'Probes are a natural way of assessing this', 'When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network’s learned representations', 'If the probe does well, the researcher may conclude that the representations encode knowledge related to the task', 'A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself', 'We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation', 'The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines', 'We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research—plus English—totalling eleven languages', 'Our implementation is available in https://github.com/rycolab/info-theoretic-probing.'], ['State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting', 'This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions', 'We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level', 'More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers', 'This approach does not rely on a shared vocabulary or joint training', 'However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD)', 'Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages', 'We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.'], ['This paper investigates contextual word representation models from the lens of similarity analysis', 'Given a collection of trained models, we measure the similarity of their internal representations and attention', 'Critically, these models come from vastly different architectures', 'We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation', 'The analysis reveals that models within the same family are more similar to one another, as may be expected', 'Surprisingly, different architectures have rather similar representations, but different individual neurons', 'We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.'], ['The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding', 'However, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content', 'This paper proposes a method to employ weak-supervision directly at the word sense level', 'Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses', 'Accordingly, we attain a lexical-semantic level language model, without the use of human annotation', 'SenseBERT achieves significantly improved lexical understanding, as we demonstrate by experimenting on SemEval Word Sense Disambiguation, and by attaining a state of the art result on the ‘Word in Context’ task.'], ['In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e', 'replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary', 'Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting', 'This makes it impossible to understand the ability of simplification models in more realistic settings', 'To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English', 'ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations', 'Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task', 'Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.'], ['Thanks to the wealth of high-quality annotated images available in popular repositories such as ImageNet, multimodal language-vision research is in full bloom', 'However, events, feelings and many other kinds of concepts which can be visually grounded are not well represented in current datasets', 'Nevertheless, we would expect a wide-coverage language understanding system to be able to classify images depicting recess and remorse, not just cats, dogs and bridges', 'We fill this gap by presenting BabelPic, a hand-labeled dataset built by cleaning the image-synset association found within the BabelNet Lexical Knowledge Base (LKB)', 'BabelPic explicitly targets non-concrete concepts, thus providing refreshing new data for the community', 'We also show that pre-trained language-vision systems can be used to further expand the resource by exploiting natural language knowledge available in the LKB', 'BabelPic is available for download at http://babelpic.org.'], ['Predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task, which usually treats labels as anonymous classes to predict', 'They ignore information that may be conveyed by the emotion labels themselves', 'We propose that the semantics of emotion labels can guide a model’s attention when representing the input story', 'Further, we observe that the emotions evoked by an event are often related: an event that evokes joy is unlikely to also evoke sadness', 'In this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label correlations both during training and inference', 'We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data', 'Our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-the-art on an emotion inference task.'], ['We propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game Minecraft', 'The dataset consists of 7K human utterances and their corresponding parses', 'Given proper world state, the parses can be interpreted and executed in game', 'We report the performance of baseline models, and analyze their successes and failures.'], ['Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address', 'They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws.In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019) to these cases', 'We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues', 'For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability', 'We demonstrate the efficacy of our approach across several dialogue tasks.'], ['Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems', 'Yet explaining their decisions is difficult despite recent work probing their internal representations', 'We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency', 'We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue', 'We find that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models', 'However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.'], ['LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks', 'Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number agreement in English', 'Lacking this understanding, the generality of LSTM performance on this task and their suitability for related tasks remains uncertain', 'Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults', 'We introduce *influence paths*, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network', 'The approach refines the notion of influence (the subject’s grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate or neuron-level paths', 'The set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors)', 'We exemplify the methodology on a widely-studied multi-layer LSTM language model, demonstrating its accounting for subject-verb number agreement', 'The results offer both a finer and a more complete view of an LSTM’s handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation.'], ['Contextualized representations (e.g', 'ELMo, BERT) have become the default pretrained representations for downstream NLP applications', 'In some settings, this transition has rendered their static embedding predecessors (e.g', 'Word2Vec, GloVe) obsolete', 'As a side-effect, we observe that older interpretability methods for static embeddings — while more diverse and mature than those available for their dynamic counterparts — are underutilized in studying newer contextualized representations', 'Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights', 'Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation', 'Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data', 'Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.'], ['Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing', 'In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders', 'We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks', 'Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions', 'Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy', 'Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender', 'Consequently, our results cast doubt on attention’s reliability as a tool for auditing algorithms in the context of fairness and accountability.'], ['We propose a general framework to study language emergence through signaling games with neural agents', 'Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge', 'We explore whether categorical perception effects follow and show that the messages are not compositional.'], ['Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect', 'To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words', 'We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability—but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance', 'We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.'], ['Videos convey rich information', 'Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip', 'Hence, it is important to develop automated models that can accurately extract such information from videos', 'Answering questions on videos is one of the tasks which can evaluate such AI abilities', 'In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions', 'Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions', 'Moreover, our model is also comprised of dual-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier', 'Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations', 'We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the state-of-the-art by a large margin (74.09% versus 70.52%)', 'We also present several word, object, and frame level visualization studies.'], ['By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models', 'We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time', 'Existing models for this setting sample new descriptions at test time and use those to classify images', 'Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language', 'LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.'], ['While much work on deep latent variable models of text uses continuous latent variables, discrete latent variables are interesting because they are more interpretable and typically more space efficient', 'We consider several approaches to learning discrete latent variable models for text in the case where exact marginalization over these variables is intractable', 'We compare the performance of the learned representations as features for low-resource document and sentence classification', 'Our best models outperform the previous best reported results with continuous representations in these low-resource settings, while learning significantly more compressed representations', 'Interestingly, we find that an amortized variant of Hard EM performs particularly well in the lowest-resource regimes.'], ['Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions', 'Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy', 'However, designing good constraints often relies on domain expertise', 'In this paper, we study the problem of learning such constraints', 'We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables', 'Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.'], ['Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations', 'We propose Conpono, an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences', 'Given an anchor sentence, our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus', 'On the discourse representation benchmark DiscoEval, our model improves over the previous state-of-the-art by up to 13% and on average 4% absolute across 7 tasks', 'Our model is the same size as BERT-Base, but outperforms the much larger BERT-Large model and other more recent approaches that incorporate discourse', 'We also show that Conpono yields gains of 2%-6% absolute even for tasks that do not explicitly evaluate discourse: textual entailment (RTE), common sense reasoning (COPA) and reading comprehension (ReCoRD).'], ['Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools', 'In the cooking domain, the web offers many, partially-overlapping, text and video recipes (i.e', 'procedures) that describe how to make the same dish (i.e', 'high-level task)', 'Aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions, providing commonsense insight into how real-world procedures are structured', 'Learning to align these different instruction sets is challenging because: a) different recipes vary in their order of instructions and use of ingredients; and b) video instructions can be noisy and tend to contain far more information than text instructions', 'To address these challenges, we use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish', 'We then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish', 'We release the Microsoft Research Multimodal Aligned Recipe Corpus containing ~150K pairwise alignments between recipes across 4262 dishes with rich commonsense information.'], ['We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure', 'We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set', 'Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses', 'The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.'], ['Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors', 'Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models', 'CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly', 'We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models', 'In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model', 'In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.'], ['There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet', 'For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users', 'Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences', 'In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types', 'We trained in-domain BERT representations (BERTOverflow) on 152 million sentences from StackOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT', 'We also present the SoftNER model which achieves an overall 79.10 F-1 score for code and named entity recognition on StackOverflow data', 'Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERT-based tagging model', 'Our code and data are available at: https://github.com/jeniyat/StackOverflowNER/'], ['We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue', 'We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences', 'We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks', 'Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE', 'Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings', 'DialogRE is available at https://dataset.org/dialogre/.'], ['Commonly adopted metrics for extractive summarization focus on lexical overlap at the token level', 'In this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries', 'Specifically, we treat each sentence in the reference summary as a facet, identify the sentences in the document that express the semantics of each facet as support sentences of the facet, and automatically evaluate extractive summarization methods by comparing the indices of extracted sentences and support sentences of all the facets in the reference summary', 'To facilitate this new evaluation setup, we construct an extractive version of the CNN/Daily Mail dataset and perform a thorough quantitative investigation, through which we demonstrate that facet-aware evaluation manifests better correlation with human judgment than ROUGE, enables fine-grained evaluation as well as comparative analysis, and reveals valuable insights of state-of-the-art summarization methods', 'Data can be found at https://github.com/morningmoni/FAR.'], ['Automated generation of conversational dialogue using modern neural architectures has made notable advances', 'However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem', 'We introduce a new strategy to address this problem, called Diversity-Informed Data Collection', 'Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpus-level statistics to determine which conversational participants to collect data from', 'Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation', 'This method is generalizable and can be used with other corpus-level metrics.'], ['We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines', 'The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers', 'Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects', 'In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date', 'We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.'], ['Automatic metrics are fundamental for the development and evaluation of machine translation systems', 'Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem', 'We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric’s efficacy', 'Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected', 'Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.'], ['Generating a readable summary that describes the functionality of a program is known as source code summarization', 'In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial', 'To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies', 'In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin', 'We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens’ position hinders, while relative encoding significantly improves the summarization performance', 'We have made our code publicly available to facilitate future research.'], ['Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input', 'Existing automatic evaluation metrics for summarization are largely insensitive to such errors', 'We propose QAGS (pronounced “kags”), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary', 'QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source', 'To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets', 'QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics', 'Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why', 'We believe QAGS is a promising tool in automatically generating usable and factually consistent text', 'Code for QAGS will be available at https://github.com/W4ngatang/qags.'], ['Recently BERT has been adopted for document encoding in state-of-the-art text summarization models', 'However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries', 'Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents', 'To address these issues, we present a discourse-aware neural summarization model - DiscoBert', 'DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity', 'To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks', 'Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.'], ['Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information', 'A good summary is characterized by language fluency and high information overlap with the source sentence', 'We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics', 'We search for a high-scoring summary by discrete optimization', 'Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores', 'Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length', 'Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets.'], ['We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides', 'This is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and generalization found in the summaries', 'We focus on extractive summarization, which requires the creation of a gold-standard set of extractive summaries', 'We present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also experiment with different alignment methods', 'Our experiments demonstrate significant improvement over prior alignment approaches for our task as shown through automatic metrics and a crowd-sourced pyramid analysis.'], ['Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e', 'unfaithful', 'Existing automatic metrics do not capture such mistakes effectively', 'We tackle the problem of evaluating faithfulness of a generated summary given its source document', 'We first collected human annotations of faithfulness for outputs from numerous models on two datasets', 'We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful', 'Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension', 'Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary', 'Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.'], ['Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient', 'We introduce a new evaluation metric which is based on fact-level content weighting, i.e', 'relating the facts of the document to the facts of the summary', 'We fol- low the assumption that a good summary will reflect all relevant facts, i.e', 'the ones present in the ground truth (human-generated refer- ence summary)', 'We confirm this hypothe- sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight- based metric of Hardy et al', '(2019).'], ['Current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles', 'We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), thus attracting more readers', 'With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates stylistic headlines by combining the summarization and reconstruction tasks into a multitasking framework', 'We also introduced a novel parameter sharing scheme to further disentangle the style from text', 'Through both automatic and human evaluation, we demonstrate that TitleStylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait', 'The attraction score of our model generated headlines outperforms the state-of-the-art summarization model by 9.68%, even outperforming human-written references.'], ['Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive', 'We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries', 'In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD', 'We propose the use of dual encoders—a sequential document encoder and a graph-structured encoder—to maintain the global context and local characteristics of entities, complementing each other', 'We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions', 'Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets', 'We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models', 'Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.'], ['Neural abstractive summarization models are able to generate summaries which have high overlap with human references', 'However, existing models are not optimized for factual correctness, a critical metric in real-world applications', 'In this work, we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module', 'We further propose a training strategy which optimizes a neural summarization model with a factual correctness reward via reinforcement learning', 'We apply the proposed method to the summarization of radiology reports, where factual correctness is a key requirement', 'On two separate datasets collected from hospitals, we show via both automatic and human evaluation that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system, producing radiology summaries that approach the quality of human-authored ones.'], ['This paper describes the Critical Role Dungeons and Dragons Dataset (CRD3) and related analyses', 'Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game', 'The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns', 'It also includes corresponding abstractive summaries collected from the Fandom wiki', 'The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction', 'For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail, and semantic ties to the previous dialogues', 'In addition, we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural ML approaches, and we provide an abstractive summarization benchmark and evaluation.'], ['This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint', 'It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary', 'A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries', 'When tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 R-1 points, and approaches results of competitive supervised methods', 'Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision.'], ['Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews', 'While the majority of previous work has focused on the extractive setting, i.e., selecting fragments from input reviews to produce a summary, we let the model generate novel sentences and hence produce abstractive summaries', 'Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs', 'Since such training data is expensive to acquire, we instead consider the unsupervised setting, in other words, we do not use any summaries in training', 'We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product, we should be able to control the “amount of novelty” going into the new review or, equivalently, vary the extent to which it deviates from the input', 'At test time, when generating summaries, we force the novelty to be minimal, and produce a text reflecting consensus opinions', 'We capture this intuition by defining a hierarchical variational autoencoder model', 'Both individual reviews and the products they correspond to are associated with stochastic latent codes, and the review generator (“decoder”) has direct access to the text of input reviews through the pointer-generator mechanism', 'Experiments on Amazon and Yelp datasets, show that setting at test time the review’s latent code to its mean, allows the model to produce fluent and coherent summaries reflecting common opinions.'], ['Human speakers have an extensive toolkit of ways to express themselves', 'In this paper, we engage with an idea largely absent from discussions of meaning in natural language understanding—namely, that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed', 'We first define this phenomenon more precisely, drawing on considerable prior work in theoretical cognitive semantics and psycholinguistics', 'We then survey some dimensions of construed meaning and show how insights from construal could inform theoretical and practical work in NLP.'], ['The success of the large neural language models on many NLP tasks is exciting', 'However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”', 'In this position paper, we argue that a system trained only on form has a priori no way to learn meaning', 'In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.'], ['We extracted information from the ACL Anthology (AA) and Google Scholar (GS) to examine trends in citations of NLP papers', 'We explore questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers from different areas of within NLP? etc', 'Notably, we show that only about 56% of the papers in AA are cited ten or more times', 'CL Journal has the most cited papers, but its citation dominance has lessened in recent years', 'On average, long papers get almost three times as many citations as short papers; and papers on sentiment classification, anaphora resolution, and entity recognition have the highest median citations', 'The analyses presented here, and the associated dataset of NLP papers mapped to citations, have a number of uses including: understanding how the field is growing and quantifying the impact of different types of papers.'], ['This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding', 'This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set', 'This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set', 'This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way', 'We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.'], ['Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain', 'In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork', 'Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods', 'In this paper, we introduce the history, the current state, and the future directions of research in LegalAI', 'We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI', 'We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions', 'You can find the implementation of our work from https://github.com/thunlp/CLAIM.'], ['While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task', 'However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task', 'To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations', 'We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer', 'We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best', 'We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution', 'However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks', 'We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.'], ['An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models)', 'However, these works have been conducted individually, without a unifying framework to organize efforts within the field', 'This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures', 'In this paper, we propose a unifying predictive bias framework for NLP', 'We summarize the NLP literature and suggest general mathematical definitions of predictive bias', 'We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias', 'Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.'], ['Pre-trained visually grounded language models such as ViLBERT, LXMERT, and UNITER have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear', 'In this work, we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions', 'Specifically, some heads can map entities to image regions, performing the task known as entity grounding', 'Some heads can even detect the syntactic relations between non-entity words and image regions, tracking, for example, associations between verbs and regions corresponding to their arguments', 'We denote this ability as syntactic grounding', 'We verify grounding both quantitatively and qualitatively, using Flickr30K Entities as a testbed.'], ['Throughout a conversation, participants make choices that can orient the flow of the interaction', 'Such choices are particularly salient in the consequential domain of crisis counseling, where a difficulty for counselors is balancing between two key objectives: advancing the conversation towards a resolution, and empathetically addressing the crisis situation', 'In this work, we develop an unsupervised methodology to quantify how counselors manage this balance', 'Our main intuition is that if an utterance can only receive a narrow range of appropriate replies, then its likely aim is to advance the conversation forwards, towards a target within that range', 'Likewise, an utterance that can only appropriately follow a narrow range of possible utterances is likely aimed backwards at addressing a specific situation within that range', 'By applying this intuition, we can map each utterance to a continuous orientation axis that captures the degree to which it is intended to direct the flow of the conversation forwards or backwards', 'This unsupervised method allows us to characterize counselor behaviors in a large dataset of crisis counseling conversations, where we show that known counseling strategies intuitively align with this axis', 'We also illustrate how our measure can be indicative of a conversation’s progress, as well as its effectiveness.'], ['Natural disasters (e.g., hurricanes) affect millions of people each year, causing widespread destruction in their wake', 'People have recently taken to social media websites (e.g., Twitter) to share their sentiments and feelings with the larger community', 'Consequently, these platforms have become instrumental in understanding and perceiving emotions at scale', 'In this paper, we introduce HurricaneEmo, an emotion dataset of 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria', 'We present a comprehensive study of fine-grained emotions and propose classification tasks to discriminate between coarse-grained emotion groups', 'Our best BERT model, even after task-guided pre-training which leverages unlabeled Twitter data, achieves only 68% accuracy (averaged across all groups)', 'HurricaneEmo serves not only as a challenging benchmark for models but also as a valuable resource for analyzing emotions in disaster-centric domains.'], ['Not all documents are equally important', 'Language processing is increasingly finding use as a supplement for questionnaires to assess psychological attributes of consenting individuals, but most approaches neglect to consider whether all documents of an individual are equally informative', 'In this paper, we present a novel model that uses message-level attention to learn the relative weight of users’ social media posts for assessing their five factor personality traits', 'We demonstrate that models with message-level attention outperform those with word-level attention, and ultimately yield state-of-the-art accuracies for all five traits by using both word and message attention in combination with past approaches (an average increase in Pearson r of 2.5%)', 'In addition, examination of the high-signal posts identified by our model provides insight into the relationship between language and personality, helping to inform future work.'], ['People vary in their ability to make accurate predictions about the future', 'Prior studies have shown that some individuals can predict the outcome of future events with consistently better accuracy', 'This leads to a natural question: what makes some forecasters better than others? In this paper we explore connections between the language people use to describe their predictions and their forecasting skill', 'Datasets from two different forecasting domains are explored: (1) geopolitical forecasts from Good Judgment Open, an online prediction forum and (2) a corpus of company earnings forecasts made by financial analysts', 'We present a number of linguistic metrics which are computed over text associated with people’s predictions about the future including: uncertainty, readability, and emotion', 'By studying linguistic factors associated with predictions, we are able to shed some light on the approach taken by skilled forecasters', 'Furthermore, we demonstrate that it is possible to accurately predict forecasting skill using a model that is based solely on language', 'This could potentially be useful for identifying accurate predictions or potentially skilled forecasters earlier.'], ['Many applications of computational social science aim to infer causal conclusions from non-experimental data', 'Such observational data often contains confounders, variables that influence both potential causes and potential effects', 'Unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text', 'For example, an individual’s entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders.Yet, methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent.This review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions', 'Despite increased attention on adjusting for confounding using text, there are still many open problems, which we highlight in this paper.'], ['Ideal point models analyze lawmakers’ votes to quantify their political positions, or ideal points', 'But votes are not the only way to express a political position', 'Lawmakers also give speeches, release press statements, and post tweets', 'In this paper, we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors', 'We demonstrate the TBIP with two types of politicized text data: U.S', 'Senate speeches and senator tweets', 'Though the model does not analyze their votes or political affiliations, the TBIP separates lawmakers by party, learns interpretable politicized topics, and infers ideal points close to the classical vote-based ideal points', 'One benefit of analyzing texts, as opposed to votes, is that the TBIP can estimate ideal points of anyone who authors political texts, including non-voting actors', 'To this end, we use it to study tweets from the 2020 Democratic presidential candidates', 'Using only the texts of their tweets, it identifies them along an interpretable progressive-to-moderate spectrum.'], ['While national politics often receive the spotlight, the overwhelming majority of legislation proposed, discussed, and enacted is done at the state level', 'Despite this fact, there is little awareness of the dynamics that lead to adopting these policies', 'In this paper, we take the first step towards a better understanding of these processes and the underlying dynamics that shape them, using data-driven methods', 'We build a new large-scale dataset, from multiple data sources, connecting state bills and legislator information, geographical information about their districts, and donations and donors’ information', 'We suggest a novel task, predicting the legislative body’s vote breakdown for a given bill, according to different criteria of interest, such as gender, rural-urban and ideological splits', 'Finally, we suggest a shared relational embedding model, representing the interactions between the text of the bill and the legislative context in which it is presented', 'Our experiments show that providing this context helps improve the prediction over strong text-based models.'], ['Understanding human preferences, along with cultural and social nuances, lives at the heart of natural language understanding', 'Concretely, we present a new task and corpus for learning alignments between machine and human preferences', 'Our newly introduced problem is concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations', 'Our problem is framed as a natural language inference task with crowd-sourced preference votes by human players, obtained from a gamified voting platform', 'We benchmark several state-of-the-art neural models, along with BERT and friends on this task', 'Our experimental results show that current state-of-the-art NLP models still leave much room for improvement.'], ['Understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event', 'To enable computational modeling of news structures, we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources', 'Next, we propose several document-level neural-network models to automatically construct news content structures', 'Finally, we demonstrate that incorporating system predicted news structures yields new state-of-the-art performance for event coreference resolution', 'The news documents we annotated are openly available and the annotations are publicly released for future research.'], ['Pragmatic inferences often subtly depend on the presence or absence of linguistic features', 'For example, the presence of a partitive construction (of the) increases the strength of a so-called scalar inference: listeners perceive the inference that Chris did not eat all of the cookies to be stronger after hearing “Chris ate some of the cookies” than after hearing the same utterance without a partitive, “Chris ate some cookies”', 'In this work, we explore to what extent neural network sentence encoders can learn to predict the strength of scalar inferences', 'We first show that an LSTM-based sentence encoder trained on an English dataset of human inference strength ratings is able to predict ratings with high accuracy (r = 0.78)', 'We then probe the model’s behavior using manually constructed minimal sentence pairs and corpus data', 'We first that the model inferred previously established associations between linguistic features and inference strength, suggesting that the model learns to use linguistic features to predict pragmatic inferences.'], ['Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations', 'However, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature', 'In this work, we highlight these inconsistencies and propose an improved evaluation protocol', 'Paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for PDTB 2.0', 'Furthermore, this work is the first to explore fine-grained relation classification on PDTB 3.0', 'We expect our work to serve as a point of comparison for future work, and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability.'], ['We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots', 'PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture', 'We empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance', 'To measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach', 'PeTra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation.'], ['Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively', 'We propose to better explore their interaction by solving both tasks together, while the previous work treats them separately', 'For zero pronoun resolution, we study this task in a more realistic setting, where no parsing trees or only automatic trees are available, while most previous work assumes gold trees', 'Experiments on two benchmarks show that joint modeling significantly outperforms our baseline that already beats the previous state of the arts.'], ['Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like “gay” or “black” are used in offensive or prejudiced ways', 'Such biases manifest in false positives when these identifiers are present, due to models’ inability to learn the contexts which constitute a hateful usage of identifiers', 'We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms', 'Then, we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves', 'Our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance.'], ['Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models', 'Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace', 'We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms', 'We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace', 'Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.'], ['We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process', 'We further find that these papers’ proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP', 'Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems', 'These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.'], ['Warning: this paper contains content that may be offensive or upsetting', 'Language has the power to reinforce stereotypes and project social biases onto others', 'At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people’s judgments about others', 'For example, given a statement that “we shouldn’t lower our standards to hire more women,” most listeners will infer the implicature intended by the speaker - that “women (candidates) are less qualified.” Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language', 'We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others', 'In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups', 'We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text', 'We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames', 'Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.'], ['Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models', 'In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained', 'In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis', 'Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability', 'We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.'], ['As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes', 'Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs', 'While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT', 'In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases', 'We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding', 'We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.'], ['Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs', 'A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing', 'However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods', 'In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem', 'The proposed protocol is robust to handle bias in the model, which can substantially affect the final results', 'We conduct extensive experiments and report performance of several existing methods using our protocol', 'The reproducible code has been made publicly available.'], ['A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy', 'However, these studies are based primarily on monolingual evidence from English', 'To investigate how these models’ ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models', 'CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop', 'We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT', 'Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses', 'On other constructions, agreement accuracy was generally higher in languages with richer morphology', 'Multilingual models generally underperformed monolingual models', 'Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages.'], ['Algorithmic approaches to interpreting machine learning models have proliferated in recent years', 'We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors', 'A model is simulatable when a person can predict its behavior on new inputs', 'Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method', 'Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests', 'We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are', 'Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains', 'We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.'], ['Modern deep learning models for NLP are notoriously opaque', 'This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights', 'Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text', 'While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning', 'In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers', 'Influence functions explain the decisions of a model by identifying influential training examples', 'Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work', 'We conduct a comparison between influence functions and common word-saliency methods on representative tasks', 'As suspected, we find that influence functions are particularly useful for natural language inference, a task in which ‘saliency maps’ may not have clear interpretation', 'Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.'], ['Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually', 'To better understand this overlap, we extend recent work on finding syntactic trees in neural networks’ internal representations to the multilingual setting', 'We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages', 'Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy', 'This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.'], ['Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness', 'In natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them', 'It poses challenges for humans to interpret an explanation and connect it to model prediction', 'In this work, we build hierarchical explanations by detecting feature interactions', 'Such explanations visualize how words and phrases are combined at different levels of the hierarchy, which can help users understand the decision-making of black-box models', 'The proposed method is evaluated with three neural text classifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic and human evaluations', 'Experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans.'], ['Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture', 'However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour', 'In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps', 'We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour', 'To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.'], ['Selecting input features of top relevance has become a popular method for building self-explaining models', 'In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction', 'Our approach employs optimal transport (OT) to find a minimal cost alignment between the inputs', 'However, directly applying OT often produces dense and therefore uninterpretable alignments', 'To overcome this limitation, we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity', 'Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations', 'We evaluate our model on the StackExchange, MultiNews, e-SNLI, and MultiRC datasets', 'Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models.'], ['Complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer', 'A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task', 'In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection', 'We find that these intermediate annotations can provide two-fold benefits', 'First, we observe that for any collection budget, spending a fraction of it on intermediate annotations results in improved model performance, for two complex compositional datasets: DROP and Quoref', 'Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.'], ['Answer retrieval is to find the most aligned answer from a large set of candidates given a question', 'Learning vector representations of questions/answers is the key factor', 'Question-answer alignment and question/answer semantics are two important signals for learning the representations', 'Existing methods learned semantic representations with dual encoders or dual variational auto-encoders', 'The semantic information was learned from language models or question-to-question (answer-to-answer) generative processes', 'However, the alignment and semantics were too separate to capture the aligned semantics between question and answer', 'In this work, we propose to cross variational auto-encoders by generating questions with aligned answers and generating answers with aligned questions', 'Experiments show that our method outperforms the state-of-the-art answer retrieval method on SQuAD.'], ['Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events', 'This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models', 'Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model', 'Improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (QA) tasks, including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension', 'In particular, our method significantly improves the performance of RoBERTa-based models by 1-5% across datasets', 'We advance state of the art by around 5-8% on WIQA and QuaRel and reduce consistency violations by 58% on HotpotQA', 'We further demonstrate that our approach can learn effectively from limited data.'], ['Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA)', 'In this paper we ask: Is textual diversity in QG beneficial for downstream QA? Using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search', 'We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA.'], ['We address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings', 'We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans)', 'We show that these assumptions interact, and that different configurations provide complementary benefits', 'We demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation', 'Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries.'], ['We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction', 'SCDE is a human created sentence cloze dataset, collected from public school English examinations', 'Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers', 'Experimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neighborhood', 'The blanks require joint solving and significantly impair each other’s context', 'Furthermore, through ablations, we show that the distractors are of high quality and make the task more challenging', 'Our experiments show that there is a significant performance gap between advanced models (72%) and humans (87%), encouraging future models to bridge this gap.'], ['To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering', 'Moreover, users often ask questions that diverge from the model’s training data, making errors more likely and thus abstention more critical', 'In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy', 'Abstention policies based solely on the model’s softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs', 'Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely', 'Crucially, the calibrator benefits from observing the model’s behavior on out-of-domain data, even if from a different domain than the test data', 'We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets', 'Our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model’s probabilities only answers 48% at 80% accuracy.'], ['Large transformer-based language models have been shown to be very effective in many classification tasks', 'However, their computational complexity prevents their use in applications requiring the classification of a large set of candidates', 'While previous works have investigated approaches to reduce model size, relatively little attention has been paid to techniques to improve batch throughput during inference', 'In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-based models into a cascade of rankers', 'Each ranker is used to prune a subset of candidates in a batch, thus dramatically increasing throughput at inference time', 'Partial encodings from the transformer model are shared among rerankers, providing further speed-up', 'When compared to a state-of-the-art transformer model, our approach reduces computation by 37% with almost no impact on accuracy, as measured on two English Question Answering datasets.'], ['We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue', 'First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts', 'Then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (QA)', 'Our approach is evaluated on the FriendsQA dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models, BERT and RoBERTa, respectively.'], ['Empirical research in Natural Language Processing (NLP) has adopted a narrow set of principles for assessing hypotheses, relying mainly on p-value computation, which suffers from several known issues', 'While alternative proposals have been well-debated and adopted in other fields, they remain rarely discussed or used within the NLP community', 'We address this gap by contrasting various hypothesis assessment techniques, especially those not commonly used in the field (such as evaluations based on Bayesian inference)', 'Since these statistical techniques differ in the hypotheses they can support, we argue that practitioners should first decide their target hypothesis before choosing an assessment method', 'This is crucial because common fallacies, misconceptions, and misinterpretation surrounding hypothesis assessment methods often stem from a discrepancy between what one would like to claim versus what the method used actually assesses', 'Our survey reveals that these issues are omnipresent in the NLP research community', 'As a step forward, we provide best practices and guidelines tailored to NLP research, as well as an easy-to-use package for Bayesian assessment of hypotheses, complementing existing tools.'], ['We present STARC (Structured Annotations for Reading Comprehension), a new annotation framework for assessing reading comprehension with multiple choice questions', 'Our framework introduces a principled structure for the answer choices and ties them to textual span annotations', 'The framework is implemented in OneStopQA, a new high-quality dataset for evaluation and analysis of reading comprehension in English', 'We use this dataset to demonstrate that STARC can be leveraged for a key new application for the development of SAT-like reading comprehension materials: automatic annotation quality probing via span ablation experiments', 'We further show that it enables in-depth analyses and comparisons between machine and human reading comprehension behavior, including error distributions and guessing ability', 'Our experiments also reveal that the standard multiple choice dataset in NLP, RACE, is limited in its ability to measure reading comprehension', '47% of its questions can be guessed by machines without accessing the passage, and 18% are unanimously judged by humans as not having a unique correct answer', 'OneStopQA provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance.'], ['In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge (WSC)', 'For each of the questions, we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories', 'By doing so, we better understand the limitation of existing methods (i.e., what kind of knowledge cannot be effectively represented or inferred with existing methods) and shed some light on the commonsense knowledge that we need to acquire in the future for better commonsense reasoning', 'Moreover, to investigate whether current WSC models can understand the commonsense or they simply solve the WSC questions based on the statistical bias of the dataset, we leverage the collected reasons to develop a new task called WinoWhy, which requires models to distinguish plausible reasons from very similar but wrong reasons for all WSC questions', 'Experimental results prove that even though pre-trained language representation models have achieved promising progress on the original WSC dataset, they are still struggling at WinoWhy', 'Further experiments show that even though supervised models can achieve better performance, the performance of these models can be sensitive to the dataset distribution', 'WinoWhy and all codes are available at: https://github.com/HKUST-KnowComp/WinoWhy.'], ['In online debates, users express different levels of agreement/disagreement with one another’s arguments and ideas', 'Often levels of agreement/disagreement are implicit in the text, and must be predicted to analyze collective opinions', 'Existing stance detection methods predict the polarity of a post’s stance toward a topic or post, but don’t consider the stance’s degree of intensity', 'We introduce a new research problem, stance polarity and intensity prediction in response relationships between posts', 'This problem is challenging because differences in stance intensity are often subtle and require nuanced language understanding', 'Cyber argumentation research has shown that incorporating both stance polarity and intensity data in online debates leads to better discussion analysis', 'We explore five different learning models: Ridge-M regression, Ridge-S regression, SVR-RF-R, pkudblab-PIP, and T-PAN-PIP for predicting stance polarity and intensity in argumentation', 'These models are evaluated using a new dataset for stance polarity and intensity prediction collected using a cyber argumentation platform', 'The SVR-RF-R model performs best for prediction of stance polarity with an accuracy of 70.43% and intensity with RMSE of 0.596', 'This work is the first to train models for predicting a post’s stance polarity and intensity in one combined value in cyber argumentation with reasonably good accuracy.'], ['Recent neural network models have achieved impressive performance on sentiment classification in English as well as other languages', 'Their success heavily depends on the availability of a large amount of labeled data or parallel corpus', 'In this paper, we investigate an extreme scenario of cross-lingual sentiment classification, in which the low-resource language does not have any labels or parallel corpus', 'We propose an unsupervised cross-lingual sentiment classification model named multi-view encoder-classifier (MVEC) that leverages an unsupervised machine translation (UMT) system and a language discriminator', 'Unlike previous language model (LM) based fine-tuning approaches that adjust parameters solely based on the classification error on training data, we employ the encoder-decoder framework of a UMT as a regularization component on the shared network parameters', 'In particular, the cross-lingual encoder of our model learns a shared representation, which is effective for both reconstructing input sentences of two languages and generating more representative views from the input for classification', 'Extensive experiments on five language pairs verify that our model significantly outperforms other models for 8/11 sentiment classification tasks.'], ['We present an efficient annotation framework for argument quality, a feature difficult to be measured reliably as per previous work', 'A stochastic transitivity model is combined with an effective sampling strategy to infer high-quality labels with low effort from crowdsourced pairwise judgments', 'The model’s capabilities are showcased by compiling Webis-ArgQuality-20, an argument quality corpus that comprises scores for rhetorical, logical, dialectical, and overall quality inferred from a total of 41,859 pairwise judgments among 1,271 arguments', 'With up to 93% cost savings, our approach significantly outperforms existing annotation procedures', 'Furthermore, novel insight into argument quality is provided through statistical analysis, and a new aggregation method to infer overall quality from individual quality dimensions is proposed.'], ['This paper studies the task of comparative preference classification (CPC)', 'Given two entities in a sentence, our goal is to classify whether the first (or the second) entity is preferred over the other or no comparison is expressed at all between the two entities', 'Existing works either do not learn entity-aware representations well and fail to deal with sentences involving multiple entity pairs or use sequential modeling approaches that are unable to capture long-range dependencies between the entities', 'Some also use traditional machine learning approaches that do not generalize well', 'This paper proposes a novel Entity-aware Dependency-based Deep Graph Attention Network (ED-GAT) that employs a multi-hop graph attention over a dependency graph sentence representation to leverage both the semantic information from word embeddings and the syntactic information from the dependency graph to solve the problem', 'Empirical evaluation shows that the proposed model achieves the state-of-the-art performance in comparative preference classification.'], ['We present OpinionDigest, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training', 'The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews, and trains a Transformer model to reconstruct the original reviews from these extractions', 'At summarization time, we merge extractions from multiple reviews and select the most popular ones', 'The selected opinions are used as input to the trained Transformer model, which verbalizes them into an opinion summary', 'OpinionDigest can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and/or sentiment', 'Automatic evaluation on Yelp data shows that our framework outperforms competitive baselines', 'Human studies on two corpora verify that OpinionDigest produces informative summaries and shows promising customization capabilities.'], ['Affective tasks such as sentiment analysis, emotion classification, and sarcasm detection have been popular in recent years due to an abundance of user-generated data, accurate computational linguistic models, and a broad range of relevant applications in various domains', 'At the same time, many studies have highlighted the importance of text preprocessing, as an integral step to any natural language processing prediction model and downstream task', 'While preprocessing in affective systems is well-studied, preprocessing in word vector-based models applied to affective systems, is not', 'To address this limitation, we conduct a comprehensive analysis of the role of preprocessing techniques in affective analysis based on word vector models', 'Our analysis is the first of its kind and provides useful insights of the importance of each preprocessing technique when applied at the training phase, commonly ignored in pretrained word vector models, and/or at the downstream task phase.'], ['Generative dialogue systems tend to produce generic responses, which often leads to boring conversations', 'For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs', 'While this paradigm works to a certain extent, it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context', 'Thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations', 'To this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI', 'We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques, Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI', 'We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation', 'Extensive evaluations over both an open-released English dataset and our Chinese dataset demonstrate that our approach ConKADI outperforms the state-of-the-art approach CCM, in most experiments.'], ['Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines', 'The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models', 'Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words', 'In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one', 'We carry out evaluations by both human and automatic metrics', 'Experiments on the Persona-Chat dataset show that our approach achieves good performance.'], ['Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems', 'Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task', 'However, fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective, resulting in similar dialogue models for different tasks', 'In this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting', 'In our approach, each dialogue model consists of a shared module, a gating module, and a private module', 'The first two modules are shared among all the tasks, while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task', 'The extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency, response quality, and diversity.'], ['Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses', 'In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns', 'We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network', 'Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context', 'We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.'], ['The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels', 'In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks', 'Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task', 'For example, extracting entities with the per label is formalized as extracting answer spans to the question “which person is mentioned in the text\".This formulation naturally tackles the entity overlapping issue in nested NER: the extraction of two overlapping entities with different categories requires answering two independent questions', 'Additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested NER, but flat NER', 'We conduct experiments on both nested and flat NER datasets.Experiment results demonstrate the effectiveness of the proposed formulation', 'We are able to achieve a vast amount of performance boost over current SOTA models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37,respectively on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets, i.e., +0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA and Chinese OntoNotes 4.0.'], ['Unlike widely used Named Entity Recognition (NER) data sets in generic domains, biomedical NER data sets often contain mentions consisting of discontinuous spans', 'Conventional sequence tagging techniques encode Markov assumptions that are efficient but preclude recovery of these mentions', 'We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER', 'Through extensive experiments on three biomedical data sets, we show that our model can effectively recognize discontinuous mentions without sacrificing the accuracy on continuous mentions.'], ['While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task', 'Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et', 'al', '18)', 'Our analysis reveals that CopyAttention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information', 'We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples', 'This approach overcomes both shortcomings of CopyAttention, resulting in a variable number of diverse extractions per sentence', 'We train IMoJIE on training data bootstrapped from extractions of several non-neural systems, which have been automatically filtered to reduce redundancy and noise', 'IMoJIE outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task.'], ['Event Detection (ED) is a fundamental task in automatically structuring texts', 'Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words', 'To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations', 'Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words', 'The source code is released on https://github.com/shuaiwa16/ekd.git.'], ['Exploiting sentence-level labels, which are easy to obtain, is one of the plausible methods to improve low-resource named entity recognition (NER), where token-level labels are costly to annotate', 'Current models for jointly learning sentence and token labeling are limited to binary classification', 'We present a joint model that supports multi-class classification and introduce a simple variant of self-attention that allows the model to learn scaling factors', 'Our model produces 3.78%, 4.20%, 2.08% improvements in F1 over the BiLSTM-CRF baseline on e-commerce product titles in three different low-resource languages: Vietnamese, Thai, and Indonesian, respectively.'], ['Cross-domain NER is a challenging yet practical problem', 'Entity mentions can be highly different across domains', 'However, the correlations between entity types can be relatively more stable across domains', 'We investigate a multi-cell compositional LSTM structure for multi-task learning, modeling each entity type using a separate cell state', 'With the help of entity typed units, cross-domain knowledge transfer can be made in an entity type level', 'Theoretically, the resulting distinct feature distributions for each entity type make it more powerful for cross-domain transfer', 'Empirically, experiments on four few-shot and zero-shot datasets show our method significantly outperforms a series of multi-task learning methods and achieves the best results.'], ['This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER)', 'In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape', 'Each time an embedding passes through a layer of the pyramid, its length is reduced by one', 'Its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention', 'We also design an inverse pyramid to allow bidirectional interaction between layers', 'The proposed method achieves state-of-the-art F1 scores in nested NER on ACE-2004, ACE-2005, GENIA, and NNE, which are 80.27, 79.42, 77.78, and 93.70 with conventional embeddings, and 87.74, 86.34, 79.31, and 94.68 with pre-trained contextualized embeddings', 'In addition, our model can be used for the more general task of Overlapping Named Entity Recognition', 'A preliminary experiment confirms the effectiveness of our method in overlapping NER.'], ['The goal of Knowledge graph embedding (KGE) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples', 'The conventional shallow models are limited to their expressiveness', 'ConvE (Dettmers et al., 2018) takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings', 'However, there is no structural information in the embedding space of ConvE, and the performance is still limited by the number of interactions', 'The recent KBGAT (Nathani et al., 2019) provides another way to learn embeddings by adaptively utilizing structural information', 'In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding (ReInceptionE)', 'Specifically, we first explore the Inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings', 'Then, we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information', 'Experimental results on both WN18RR and FB15k-237 datasets demonstrate that ReInceptionE achieves competitive performance compared with state-of-the-art methods.'], ['Distant supervision based methods for entity and relation extraction have received increasing popularity due to the fact that these methods require light human annotation efforts', 'In this paper, we consider the problem of shifted label distribution, which is caused by the inconsistency between the noisy-labeled training set subject to external knowledge graph and the human-annotated test set, and exacerbated by the pipelined entity-then-relation extraction manner with noise propagation', 'We propose a joint extraction approach to address this problem by re-labeling noisy instances with a group of cooperative multiagents', 'To handle noisy instances in a fine-grained manner, each agent in the cooperative group evaluates the instance by calculating a continuous confidence score from its own perspective; To leverage the correlations between these two extraction tasks, a confidence consensus module is designed to gather the wisdom of all agents and re-distribute the noisy training set with confidence-scored labels', 'Further, the confidences are used to adjust the training losses of extractors', 'Experimental results on two real-world datasets verify the benefits of re-labeling noisy instance, and show that the proposed model significantly outperforms the state-of-the-art entity and relation extraction methods.'], ['Recently, many works have tried to augment the performance of Chinese named entity recognition (NER) using word lexicons', 'As a representative, Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets', 'However, Lattice-LSTM has a complex model architecture', 'This limits its application in many industrial areas where real-time NER responses are needed', 'In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations', 'This method avoids designing a complicated sequence modeling architecture, and for any neural NER model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information', 'Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-of-the-art methods, along with a better performance', 'The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT.'], ['In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT)', 'The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs', 'We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning', 'Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora.'], ['The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns', 'Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation', 'In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context', 'Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context', 'We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU.'], ['Although neural machine translation (NMT) has achieved significant progress in recent years, most previous NMT models only depend on the source text to generate translation', 'Inspired by the success of template-based and syntax-based approaches in other fields, we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure', 'In order to learn the syntactic structure of the target sentences, we adopt constituency-based parse tree to generate candidate templates', 'We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text', 'Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrates the effectiveness of soft target templates.'], ['In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts', 'Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts', 'Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training', 'We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations', 'We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.'], ['Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language', 'Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder', 'Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.'], ['Measuring the scholarly impact of a document without citations is an important and challenging problem', 'Existing approaches such as Document Influence Model (DIM) are based on dynamic topic models, which only consider the word frequency change', 'In this paper, we use both frequency changes and word semantic shifts to measure document influence by developing a neural network framework', 'Our model has three steps', 'Firstly, we train the word embeddings for different time periods', 'Subsequently, we propose an unsupervised method to align vectors for different time periods', 'Finally, we compute the influence value of documents', 'Our experimental results show that our model outperforms DIM.'], ['Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation', 'Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity', 'To address these problems, we propose a novel retrieval-based method for paraphrase generation', 'Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index', 'With its novel editor module, the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences', 'In order to have fine-grained control over the editing process, our model uses the newly introduced concept of Micro Edit Vectors', 'It both extracts and exploits these vectors using the attention mechanism in the Transformer architecture', 'Experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics, and human evaluation of relevance, grammaticality, and diversity of generated paraphrases.'], ['We study the problem of multilingual masked language modeling, i.e', 'the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer', 'We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains', 'The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder', 'To better understand this result, we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces', 'For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process.'], ['Pre-trained language models like BERT have proven to be highly performant', 'However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources', 'To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time', 'The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided', 'Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance', 'Our model achieves promising results in twelve English and Chinese datasets', 'It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff.'], ['Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents', 'Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation', 'Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa', 'The code and resources are available at https://github.com/neulab/external-knowledge-codegen.'], ['Verifying the correctness of a textual statement requires not only semantic reasoning about the meaning of words, but also symbolic reasoning about logical operations like count, superlative, aggregation, etc', 'In this work, we propose LogicalFactChecker, a neural network approach capable of leveraging logical operations for fact checking', 'It achieves the state-of-the-art performance on TABFACT, a large-scale, benchmark dataset built for verifying a textual statement with semi-structured tables', 'This is achieved by a graph module network built upon the Transformer-based architecture', 'With a textual statement and a table as the input, LogicalFactChecker automatically derives a program (a.k.a', 'logical form) of the statement in a semantic parsing manner', 'A heterogeneous graph is then constructed to capture not only the structures of the table and the program, but also the connections between inputs with different modalities', 'Such a graph reveals the related contexts of each word in the statement, the table and the program', 'The graph is used to obtain graph-enhanced contextual representations of words in Transformer-based architecture', 'After that, a program-driven module network is further introduced to exploit the hierarchical structure of the program, where semantic compositionality is dynamically modeled along the program structure with a set of function-specific modules', 'Ablation experiments suggest that both the heterogeneous graph and the module network are important to obtain strong results.'], ['Adversarial attacks are carried out to reveal the vulnerability of deep neural networks', 'Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input', 'Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods', 'However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed', 'In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately', 'We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets', 'Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods', 'Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training', 'All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.'], ['Existing datasets for regular expression (regex) generation from natural language are limited in complexity; compared to regex tasks that users post on StackOverflow, the regexes in these datasets are simple, and the language used to describe them is not diverse', 'We introduce StructuredRegex, a new regex synthesis dataset differing from prior ones in three aspects', 'First, to obtain structurally complex and realistic regexes, we generate the regexes using a probabilistic grammar with pre-defined macros observed from real-world StackOverflow posts', 'Second, to obtain linguistically diverse natural language descriptions, we show crowdworkers abstract depictions of the underlying regex and ask them to describe the pattern they see, rather than having them paraphrase synthetic language', 'Third, we augment each regex example with a collection of strings that are and are not matched by the ground truth regex, similar to how real users give examples', 'Our quantitative and qualitative analysis demonstrates the advantages of StructuredRegex over prior datasets', 'Further experimental results using various multimodal synthesis techniques highlight the challenge presented by our dataset, including non-local constraints and multi-modal inputs.'], ['With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks', 'At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally', 'However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum', 'Based on this idea, we propose our Curriculum Learning approach', 'By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models', 'Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.'], ['Despite the success of language models using neural networks, it remains unclear to what extent neural models have the generalization ability to perform inferences', 'In this paper, we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition', 'We consider four aspects of monotonicity inferences and test whether the models can systematically interpret lexical and logical phenomena on different training/test splits', 'A series of experiments show that three neural models systematically draw inferences on unseen combinations of lexical and logical phenomena when the syntactic structures of the sentences are similar between the training and test sets', 'However, the performance of the models significantly decreases when the structures are slightly changed in the test set while retaining all vocabularies and constituents already appearing in the training set', 'This indicates that the generalization ability of neural models is limited to cases where the syntactic structures are nearly the same as those in the training set.'], ['Generating inferential texts about an event in different perspectives requires reasoning over different contexts that the event occurs', 'Existing works usually ignore the context that is not explicitly provided, resulting in a context-independent semantic representation that struggles to support the generation', 'To address this, we propose an approach that automatically finds evidence for an event from a large text corpus, and leverages the evidence to guide the generation of inferential texts', 'Our approach works in an encoderdecoder manner and is equipped with Vector Quantised-Variational Autoencoder, where the encoder outputs representations from a distribution over discrete variables', 'Such discrete representations enable automatically selecting relevant evidence, which not only facilitates evidence-aware generation, but also provides a natural way to uncover rationales behind the generation', 'Our approach provides state-of-the-art performance on both Event2mind and Atomic datasets', 'More importantly, we find that with discrete representations, our model selectively uses evidence to generate different inferential texts.'], ['Given a sentence and its relevant answer, how to ask good questions is a challenging task, which has many real applications', 'Inspired by human’s paraphrasing capability to ask questions of the same meaning but with diverse expressions, we propose to incorporate paraphrase knowledge into question generation(QG) to generate human-like questions', 'Specifically, we present a two-hand hybrid model leveraging a self-built paraphrase resource, which is automatically conducted by a simple back-translation method', 'On the one hand, we conduct multi-task learning with sentence-level paraphrase generation (PG) as an auxiliary task to supplement paraphrase knowledge to the task-share encoder', 'On the other hand, we adopt a new loss function for diversity training to introduce more question patterns to QG', 'Extensive experimental results show that our proposed model obtains obvious performance gain over several strong baselines, and further human evaluation validates that our model can ask questions of high quality by leveraging paraphrase knowledge.'], ['Knowledge inference on knowledge graph has attracted extensive attention, which aims to find out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications', 'However, researchers have mainly poured attention to knowledge inference on binary facts', 'The studies on n-ary facts are relatively scarcer, although they are also ubiquitous in the real world', 'Therefore, this paper addresses knowledge inference on n-ary facts', 'We represent each n-ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute-value pair(s)', 'We further propose a neural network model, NeuInfer, for knowledge inference on n-ary facts', 'Besides handling the common task to infer an unknown element in a whole fact, NeuInfer can cope with a new type of task, flexible knowledge inference', 'It aims to infer an unknown element in a partial fact consisting of the primary triple coupled with any number of its auxiliary description(s)', 'Experimental results demonstrate the remarkable superiority of NeuInfer.'], ['Chinese short text matching usually employs word sequences rather than character sequences to get better performance', 'However, Chinese word segmentation can be erroneous, ambiguous or inconsistent, which consequently hurts the final matching performance', 'To address this problem, we propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information', 'Instead of a character sequence or a single word sequence, paired word lattices formed from multiple word segmentation hypotheses are used as input and the model learns a graph representation according to an attentive graph matching mechanism', 'Experiments on two Chinese datasets show that our models outperform the state-of-the-art short text matching models.'], ['Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics', 'However, the existing parameter inference method like Monte Carlo sampling is quite time-consuming', 'In this paper, we propose two efficient neural mixed counting models, i.e., the Negative Binomial-Neural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery', 'Neural variational inference algorithms are developed to infer model parameters by using the reparameterization of Gamma distribution and the Gaussian approximation of Poisson distribution', 'Experiments on real-world datasets indicate that our models outperform state-of-the-art baseline models in terms of perplexity and topic coherence', 'The results also validate that both NB-NTM and GNB-NTM can produce explainable intermediate variables by generating dispersed proportions of document topics.'], ['Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence', 'In this work, we present a method suitable for reasoning about the semantic-level structure of evidence', 'Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling', 'We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet', 'Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances', 'Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph', 'We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy', 'Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.'], ['In this paper, we study the challenging problem of automatic generation of citation texts in scholarly papers', 'Given the context of a citing paper A and a cited paper B, the task aims to generate a short text to describe B in the given context of A', 'One big challenge for addressing this task is the lack of training data', 'Usually, explicit citation texts are easy to extract, but it is not easy to extract implicit citation texts from scholarly papers', 'We thus first train an implicit citation extraction model based on BERT and leverage the model to construct a large training dataset for the citation text generation task', 'Then we propose and train a multi-source pointer-generator network with cross attention mechanism for citation text generation', 'Empirical evaluation results on a manually labeled test dataset verify the efficacy of our model', 'This pilot study confirms the feasibility of automatically generating citation texts in scholarly papers and the technique has the great potential to help researchers prepare their scientific papers.'], ['In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization', 'To well handle the problem of composing EDUs into an informative and fluent summary, we propose a novel summarization method that first designs an EDU selection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence', 'We also design the reinforcement learning mechanism to use EDU fusion results to reward the EDU selection action, boosting the final summarization performance', 'Experiments on CNN/Daily Mail have demonstrated the effectiveness of our model.'], ['This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems', 'Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space', 'Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset', 'Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1)', 'Experiments on the other five datasets also show the effectiveness of the matching framework', 'We believe the power of this matching-based summarization framework has not been fully exploited', 'To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github.com/maszhongming/MatchSum.'], ['As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches', 'An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships', 'In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences', 'These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations', 'Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes', 'To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits', 'The code will be released on Github.'], ['Cross-lingual summarization is the task of generating a summary in one language given a text in a different language', 'Previous works on cross-lingual summarization mainly focus on using pipeline methods or training an end-to-end model using the translated parallel data', 'However, it is a big challenge for the model to directly learn cross-lingual summarization as it requires learning to understand different languages and learning how to summarize at the same time', 'In this paper, we propose to ease the cross-lingual summarization training by jointly learning to align and summarize', 'We design relevant loss functions to train this framework and propose several methods to enhance the isomorphism and cross-lingual transfer between languages', 'Experimental results show that our model can outperform competitive models in most cases', 'In addition, we show that our model even has the ability to generate cross-lingual summaries without access to any cross-lingual corpus.'], ['Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries', 'In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries', 'Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents', 'Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries', 'Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly', 'Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.'], ['In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents', 'The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary', 'We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization', 'Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.'], ['We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence', 'In order to maximally leverage current neural architectures, the model scores each word’s tags in parallel, with minimal task-specific structure', 'After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time', 'Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.'], ['Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation (QE) for machine translation', 'A carefully engineered ensemble of such models won the QE shared task at WMT19', 'Our in-depth analysis, however, shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets: (i) The distributions of quality scores are imbalanced and skewed towards good quality scores; (iii) QE models can perform well on these datasets while looking at only source or translated sentences; (iii) They contain statistical artifacts that correlate well with human-annotated QE labels', 'Our findings suggest that although QE models might capture fluency of translated sentences and complexity of source sentences, they cannot model adequacy of translations effectively.'], ['While natural language understanding (NLU) is advancing rapidly, today’s technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization', 'This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL)', 'According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction', 'This position paper argues that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems, and proposes a system architecture along with a roadmap towards realizing this vision.'], ['Language technologies contribute to promoting multilingualism and linguistic diversity around the world', 'However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications', 'In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time', 'Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the “language agnostic” status of current models and systems', 'Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.'], ['In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures', 'We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model', 'This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding.'], ['Corpus query systems exist to address the multifarious information needs of any person interested in the content of annotated corpora', 'In this role they play an important part in making those resources usable for a wider audience', 'Over the past decades, several such query systems and languages have emerged, varying greatly in their expressiveness and technical details', 'This paper offers a broad overview of the history of corpora and corpus query tools', 'It focusses strongly on the query side and hints at exciting directions for future development.'], ['Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs', 'However, most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history', 'Besides, existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of hard slots and eventually hurts overall performance', 'In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations', 'We further propose an adaptive objective to alleviate the slot imbalance problem by dynamically adjust weights of different slots during training', 'Experimental results show that our approach reaches 52.68% and 58.55% joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new state-of-the-art performance with considerable improvements (+1.24% and +5.98%).'], ['Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm', 'As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model', 'However, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear', 'This impedes the learning of those data-driven neural dialogue models', 'Therefore, effective dialogue learning requires not only more reliable learning samples, but also fewer noisy samples', 'In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously', 'In particular, the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data', 'Note that, the proposed data manipulation framework is fully data-driven and learnable', 'It not only manipulates training samples to optimize the dialogue generation model, but also learns to increase its manipulation skills through gradient descent with validation samples', 'Extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments.'], ['Recent studies have shown remarkable success in end-to-end task-oriented dialog system', 'However, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling', 'This makes it difficult to scalable for a new domain with limited labeled data', 'However, there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains', 'To this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge', 'In addition, we propose a novel Dynamic Fusion Network (DF-Net) which automatically exploit the relevance between the target domain and each domain', 'Results show that our models outperforms existing methods on multi-domain dialogue, giving the state-of-the-art in the literature', 'Besides, with little training data, we show its transferability by outperforming prior best model by 13.9% on average.'], ['Training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users', 'Human demonstrations can be used to accelerate learning progress', 'However, how to effectively leverage demonstrations to learn dialogue policy remains less explored', 'In this paper, we present Sˆ2Agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping', 'We use an imitation model to distill knowledge from demonstrations, based on which policy shaping estimates feedback on how the agent should act in policy space', 'Reward shaping is then incorporated to bonus state-actions similar to demonstrations explicitly in value space encouraging better exploration', 'The effectiveness of the proposed Sˆ2Agentt is demonstrated in three dialogue domains and a challenging domain adaptation task with both user simulator evaluation and human evaluation.'], ['Dialogue state tracker is responsible for inferring user intentions through dialogue history', 'Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information', 'We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information’s interference and improve long dialogue context tracking', 'Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module', 'Our model yields a significantly improved performance compared to previous state-of the-art models on the MultiWOZ dataset.'], ['Automatic evaluation of open-domain dialogue response generation is very challenging because there are many appropriate responses for a given context', 'Existing evaluation models merely compare the generated response with the ground truth response and rate many of the appropriate responses as inappropriate if they deviate from the ground truth', 'One approach to resolve this problem is to consider the similarity of the generated response with the conversational context', 'In this paper, we propose an automatic evaluation model based on that idea and learn the model parameters from an unlabeled conversation corpus', 'Our approach considers the speakers in defining the different levels of similar context', 'We use a Twitter conversation corpus that contains many speakers and conversations to test our evaluation model', 'Experiments show that our model outperforms the other existing evaluation metrics in terms of high correlation with human annotation scores', 'We also show that our model trained on Twitter can be applied to movie dialogues without any additional training', 'We provide our code and the learned parameters so that they can be used for automatic evaluation of dialogue response generation models.'], ['Due to its great importance in deep natural language understanding and various down-stream applications, text-level parsing of discourse rhetorical structure (DRS) has been drawing more and more attention in recent years', 'However, all the previous studies on text-level discourse parsing adopt bottom-up approaches, which much limit the DRS determination on local information and fail to well benefit from global information of the overall discourse', 'In this paper, we justify from both computational and perceptive points-of-view that the top-down architecture is more suitable for text-level DRS parsing', 'On the basis, we propose a top-down neural architecture toward text-level DRS parsing', 'In particular, we cast discourse parsing as a recursive split point ranking task, where a split point is classified to different levels according to its rank and the elementary discourse units (EDUs) associated with it are arranged accordingly', 'In this way, we can determine the complete DRS as a hierarchical tree structure via an encoder-decoder with an internal stack', 'Experimentation on both the English RST-DT corpus and the Chinese CDTB corpus shows the great effectiveness of our proposed top-down approach towards text-level DRS parsing.'], ['An in-depth exploration of protein-protein interactions (PPI) is essential to understand the metabolism in addition to the regulations of biological entities like proteins, carbohydrates, and many more', 'Most of the recent PPI tasks in BioNLP domain have been carried out solely using textual data', 'In this paper, we argue that incorporating multimodal cues can improve the automatic identification of PPI', 'As a first step towards enabling the development of multimodal approaches for PPI identification, we have developed two multi-modal datasets which are extensions and multi-modal versions of two popular benchmark PPI corpora (BioInfer and HRPD50)', 'Besides, existing textual modalities, two new modalities, 3D protein structure and underlying genomic sequence, are also added to each instance', 'Further, a novel deep multi-modal architecture is also implemented to efficiently predict the protein interactions from the developed datasets', 'A detailed experimental analysis reveals the superiority of the multi-modal approach in comparison to the strong baselines including unimodal approaches and state-of the-art methods over both the generated multi-modal datasets', 'The developed multi-modal datasets are available for use at https://github.com/sduttap16/MM_PPI_NLP.'], ['In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for nested named entity recognition (NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner layers', 'Bidirectional LSTM (BiLSTM) and graph convolutional network (GCN) are adopted to jointly learn flat entities and their inner dependencies', 'Different from previous models, which only consider the unidirectional delivery of information from innermost layers to outer ones (or outside-to-inside), our model effectively captures the bidirectional interaction between them', 'We first use the entities recognized by the flat NER module to construct an entity graph, which is fed to the next graph module', 'The richer representation learned from graph module carries the dependencies of inner entities and can be exploited to improve outermost entity predictions', 'Experimental results on three standard nested NER datasets demonstrate that our BiFlaG outperforms previous state-of-the-art models.'], ['Knowledge graph (KG) entity typing aims at inferring possible missing entity type instances in KG, which is a very significant but still under-explored subtask of knowledge graph completion', 'In this paper, we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge in KGs', 'Specifically, we present two distinct knowledge-driven effective mechanisms of entity type inference', 'Accordingly, we build two novel embedding models to realize the mechanisms', 'Afterward, a joint model via connecting them is used to infer missing entity type instances, which favors inferences that agree with both entity type instances and triple knowledge in KGs', 'Experimental results on two real-world datasets (Freebase and YAGO) demonstrate the effectiveness of our proposed mechanisms and models for improving KG entity typing', 'The source code and data of this paper can be obtained from: https://github.com/Adam1679/ConnectE .'], ['Continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations', 'Some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem', 'However, these memory-based methods usually suffer from overfitting the few memorized examples of old relations, which may gradually cause inevitable confusion among existing relations', 'Inspired by the mechanism in human long-term memory formation, we introduce episodic memory activation and reconsolidation (EMAR) to continual relation learning', 'Every time neural models are activated to learn both new and memorized data, EMAR utilizes relation prototypes for memory reconsolidation exercise to keep a stable understanding of old relations', 'The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models.'], ['One great challenge in neural sequence labeling is the data sparsity problem for rare entity words and phrases', 'Most of test set entities appear only few times and are even unseen in training corpus, yielding large number of out-of-vocabulary (OOV) and low-frequency (LF) entities during evaluation', 'In this work, we propose approaches to address this problem', 'For OOV entities, we introduce local context reconstruction to implicitly incorporate contextual information into their representations', 'For LF entities, we present delexicalized entity identification to explicitly extract their frequency-agnostic and entity-type-specific representations', 'Extensive experiments on multiple benchmark datasets show that our model has significantly outperformed all previous methods and achieved new start-of-the-art results', 'Notably, our methods surpass the model fine-tuned on pre-trained language models without external resource.'], ['Interpretable rationales for model predictions play a critical role in practical applications', 'In this study, we develop models possessing interpretable inference process for structured prediction', 'Specifically, we present a method of instance-based learning that learns similarities between spans', 'At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions', 'Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance.'], ['Electronic Medical Records (EMRs) have become key components of modern medical care systems', 'Despite the merits of EMRs, many doctors suffer from writing them, which is time-consuming and tedious', 'We believe that automatically converting medical dialogues to EMRs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step', 'To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation', 'We then propose a Medical Information Extractor (MIE) towards medical dialogues', 'MIE is able to extract mentioned symptoms, surgeries, tests, other information and their corresponding status', 'To tackle the particular challenges of the task, MIE uses a deep matching architecture, taking dialogue turn-interaction into account', 'The experimental results demonstrate MIE is a promising solution to extract medical information from doctor-patient dialogues.'], ['Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities', 'NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009)', 'In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017)', 'The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately', 'We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.'], ['Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment', 'This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling the structural heterogeneity challenge', 'NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference', 'It provides two innovative components for better learning representations for entity alignment', 'It first uses a novel graph sampling method to distill a discriminative neighborhood for each entity', 'It then adopts a cross-graph neighborhood matching module to jointly encode the neighborhood difference for a given entity pair', 'Such strategies allow NMN to effectively construct matching-oriented entity representations while ignoring noisy neighbors that have a negative impact on the alignment task', 'Extensive experiments performed on three entity alignment datasets show that NMN can well estimate the neighborhood similarity in more tough cases and significantly outperforms 12 previous state-of-the-art methods.'], ['Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences', 'Efforts thus far have focused on improving extraction accuracy but little is known about their explanability', 'In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models', 'We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation', 'We also propose to automatically generate “distractor” sentences to augment the bags and train the model to ignore the distractors', 'Evaluations on the widely used FB-NYT dataset show that our methods achieve new state-of-the-art accuracy while improving model explanability.'], ['We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images', 'We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document', 'These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases.'], ['To better tackle the named entity recognition (NER) problem on languages with little/no labeled data, cross-lingual NER must effectively leverage knowledge learned from source languages with rich labeled data', 'Previous works on cross-lingual NER are mostly based on label projection with pairwise texts or direct model transfer', 'However, such methods either are not applicable if the labeled data in the source languages is unavailable, or do not leverage information contained in unlabeled data in the target language', 'In this paper, we propose a teacher-student learning method to address such limitations, where NER models in the source languages are used as teachers to train a student model on unlabeled data in the target language', 'The proposed method works for both single-source and multi-source cross-lingual NER', 'For the latter, we further propose a similarity measuring method to better weight the supervision from different teacher models', 'Extensive experiments for 3 target languages on benchmark datasets well demonstrate that our method outperforms existing state-of-the-art methods for both single-source and multi-source cross-lingual NER.'], ['Opinion entity extraction is a fundamental task in fine-grained opinion mining', 'Related studies generally extract aspects and/or opinion expressions without recognizing the relations between them', 'However, the relations are crucial for downstream tasks, including sentiment classification, opinion summarization, etc', 'In this paper, we explore Aspect-Opinion Pair Extraction (AOPE) task, which aims at extracting aspects and opinion expressions in pairs', 'To deal with this task, we propose Synchronous Double-channel Recurrent Network (SDRN) mainly consisting of an opinion entity extraction unit, a relation detection unit, and a synchronization unit', 'The opinion entity extraction unit and the relation detection unit are developed as two channels to extract opinion entities and relations simultaneously', 'Furthermore, within the synchronization unit, we design Entity Synchronization Mechanism (ESM) and Relation Synchronization Mechanism (RSM) to enhance the mutual benefit on the above two channels', 'To verify the performance of SDRN, we manually build three datasets based on SemEval 2014 and 2015 benchmarks', 'Extensive experiments demonstrate that SDRN achieves state-of-the-art performances.'], ['We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning', 'Using an annotation protocol specifically devised for capturing image–caption coherence relations, we annotate 10,000 instances from publicly-available image–caption pairs', 'We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware, controllable image captioning models', 'The results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations.'], ['In human cognition, world knowledge supports the perception of object colours: knowing that trees are typically green helps to perceive their colour in certain contexts', 'We go beyond previous studies on colour terms using isolated colour swatches and study visual grounding of colour terms in realistic objects', 'Our models integrate processing of visual information and object-specific knowledge via hard-coded (late) or learned (early) fusion', 'We find that both models consistently outperform a bottom-up baseline that predicts colour terms solely from visual inputs, but show interesting differences when predicting atypical colours of so-called colour diagnostic objects', 'Our models also achieve promising results when tested on new object categories not seen during training.'], ['Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query', 'Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span', 'In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage', 'We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL', 'The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting (QGH) strategy', 'The QGH guides VSLNet to search for matching video span within a highlighted region', 'Through extensive experiments on three benchmark datasets, we show that the proposed VSLNet outperforms the state-of-the-art methods; and adopting span-based QA framework is a promising direction to solve NLVL.'], ['Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image', 'We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn’t matter', 'To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn’t', 'Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes', 'Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task', 'We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task', 'Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv.'], ['Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks', 'Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss', 'In this work, we instead “reallocate” them—the model learns to activate different heads on different inputs', 'Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE)', 'MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters', 'Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks', 'Particularly, on the WMT14 English to German translation dataset, MAE improves over “transformer-base” by 0.8 BLEU, with a comparable number of parameters', 'Our analysis shows that our model learns to specialize different experts to different inputs.'], ['Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect', 'One sentence may contain various sentiments for different aspects', 'Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge', 'Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words', 'But the improvement is limited due to the noise and instability of dependency trees', 'To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner', 'Specifically, a dual-transformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning', 'The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa', 'The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin.'], ['We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection', 'While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling more focused attentions over the input regions', 'We propose two variants of Differentiable Window, and integrate them within the Transformer architecture in two novel ways', 'We evaluate our proposed approach on a myriad of NLP tasks, including machine translation, sentiment analysis, subject-verb agreement and language modeling', 'Our experimental results demonstrate consistent and sizable improvements across all tasks.'], ['Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples', 'Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension', 'In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings', 'Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data.'], ['It is commonly believed that knowledge of syntactic structure should improve language modeling', 'However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic', 'In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called “syntactic distances”, where information between these two separate objectives shares the same intermediate representation', 'Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.'], ['Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell', 'In this paper, we extend the search space of NAS', 'In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS)', 'For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously', 'We implement our model in a differentiable architecture search system', 'For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB', 'Moreover, the learned architectures show good transferability to other systems', 'E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.'], ['As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs', 'To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) “exit” from neural network calculations for simple instances, and late (and accurate) exit for hard instances', 'To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions', 'We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks', 'Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy', 'Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model', 'Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time', 'We publicly release our code.'], ['Polysynthetic languages have exceptionally large and sparse vocabularies, thanks to the number of morpheme slots and combinations in a word', 'This complexity, together with a general scarcity of written data, poses a challenge to the development of natural language technologies', 'To address this challenge, we offer linguistically-informed approaches for bootstrapping a neural morphological analyzer, and demonstrate its application to Kunwinjku, a polysynthetic Australian language', 'We generate data from a finite state transducer to train an encoder-decoder model', 'We improve the model by “hallucinating” missing linguistic structure into the training data, and by resampling from a Zipf distribution to simulate a more natural distribution of morphemes', 'The best model accounts for all instances of reduplication in the test set and achieves an accuracy of 94.7% overall, a 10 percentage point improvement over the FST baseline', 'This process demonstrates the feasibility of bootstrapping a neural morph analyzer from minimal resources.'], ['Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation (CWS)', 'Nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (OOV) problem', 'In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain CWS', '1) We rethink the essence of “Chinese words” and design an automatic distant annotation mechanism, which does not need any supervision or pre-defined dictionaries on the target domain', 'The method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain', '2) We further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information', 'Experiments on multiple real-world datasets across various domains show the superiority and robustness of our model, significantly outperforming previous state-of-the-arts cross-domain CWS methods.'], ['This paper describes a language-independent model for fully unsupervised morphological analysis that exploits a universal framework leveraging morphological typology', 'By modeling morphological processes including suffixation, prefixation, infixation, and full and partial reduplication with constrained stem change rules, our system effectively constrains the search space and offers a wide coverage in terms of morphological typology', 'The system is tested on nine typologically and genetically diverse languages, and shows superior performance over leading systems', 'We also investigate the effect of an oracle that provides only a handful of bits per language to signal morphological type.'], ['The noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties', 'Class membership is far from deterministic, but the phonological form of a noun and/or its meaning can often provide imperfect clues', 'Here, we investigate the strength of those clues', 'More specifically, we operationalize this by measuring how much information, in bits, we can glean about declension class from knowing the form and/or meaning of nouns', 'We know that form and meaning are often also indicative of grammatical gender—which, as we quantitatively verify, can itself share information with declension class—so we also control for gender', 'We find for two Indo-European languages (Czech and German) that form and meaning respectively share significant amounts of information with class (and contribute additional information above and beyond gender)', 'The three-way interaction between class, form, and meaning (given gender) is also significant', 'Our study is important for two reasons: First, we introduce a new method that provides additional quantitative support for a classic linguistic finding that form and meaning are relevant for the classification of nouns into declensions', 'Secondly, we show not only that individual declensions classes vary in the strength of their clues within a language, but also that these variations themselves vary across languages.'], ['We propose the task of unsupervised morphological paradigm completion', 'Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas', 'From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators', 'From a cognitive science perspective, this can shed light on how children acquire morphological knowledge', 'We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation', 'We perform an evaluation on 14 typologically diverse languages', 'Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.'], ['Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer)', 'Despite the effectiveness of existing methods on this benchmark, they treat these two sub-tasks individually during training while ignoring their dependencies', 'To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and tokens', 'We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously', 'The long and short answers can be extracted from paragraph-level representation and token-level representation, respectively', 'In this way, we can model the dependencies between the two-grained answers to provide evidence for each other', 'We jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria.'], ['Question Answering (QA) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models', 'Recent research works have attempted to extend these successes to the settings with few or no labeled data available', 'In this work, we introduce two approaches to improve unsupervised QA', 'First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as RefQA)', 'Second, we take advantage of the QA model to extract more appropriate answers, which iteratively refines data over RefQA', 'We conduct experiments on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data', 'Our approach outperforms previous unsupervised approaches by a large margin, and is competitive with early supervised models', 'We also show the effectiveness of our approach in the few-shot learning setting.'], ['This paper focuses on generating multi-hop reasoning questions from the raw text in a low resource circumstance', 'Such questions have to be syntactically valid and need to logically correlate with the answers by deducing over multiple relations on several sentences in the text', 'Specifically, we first build a multi-hop generation model and guide it to satisfy the logical rationality by the reasoning chain extracted from a given text', 'Since the labeled data is limited and insufficient for training, we propose to learn the model with the help of a large scale of unlabeled data that is much easier to obtain', 'Such data contains rich expressive forms of the questions with structural patterns on syntax and semantics', 'These patterns can be estimated by the neural hidden semi-Markov model using latent variables', 'With latent patterns as a prior, we can regularize the generation model and produce the optimal results', 'Experimental results on the HotpotQA data set demonstrate the effectiveness of our model', 'Moreover, we apply the generated results to the task of machine reading comprehension and achieve significant performance improvements.'], ['Recent studies have revealed that reading comprehension (RC) systems learn to exploit annotation artifacts and other biases in current datasets', 'This prevents the community from reliably measuring the progress of RC systems', 'To address this issue, we introduce R4C, a new task for evaluating RC systems’ internal reasoning', 'R4C requires giving not only answers but also derivations: explanations that justify predicted answers', 'We present a reliable, crowdsourced framework for scalably annotating RC datasets with derivations', 'We create and publicly release the R4C dataset, the first, quality-assured dataset consisting of 4.6k questions, each of which is annotated with 3 reference derivations (i.e', '13.8k derivations)', 'Experiments show that our automatic evaluation metrics using multiple reference derivations are reliable, and that R4C assesses different skills from an existing benchmark.'], ['In this paper, we study machine reading comprehension (MRC) on long texts: where a model takes as inputs a lengthy document and a query, extracts a text span from the document as an answer', 'State-of-the-art models (e.g., BERT) tend to use a stack of transformer layers that are pre-trained from a large number of unlabeled language corpora to encode the joint contextual information of query and document', 'However, these transformer models can only take as input a fixed-length (e.g., 512) text', 'To deal with even longer text inputs, previous approaches usually chunk them into equally-spaced segments and predict answers based on each segment independently without considering the information from other segments', 'As a result, they may form segments that fail to cover complete answers or retain insufficient contexts around the correct answer required for question answering', 'Moreover, they are less capable of answering questions that need cross-segment information', 'We propose to let a model learn to chunk in a more flexible way via reinforcement learning: a model can decide the next segment that it wants to process in either direction', 'We also apply recurrent mechanisms to enable information to flow across segments', 'Experiments on three MRC tasks – CoQA, QuAC, and TriviaQA – demonstrate the effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions.'], ['Reading long documents to answer open-domain questions remains challenging in natural language understanding', 'In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question answering', 'RikiNet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor', 'The reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms', 'The representations are then fed into the predictor to obtain the span of the short answer, the paragraph of the long answer, and the answer type in a cascaded manner', 'On the Natural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1 on long-answer and short-answer tasks', 'To our best knowledge, it is the first single model that outperforms the single human performance', 'Furthermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks, achieving the best performance on the official NQ leaderboard.'], ['We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing', 'The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach', 'Moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available', 'We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for English Resource Semantics', 'At the core of this parser is a novel neural graph rewriting system which combines the strengths of Hyperedge Replacement Grammar, a knowledge-intensive model, and Graph Neural Networks, a data-intensive model', 'Our parser achieves an accuracy of 92.39% in terms of elementary dependency match, which is a 2.88 point improvement over the best data-driven model in the literature', 'The output of our parser is highly coherent: at least 91% graphs are valid, in that they allow at least one sound scope-resolved logical form.'], ['This paper is concerned with semantic parsing for English as a second language (ESL)', 'Motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface during second language acquisition, we formulate the task based on the divergence between literal and intended meanings', 'We combine the complementary strengths of English Resource Grammar, a linguistically-precise hand-crafted deep grammar, and TLE, an existing manually annotated ESL UD-TreeBank with a novel reranking model', 'Experiments demonstrate that in comparison to human annotations, our method can obtain a very promising SemBanking quality', 'By means of the newly created corpus, we evaluate state-of-the-art semantic parsing as well as grammatical error correction models', 'The evaluation profiles the performance of neural NLP techniques for handling ESL data and suggests some research directions.'], ['Semantic dependency parsing, which aims to find rich bi-lexical relationships, allows words to have multiple dependency heads, resulting in graph-structured representations', 'We propose an approach to semi-supervised learning of semantic dependency parsers based on the CRF autoencoder framework', 'Our encoder is a discriminative neural semantic dependency parser that predicts the latent parse graph of the input sentence', 'Our decoder is a generative neural model that reconstructs the input sentence conditioned on the latent parse graph', 'Our model is arc-factored and therefore parsing and learning are both tractable', 'Experiments show our model achieves significant and consistent improvement over the supervised baseline.'], ['One daunting problem for semantic parsing is the scarcity of annotation', 'Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance', 'The downstream naive semantic parser accepts the intermediate output and returns the target logical form', 'Furthermore, the entire training process is split into two phases: pre-training and cycle learning', 'Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model', 'Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.'], ['Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently', 'State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem', 'Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing', 'In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling', 'Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature.'], ['We tackle the task of Term Set Expansion (TSE): given a small seed set of example terms from a semantic class, finding more members of that class', 'The task is of great practical utility, and also of theoretical utility as it requires generalization from few examples', 'Previous approaches to the TSE task can be characterized as either distributional or pattern-based', 'We harness the power of neural masked language models (MLM) and propose a novel TSE algorithm, which combines the pattern-based and distributional approaches', 'Due to the small size of the seed set, fine-tuning methods are not effective, calling for more creative use of the MLM', 'The gist of the idea is to use the MLM to first mine for informative patterns with respect to the seed set, and then to obtain more members of the seed class by generalizing these patterns', 'Our method outperforms state-of-the-art TSE algorithms', 'Implementation is available at: https://github.com/ guykush/TermSetExpansion-MPB/'], ['Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information', 'However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed', 'In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans', 'Each span corresponds to a character or latent word and its position in the original lattice', 'With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallel ability', 'Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency.'], ['Entity embeddings, which represent different aspects of each entity with a single vector like word embeddings, are a key component of neural entity linking models', 'Existing entity embeddings are learned from canonical Wikipedia articles and local contexts surrounding target entities', 'Such entity embeddings are effective, but too distinctive for linking models to learn contextual commonality', 'We propose a simple yet effective method, FGS2EE, to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality', 'FGS2EE first uses the embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation', 'Extensive experiments show the effectiveness of such embeddings', 'Based on our entity embeddings, we achieved new sate-of-the-art performance on entity linking.'], ['We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT)', 'Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013–2015 containing English documents and queries in several European languages', 'We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach)', 'The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages', 'NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT.'], ['Showing items that do not match search query intent degrades customer experience in e-commerce', 'These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs', 'Mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain', 'In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier', 'We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples', 'This not only makes the classifier more robust but also boosts the overall ranking performance', 'Our model achieves a relative gain compared to baselines by over 26% in F-score, and over 17% in Area Under PR curve', 'On live search traffic, our model gains significant improvement in multiple countries.'], ['Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e', 'over-confident) predictions, a common sign of overfitting', 'This class of techniques, of which label smoothing is one, has a connection to entropy regularization', 'Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored', 'We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks', 'We also find that variance in model performance can be explained largely by the resulting entropy of the model', 'Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place.'], ['Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations', 'Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations', 'The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms', 'We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.'], ['Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts', 'KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space', 'For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations', 'However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs', 'In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns', 'Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns', 'Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions', 'Furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations', 'In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10.'], ['Effective projection-based cross-lingual word embedding (CLWE) induction critically relies on the iterative self-learning procedure', 'It gradually expands the initial small seed dictionary to learn improved cross-lingual mappings', 'In this work, we present ClassyMap, a classification-based approach to self-learning, yielding a more robust and a more effective induction of projection-based CLWEs', 'Unlike prior self-learning methods, our approach allows for integration of diverse features into the iterative process', 'We show the benefits of ClassyMap for bilingual lexicon induction: we report consistent improvements in a weakly supervised setup (500 seed translation pairs) on a benchmark with 28 language pairs.'], ['Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines', 'This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included', 'Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities', 'But what happens with speech translation, where the input is an audio signal? Can audio provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French).'], ['Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages', 'The keys lie in the assessment of data difficulty and model competence', 'We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage', 'Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty', 'Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed', 'Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.'], ['Exploiting natural language processing in the clinical domain requires de-identification, i.e., anonymization of personal information in texts', 'However, current research considers de-identification and downstream tasks, such as concept extraction, only in isolation and does not study the effects of de-identification on other tasks', 'In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction', 'In particular, we propose a stacked model with restricted access to privacy sensitive information and a multitask model', 'We set the new state of the art on benchmark datasets in English (96.1% F1 for de-identification and 88.9% F1 for concept extraction) and Spanish (91.4% F1 for concept extraction).'], ['In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task', 'We formulate the problem as a span prediction task, like in question answering: A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query', 'This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the question answering framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing question answering datasets can be used for data augmentation to improve the model’s generalization capability', 'Experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark and 87.5 (+2.5) F1 score on the GAP benchmark.'], ['The inability to correctly resolve rumours circulating online can have harmful real-world consequences', 'We present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification', 'We show that these estimates can be used to filter out model predictions likely to be erroneous so that these difficult instances can be prioritised by a human fact-checker', 'We propose two methods for uncertainty-based instance rejection, supervised and unsupervised', 'We also show how uncertainty estimates can be used to interpret model performance as a rumour unfolds.'], ['Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB)', 'It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge', 'The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs', 'Existing approaches are mostly inappropriate for this, as they depend on training data', 'However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch', 'We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users', 'We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy', 'In a user study, the annotation speed improves by 35% compared to annotating without interactive support; users report that they strongly prefer our system', 'An open-source and ready-to-use implementation based on the text annotation platform INCEpTION (https://inception-project.github.io) is made available.'], ['Transfer learning using ImageNet pre-trained models has been the de facto approach in a wide range of computer vision tasks', 'However, fine-tuning still requires task-specific training data', 'In this paper, we propose N3 (Neural Networks from Natural Language) - a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model', 'N3 leverages language descriptions to generate parameter adaptations as well as a new task-specific classification layer for a pre-trained neural network, effectively “fine-tuning” the network for a new task using only language descriptions as input', 'To the best of our knowledge, N3 is the first method to synthesize entire neural networks from natural language', 'Experimental results show that N3 can out-perform previous natural-language based zero-shot learning methods across 4 different zero-shot image classification benchmarks', 'We also demonstrate a simple method to help identify keywords in language descriptions leveraged by N3 when synthesizing model parameters.'], ['Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen', 'Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released', 'Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation', 'In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase', 'Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset', 'We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.'], ['Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding', 'Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English', 'While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances', 'Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection', 'In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations', 'Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly.'], ['We address the task of unsupervised Semantic Textual Similarity (STS) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings', 'We apply, extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level, including dimensionality reduction (Yin and Schütze, 2016), generalized Canonical Correlation Analysis (Rastogi et al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018)', 'Our sentence meta-embeddings set a new unsupervised State of The Art (SoTA) on the STS Benchmark and on the STS12-STS16 datasets, with gains of between 3.7% and 6.4% Pearson’s r over single-source systems.'], ['Transition-based parsers implemented with Pointer Networks have become the new state of the art in dependency parsing, excelling in producing labelled syntactic trees and outperforming graph-based models in this task', 'In order to further test the capabilities of these powerful neural networks on a harder NLP problem, we propose a transition system that, thanks to Pointer Networks, can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing', 'In addition, we enhance our approach with deep contextualized word embeddings extracted from BERT', 'The resulting system not only outperforms all existing transition-based models, but also matches the best fully-supervised accuracy to date on the SemEval 2015 Task 18 datasets among previous state-of-the-art graph-based parsers.'], ['Semantic similarity detection is a fundamental task in natural language understanding', 'Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks', 'There is currently no standard way of combining topics with pretrained contextual representations such as BERT', 'We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets', 'We find that the addition of topics to BERT helps particularly with resolving domain-specific cases.'], ['Aspect term extraction aims to extract aspect terms from review texts as opinion targets for sentiment analysis', 'One of the big challenges with this task is the lack of sufficient annotated data', 'While data augmentation is potentially an effective technique to address the above issue, it is uncontrollable as it may change aspect words and aspect labels unexpectedly', 'In this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence while preserving the original opinion targets and labels', 'We propose a masked sequence-to-sequence method for conditional augmentation of aspect term extraction', 'Unlike existing augmentation approaches, ours is controllable and allows to generate more diversified sentences', 'Experimental results confirm that our method alleviates the data scarcity problem significantly', 'It also effectively boosts the performances of several current models for aspect term extraction.'], ['Predicting the persuasiveness of arguments has applications as diverse as writing assistance, essay scoring, and advertising', 'While clearly relevant to the task, the personal characteristics of an argument’s source and audience have not yet been fully exploited toward automated persuasiveness prediction', 'In this paper, we model debaters’ prior beliefs, interests, and personality traits based on their previous activity, without dependence on explicit user profiles or questionnaires', 'Using a dataset of over 60,000 argumentative discussions, comprising more than three million individual posts collected from the subreddit r/ChangeMyView, we demonstrate that our modeling of debater’s characteristics enhances the prediction of argument persuasiveness as well as of debaters’ resistance to persuasion.'], ['An educated and informed consumption of media content has become a challenge in modern times', 'With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in “echo chambers” and may fall prey to fake news and disinformation, lacking easy access to dissenting views', 'We suggest a novel task aiming to alleviate some of these concerns – that of detecting articles that most effectively counter the arguments – and not just the stance – made in a given text', 'We study this problem in the context of debate speeches', 'Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it', 'We provide a large dataset of 3,685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community', 'We explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research', 'All data collected during this work is freely available for research.'], ['Neural network-based sequence-to-sequence (seq2seq) models strongly suffer from the low-diversity problem when it comes to open-domain dialogue generation', 'As bland and generic utterances usually dominate the frequency distribution in our daily chitchat, avoiding them to generate more interesting responses requires complex data filtering, sampling techniques or modifying the training objective', 'In this paper, we propose a new perspective to diversify dialogue generation by leveraging non-conversational text', 'Compared with bilateral conversations, non-conversational text are easier to obtain, more diverse and cover a much broader range of topics', 'We collect a large-scale non-conversational corpus from multi sources including forum comments, idioms and book snippets', 'We further present a training paradigm to effectively incorporate these text via iterative back translation', 'The resulting model is tested on two conversational datasets from different domains and is shown to produce significantly more diverse responses without sacrificing the relevance with context.'], ['The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations', 'In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs', 'Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0', 'These conversations contain in-depth discussions on related topics and natural transition between multiple topics', 'To facilitate the following research on this corpus, we provide several benchmark models', 'Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research', 'Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation', 'The corpus and benchmark models are publicly available.'], ['A Dialogue State Tracker (DST) is a core component of a modular task-oriented dialogue system', 'Tremendous progress has been made in recent years', 'However, the major challenges remain', 'The state-of-the-art accuracy for DST is below 50% for a multi-domain dialogue task', 'A learnable DST for any new domain requires a large amount of labeled in-domain data and training from scratch', 'In this paper, we propose a Meta-Reinforced Multi-Domain State Generator (MERET)', 'Our first contribution is to improve the DST accuracy', 'We enhance a neural model based DST generator with a reward manager, which is built on policy gradient reinforcement learning (RL) to fine-tune the generator', 'With this change, we are able to improve the joint accuracy of DST from 48.79% to 50.91% on the MultiWOZ corpus', 'Second, we explore to train a DST meta-learning model with a few domains as source domains and a new domain as target domain', 'We apply the model-agnostic meta-learning algorithm (MAML) to DST and the obtained meta-learning model is used for new domain adaptation', 'Our experimental results show this solution is able to outperform the traditional training approach with extremely less training data in target domain.'], ['Based on the recently proposed transferable dialogue state generator (TRADE) that predicts dialogue states from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation', 'By enabling the model to learn a better representation of the long dialogue context, our approaches attempt to solve the problem that the performance of the baseline significantly drops when the input dialogue context sequence is long', 'In our experiments, our proposed model achieves a 7.03% relative improvement over the baseline, establishing a new state-of-the-art joint goal accuracy of 52.04% on the MultiWOZ 2.0 dataset.'], ['Generating fluent and informative responses is of critical importance for task-oriented dialogue systems', 'Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation', 'There are at least two shortcomings with such approaches', 'First, the inherent structures of multi-domain dialogue acts are neglected', 'Second, the semantic associations between acts and responses are not taken into account for response generation', 'To address these issues, we propose a neural co-generation model that generates dialogue acts and responses concurrently', 'Unlike those pipeline approaches, our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed', 'We train the two modules jointly using an uncertainty loss to adjust their task weights adaptively', 'Extensive experiments are conducted on the large-scale MultiWOZ dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations.'], ['Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data', 'In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences', 'In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer', 'Specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation', 'In a denoising auto-encoding manner, we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously', 'In this way, this model is endowed with the ability to automatically predict the style relevance of each output word', 'Then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer', 'Particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms', 'Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation.'], ['The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation', 'Recent studies propose various models to encode graph structure', 'However, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way', 'In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes', 'Experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of AMR-to-text generation and syntax-based neural machine translation.'], ['The neural attention model has achieved great success in data-to-text generation tasks', 'Though usually excelling at producing fluent text, it suffers from the problem of information missing, repetition and “hallucination”', 'Due to the black-box nature of the neural attention architecture, avoiding these problems in a systematic way is non-trivial', 'To address this concern, we propose to explicitly segment target text into fragment units and align them with their data correspondences', 'The segmentation and correspondence are jointly learned as latent variables without any human annotations', 'We further impose a soft statistical constraint to regularize the segmental granularity', 'The resulting architecture maintains the same expressive power as neural attention models, while being able to generate fully interpretable outputs with several times less computational cost', 'On both E2E and WebNLG benchmarks, we show the proposed model consistently outperforms its neural attention counterparts.'], ['Visual question answering aims to answer the natural language question about a given image', 'Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question', 'To simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question, we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual and textual advantages', 'The DC-GCN model consists of three parts: an I-GCN module to capture the relations between objects in an image, a Q-GCN module to capture the syntactic dependency relations between words in a question, and an attention alignment module to align image representations and question representations', 'Experimental results show that our model achieves comparable performance with the state-of-the-art approaches.'], ['We introduce a new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering', 'The MN-GMN uses graph structure with different region features as node attributes and applies a recently proposed powerful graph neural network model, Graph Network (GN), to reason about objects and their interactions in an image', 'The input module of the MN-GMN generates a set of visual features plus a set of encoded region-grounded captions (RGCs) for the image', 'The RGCs capture object attributes and their relationships', 'Two GNs are constructed from the input module using the visual features and encoded RGCs', 'Each node of the GNs iteratively computes a question-guided contextualized representation of the visual/textual information assigned to it', 'Then, to combine the information from both GNs, the nodes write the updated representations to an external spatial memory', 'The final states of the memory cells are fed into an answer module to predict an answer', 'Experiments show MN-GMN rivals the state-of-the-art models on Visual7W, VQA-v2.0, and CLEVR datasets.'], ['We propose a novel large-scale referring expression recognition dataset, Refer360°, consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360° scenes', 'Refer360° differs from existing related datasets in three ways', 'First, we propose a more realistic scenario where instructors and the followers have partial, yet dynamic, views of the scene – followers continuously modify their field-of-view (FoV) while interpreting instructions that specify a final target location', 'Second, instructions to find the target location consist of multiple steps for followers who will start at random FoVs', 'As a result, intermediate instructions are strongly grounded in object references, and followers must identify intermediate FoVs to find the final target location correctly', 'Third, the target locations are neither restricted to predefined objects nor chosen by annotators; instead, they are distributed randomly across scenes', 'This “point anywhere” approach leads to more linguistically complex instructions, as shown in our analyses', 'Our examination of the dataset shows that Refer360° manifests linguistically rich phenomena in a language grounding task that poses novel challenges for computational modeling of language, vision, and navigation.'], ['Pretrained language models are now ubiquitous in Natural Language Processing', 'Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages', 'This makes practical use of such models –in all languages except English– very limited', 'In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks', 'We show that the use of web crawled data is preferable to the use of Wikipedia data', 'More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB)', 'Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.'], ['Advances in variational inference enable parameterisation of probabilistic models by deep neural networks', 'This combines the statistical transparency of the probabilistic modelling framework with the representational power of deep learning', 'Yet, due to a problem known as posterior collapse, it is difficult to estimate such models in the context of language modelling effectively', 'We concentrate on one such model, the variational auto-encoder, which we argue is an important building block in hierarchical probabilistic models of language', 'This paper contributes a sober view of the problem, a survey of techniques to address it, novel techniques, and extensions to the model', 'To establish a ranking of techniques, we perform a systematic comparison using Bayesian optimisation and find that many techniques perform reasonably similar, given enough resources', 'Still, a favourite can be named based on convenience', 'We also make several empirical observations and recommendations of best practices that should help researchers interested in this exciting field.'], ['The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models', 'We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations', 'Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space', 'By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it', 'While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.'], ['Simplified Chinese to Traditional Chinese character conversion is a common preprocessing step in Chinese NLP', 'Despite this, current approaches have insufficient performance because they do not take into account that a simplified Chinese character can correspond to multiple traditional characters', 'Here, we propose a model that can disambiguate between mappings and convert between the two scripts', 'The model is based on subword segmentation, two language models, as well as a method for mapping between subword sequences', 'We further construct benchmark datasets for topic classification and script conversion', 'Our proposed method outperforms previous Chinese Character conversion approaches by 6 points in accuracy', 'These results are further confirmed in a downstream application, where 2kenize is used to convert pretraining dataset for topic classification', 'An error analysis reveals that our method’s particular strengths are in dealing with code mixing and named entities.'], ['We present the first study that examines the evolution of morphological families, i.e., sets of morphologically related words such as “trump”, “antitrumpism”, and “detrumpify”, in social media', 'We introduce the novel task of Morphological Family Expansion Prediction (MFEP) as predicting the increase in the size of a morphological family', 'We create a ten-year Reddit corpus as a benchmark for MFEP and evaluate a number of baselines on this benchmark', 'Our experiments demonstrate very good performance on MFEP.'], ['Historical text normalization, the task of mapping historical word forms to their modern counterparts, has recently attracted a lot of interest (Bollmann, 2019; Tang et al., 2018; Lusetti et al., 2018; Bollmann et al., 2018;Robertson and Goldwater, 2018; Bollmannet al., 2017; Korchagina, 2017)', 'Yet, virtually all approaches suffer from the two limitations: 1) They consider a fully supervised setup, often with impractically large manually normalized datasets; 2) Normalization happens on words in isolation', 'By utilizing a simple generative normalization model and obtaining powerful contextualization from the target-side language model, we train accurate models with unlabeled historical data', 'In realistic training scenarios, our approach often leads to reduction in manually normalized data at the same accuracy levels.'], ['Question answering and conversational systems are often baffled and need help clarifying certain ambiguities', 'However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions', 'In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange', 'The framework utilises a neural network based architecture for classifying clarification questions', 'It is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall', 'We quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering', 'The final dataset, ClarQ, consists of ~2M examples distributed across 173 domains of stackexchange', 'We release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems.'], ['The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of domain-specific information available in FAQ sites', 'We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs', 'The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing', 'Compared to previous work, DoQA comprises well-defined information needs, leading to more coherent and natural conversations with less factoid questions and is multi-domain', 'In addition, we introduce a more realistic information retrieval (IR) scenario where the system needs to find the answer in any of the FAQ documents', 'The results of an existing, strong, system show that, thanks to transfer learning from a Wikipedia QA dataset and fine tuning on a single FAQ domain, it is possible to build high quality conversational QA systems for FAQs without in-domain training data', 'The good results carry over into the more challenging IR scenario', 'In both cases, there is still ample room for improvement, as indicated by the higher human upperbound.'], ['Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets', 'Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging', 'In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress', 'We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area', 'MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese', 'MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average', 'We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA', 'In all cases, transfer results are shown to be significantly behind training-language performance.'], ['Multiple-choice question answering (MCQA) is one of the most challenging tasks in machine reading comprehension since it requires more advanced reading comprehension skills such as logical reasoning, summarization, and arithmetic operations', 'Unfortunately, most existing MCQA datasets are small in size, which increases the difficulty of model learning and generalization', 'To address this challenge, we propose a multi-source meta transfer (MMT) for low-resource MCQA', 'In this framework, we first extend meta learning by incorporating multiple training sources to learn a generalized feature representation across domains', 'To bridge the distribution gap between training sources and the target, we further introduce the meta transfer that can be integrated into the multi-source meta training', 'More importantly, the proposed MMT is independent of backbone language models', 'Extensive experiments demonstrate the superiority of MMT over state-of-the-arts, and continuous improvements can be achieved on different backbone networks on both supervised and unsupervised domain adaptation settings.'], ['Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims', 'This paper presents Kernel Graph Attention Network (KGAT), which conducts more fine-grained fact verification with kernel-based attentions', 'Given a claim and a set of potential evidence sentences that form an evidence graph, KGAT introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct fine-grained evidence propagation in the graph, into Graph Attention Networks for more accurate fact verification', 'KGAT achieves a 70.38% FEVER score and significantly outperforms existing fact verification models on FEVER, a large-scale benchmark for fact verification', 'Our analyses illustrate that, compared to dot-product attentions, the kernel-based attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph, which is the main source of KGAT’s effectiveness', 'All source codes of this work are available at https://github.com/thunlp/KernelGAT.'], ['Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims', 'A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process – generating justifications for verdicts on claims', 'This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction', 'Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system', 'The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.'], ['The discovery of supporting evidence for addressing complex mathematical problems is a semantically challenging task, which is still unexplored in the field of natural language processing for mathematical text', 'The natural language premise selection task consists in using conjectures written in both natural language and mathematical formulae to recommend premises that most likely will be useful to prove a particular statement', 'We propose an approach to solve this task as a link prediction problem, using Deep Convolutional Graph Neural Networks', 'This paper also analyses how different baselines perform in this task and shows that a graph structure can provide higher F1-score, especially when considering multi-hop premise selection.'], ['We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them', 'An existing rationale for such research is based on the lack of parallel data for many of the world’s languages', 'However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice', 'We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting', 'We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices', 'Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.'], ['Measuring what linguistic information is encoded in neural models of language has become popular in NLP', 'Researchers approach this enterprise by training “probes”—supervised models designed to extract linguistic structure from another model’s output', 'One such probe is the structural probe (Hewitt and Manning, 2019), designed to quantify the extent to which syntactic information is encoded in contextualised word representations', 'The structural probe has a novel design, unattested in the parsing literature, the precise benefit of which is not immediately obvious', 'To explore whether syntactic probes would do better to make use of existing techniques, we compare the structural probe to a more traditional parser with an identical lightweight parameterisation', 'The parser outperforms structural probe on UUAS in seven of nine analysed languages, often by a substantial amount (e.g', 'by 11.1 points in English)', 'Under a second less common metric, however, there is the opposite trend—the structural probe outperforms the parser', 'This begs the question: which metric should we prefer?'], ['It has been exactly a decade since the first establishment of SPMRL, a research initiative unifying multiple research efforts to address the peculiar challenges of Statistical Parsing for Morphologically-Rich Languages (MRLs)', 'Here we reflect on parsing MRLs in that decade, highlight the solutions and lessons learned for the architectural, modeling and lexical challenges in the pre-neural era, and argue that similar challenges re-emerge in neural architectures for MRLs', 'We then aim to offer a climax, suggesting that incorporating symbolic ideas proposed in SPMRL terms into nowadays neural architectures has the potential to push NLP for MRLs to a new level', 'We sketch a strategies for designing Neural Models for MRLs (NMRL), and showcase preliminary support for these strategies via investigating the task of multi-tagging in Hebrew, a morphologically-rich, high-fusion, language.'], ['Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention', 'This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives', 'Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations', 'However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity', 'This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.'], ['In addition to the traditional task of machines answering questions, question answering (QA) research creates interesting, challenging questions that help systems how to answer questions and reveal the best systems', 'We argue that creating a QA dataset—and the ubiquitous leaderboard that goes with it—closely resembles running a trivia tournament: you write questions, have agents (either humans or machines) answer the questions, and declare a winner', 'However, the research community has ignored the hard-learned lessons from decades of the trivia community creating vibrant, fair, and effective question answering competitions', 'After detailing problems with existing QA datasets, we outline the key lessons—removing ambiguity, discriminating skill, and adjudicating disputes—that can transfer to QA research and how they might be implemented.'], ['Distributional semantic models have become a mainstay in NLP, providing useful features for downstream tasks', 'However, assessing long-term progress requires explicit long-term goals', 'In this paper, I take a broad linguistic perspective, looking at how well current models can deal with various semantic challenges', 'Given stark differences between models proposed in different subfields, a broad perspective is needed to see how we could integrate them', 'I conclude that, while linguistic insights can guide the design of model architectures, future progress will require balancing the often conflicting demands of linguistic expressiveness and computational tractability.'], ['Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community', 'In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation', 'Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning', 'The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features', 'During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences', 'We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics', 'The code of our paper has been made publicly available.'], ['The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language Generation shared tasks with the goal of exploring approaches to surface realization from Universal-Dependency-like trees to surface strings for several languages', 'In the 2018 shared task there was very little difference in the absolute performance of systems trained with and without additional, synthetically created data, and a new rule prohibiting the use of synthetic data was introduced for the 2019 shared task', 'Contrary to the findings of the 2018 shared task, we show, in experiments on the English 2018 dataset, that the use of synthetic data can have a substantial positive effect – an improvement of almost 8 BLEU points for a previously state-of-the-art system', 'We analyse the effects of synthetic data, and we argue that its use should be encouraged rather than prohibited so that future research efforts continue to explore systems that can take advantage of such data.'], ['We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives', 'Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence', 'Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings', 'Notably this allows us to consider a large number of candidates for the next sentence during training', 'We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.'], ['In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries', 'The addition of cross-sentence argument candidates imposes great challenges for modeling', 'To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion', 'Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline', 'We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements', 'It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.'], ['Machine reading is an ambitious goal in NLP that subsumes a wide range of text understanding capabilities', 'Within this broad framework, we address the task of machine reading the time of historical events, compile datasets for the task, and develop a model for tackling it', 'Given a brief textual description of an event, we show that good performance can be achieved by extracting relevant sentences from Wikipedia, and applying a combination of task-specific and general-purpose feature embeddings for the classification', 'Furthermore, we establish a link between the historical event ordering task and the event focus time task from the information retrieval literature, showing they also provide a challenging test case for machine reading algorithms.'], ['Unsupervised relation extraction (URE) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (KBs)', 'URE methods can be categorised into generative and discriminative approaches, which rely either on hand-crafted features or surface form', 'However, we demonstrate that by using only named entities to induce relation types, we can outperform existing methods on two popular datasets', 'We conduct a comparison and evaluation of our findings with other URE techniques, to ascertain the important features in URE', 'We conclude that entity types provide a strong inductive bias for URE.'], ['Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph', 'It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections', 'In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles', 'We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources', 'We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE', 'Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models', 'Our data and code are publicly available at https://github.com/allenai/SciREX .'], ['We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems', 'Our approach exploits the characteristic structure of training corpora related to so-called “trigger” words, which are responsible for flipping the answer in pronoun disambiguation', 'We achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions', 'To this end, we leverage a mutual exclusive loss regularized by a contrastive margin', 'Our architecture is based on the recently introduced transformer networks, BERT, that exhibits strong performance on many NLP benchmarks', 'Empirical results show that our method alleviates the limitation of current supervised approaches for commonsense reasoning', 'This study opens up avenues for exploiting inexpensive self-supervision to achieve performance gain in commonsense reasoning tasks.'], ['Deep attention models have advanced the modelling of sequential data across many domains', 'For language modelling in particular, the Transformer-XL — a Transformer augmented with a long-range memory of past activations — has been shown to be state-of-the-art across a variety of well-studied benchmarks', 'The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors', 'However it is unclear whether this is necessary', 'We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.'], ['Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc', 'Similar problems have been studied extensively for other forms of data, such as images and videos', 'However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved)', 'Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics', 'A new mutual information upper bound is derived and leveraged to measure dependence between style and content', 'By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces', 'Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.'], ['We consider a task based on CVPR 2018 challenge dataset on advertisement (Ad) understanding', 'The task involves detecting the viewer’s interpretation of an Ad image captured as text', 'Recent results have shown that the embedded scene-text in the image holds a vital cue for this task', 'Motivated by this, we fine-tune the base BERT model for a sentence-pair classification task', 'Despite utilizing the scene-text as the only source of visual information, we could achieve a hit-or-miss accuracy of 84.95% on the challenge test data', 'To enable BERT to process other visual information, we append image captions to the scene-text', 'This achieves an accuracy of 89.69%, which is an improvement of 4.7%', 'This is the best reported result for this task.'], ['We present InstaMap, an instance-based method for learning projection-based cross-lingual word embeddings', 'Unlike prior work, it deviates from learning a single global linear projection', 'InstaMap is a non-parametric model that learns a non-linear projection by iteratively: (1) finding a globally optimal rotation of the source embedding space relying on the Kabsch algorithm, and then (2) moving each point along an instance-specific translation vector estimated from the translation vectors of the point’s nearest neighbours in the training dictionary', 'We report performance gains with InstaMap over four representative state-of-the-art projection-based models on bilingual lexicon induction across a set of 28 diverse language pairs', 'We note prominent improvements, especially for more distant language pairs (i.e., languages with non-isomorphic monolingual spaces).'], ['We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models', 'Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment', 'The protocol is model-agnostic and useful for a variety of tasks', 'Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the SCAN dataset and 16% on a semantic parsing task', 'Applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages.'], ['When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas', 'The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query', 'We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder', 'On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement', 'Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6% on the Spider leaderboard', 'In addition, we observe qualitative improvements in the model’s understanding of schema linking and alignment', 'Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql.'], ['Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language', 'However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly', 'This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TacoLM, a temporal common sense language model', 'Our method is shown to give quality predictions of various dimensions of temporal common sense (on UDST and a newly collected dataset from RealNews)', 'It also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the standard BERT', 'Thus, it will be an important component of temporal NLP.'], ['Large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability', 'We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding', 'Our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones.'], ['Natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text', 'However, evaluation results on existing data sets are seldom reported by taking the timestamp of the document into account', 'We analyze and propose methods that make better use of temporally-diverse training data, with a focus on the task of named entity recognition', 'To support these experiments, we introduce a novel data set of English tweets annotated with named entities', 'We empirically demonstrate the effect of temporal drift on performance, and how the temporal information of documents can be used to obtain better models compared to those that disregard temporal information', 'Our analysis gives insights into why this information is useful, in the hope of informing potential avenues of improvement for named entity recognition as well as other NLP tasks under similar experimental setups.'], ['We tackle the task of building supervised event trigger identification models which can generalize better across domains', 'Our work leverages the adversarial domain adaptation (ADA) framework to introduce domain-invariance', 'ADA uses adversarial training to construct representations that are predictive for trigger identification, but not predictive of the example’s domain', 'It requires no labeled data from the target domain, making it completely unsupervised', 'Experiments with two domains (English literature and news) show that ADA leads to an average F1 score improvement of 3.9 on out-of-domain data', 'Our best performing model (BERT-A) reaches 44-49 F1 across both domains, using no labeled target data', 'Preliminary experiments reveal that finetuning on 1% labeled data, followed by self-training leads to substantial improvement, reaching 51.5 and 67.2 F1 on literature and news respectively.'], ['Approaches to Grounded Language Learning are commonly focused on a single task-based final performance measure which may not depend on desirable properties of the learned hidden representations, such as their ability to predict object attributes or generalize to unseen situations', 'To remedy this, we present GroLLA, an evaluation framework for Grounded Language Learning with Attributes based on three sub-tasks: 1) Goal-oriented evaluation; 2) Object attribute prediction evaluation; and 3) Zero-shot evaluation', 'We also propose a new dataset CompGuessWhat?! as an instance of this framework for evaluating the quality of learned neural representations, in particular with respect to attribute grounding', 'To this end, we extend the original GuessWhat?! dataset by including a semantic layer on top of the perceptual one', 'Specifically, we enrich the VisualGenome scene graphs associated with the GuessWhat?! images with several attributes from resources such as VISA and ImSitu', 'We then compare several hidden state representations from current state-of-the-art approaches to Grounded Language Learning', 'By using diagnostic classifiers, we show that current models’ learned representations are not expressive enough to encode object attributes (average F1 of 44.27)', 'In addition, they do not learn strategies nor representations that are robust enough to perform well when novel scenes or objects are involved in gameplay (zero-shot best accuracy 50.06%).'], ['This work deals with the challenge of learning and reasoning over language and vision data for the related downstream tasks such as visual question answering (VQA) and natural language for visual reasoning (NLVR)', 'We design a novel cross-modality relevance module that is used in an end-to-end framework to learn the relevance representation between components of various input modalities under the supervision of a target task, which is more generalizable to unobserved data compared to merely reshaping the original representation space', 'In addition to modeling the relevance between the textual entities and visual entities, we model the higher-order relevance between entity relations in the text and object relations in the image', 'Our proposed approach shows competitive performance on two different language and vision tasks using public benchmarks and improves the state-of-the-art published results', 'The learned alignments of input spaces and their relevance representations by NLVR task boost the training efficiency of VQA task.'], ['We explore learning web-based tasks from a human teacher through natural language explanations and a single demonstration', 'Our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context, rather than mapping explanations to demonstrations', 'By leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations, we ensure that all considered interpretations are consistent with executable actions in any context, thus simplifying the problem of search over logical forms', 'We present a dataset of explanations paired with demonstrations for web-based tasks', 'Our methods show better task completion rates than a supervised semantic parsing baseline (40% relative improvement on average), and are competitive with simple exploration-and-demonstration based methods, while requiring no exploration of the environment', 'In learning to align explanations with demonstrations, basic properties of natural language syntax emerge as learned behavior', 'This is an interesting example of pragmatic language acquisition without any linguistic annotation.'], ['We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language', 'Our starting point is a language model that has been trained on generic, not task-specific language data', 'We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model', 'We introduce a new way for combining the two types of learning based on the idea of reranking language model samples, and show that this method outperforms others in communicating with humans in a visual referential communication task', 'Finally, we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them.'], ['Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation', 'To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search', 'We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers', 'Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing', 'Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware', 'Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device)', 'When running WMT’14 translation task on Raspberry Pi-4, HAT can achieve 3× speedup, 3.7× smaller size over baseline Transformer; 2.7× speedup, 3.6× smaller size over Evolved Transformer with 12,041× less search cost and no performance loss', 'HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.'], ['Recent work has questioned the importance of the Transformer’s multi-headed attention for achieving high translation quality', 'We push further in this direction by developing a “hard-coded” attention variant without any learned parameters', 'Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs', 'However, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention', 'Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer', 'Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models.'], ['Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers', 'We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning', 'Word embeddings play an important role in transfer learning, particularly if they are properly aligned', 'Although transfer learning can be performed without embeddings, results are sub-optimal', 'In contrast, transferring only the embeddings but nothing else yields catastrophic results', 'We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains', 'Finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.'], ['Most data selection research in machine translation focuses on improving a single domain', 'We perform data selection for multiple domains at once', 'This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches', 'Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain', 'In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.'], ['Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men', 'In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender', 'The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge', 'Rather than attempt to create a ‘balanced’ dataset, we use transfer learning on a small set of trusted, gender-balanced examples', 'This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch', 'A known pitfall of transfer learning on new domains is ‘catastrophic forgetting’, which we address at adaptation and inference time', 'During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction', 'At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU', 'We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.'], ['Machine translation has an undesirable propensity to produce “translationese” artifacts, which can lead to higher BLEU scores while being liked less by human raters', 'Motivated by this, we model translationese and original (i.e', 'natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train a sentence-level classifier to distinguish translationese from original target text, and use this classifier to tag the training data for an NMT model', 'Using this technique we bias the model to produce more natural outputs at test time, yielding gains in human evaluation scores on both accuracy and fluency', 'Additionally, we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score, increasing it while decreasing human-rated quality', 'We analyze these outputs using metrics measuring the degree of translationese, and present an analysis of the volatility of heuristic-based train-data tagging.'], ['The notion of “in-domain data” in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality', 'In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems', 'We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision – suggesting a simple data-driven definition of domains in textual data', 'We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data', 'We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.'], ['We present Neural Machine Translation (NMT) training using document-level metrics with batch-level documents', 'Previous sequence-objective approaches to NMT training focus exclusively on sentence-level metrics like sentence BLEU which do not correspond to the desired evaluation metric, typically document BLEU', 'Meanwhile research into document-level NMT training focuses on data or model architecture rather than training procedure', 'We find that each of these lines of research has a clear space in it for the other, and propose merging them with a scheme that allows a document-level evaluation metric to be used in the NMT training objective', 'We first sample pseudo-documents from sentence samples', 'We then approximate the expected document BLEU gradient with Monte Carlo sampling for use as a cost function in Minimum Risk Training (MRT)', 'This two-level sampling procedure gives NMT performance gains over sequence MRT and maximum-likelihood training', 'We demonstrate that training is more robust for document-level metrics than with sequence metrics', 'We further demonstrate improvements on NMT with TER and Grammatical Error Correction (GEC) using GLEU, both metrics used at the document level for evaluations.'], ['Variational Neural Machine Translation (VNMT) is an attractive framework for modeling the generation of target translations, conditioned not only on the source sentence but also on some latent random variables', 'The latent variable modeling may introduce useful statistical dependencies that can improve translation accuracy', 'Unfortunately, learning informative latent variables is non-trivial, as the latent space can be prohibitively large, and the latent codes are prone to be ignored by many translation models at training time', 'Previous works impose strong assumptions on the distribution of the latent code and limit the choice of the NMT architecture', 'In this paper, we propose to apply the VNMT framework to the state-of-the-art Transformer and introduce a more flexible approximate posterior based on normalizing flows', 'We demonstrate the efficacy of our proposal under both in-domain and out-of-domain conditions, significantly outperforming strong baselines.'], ['This work treats the paradigm discovery problem (PDP), the task of learning an inflectional morphological system from unannotated sentences', 'We formalize the PDP and develop evaluation metrics for judging systems', 'Using currently available resources, we construct datasets for the task', 'We also devise a heuristic benchmark for the PDP and report empirical results on five diverse languages', 'Our benchmark system first makes use of word embeddings and string similarity to cluster forms by cell and by paradigm', 'Then, we bootstrap a neural transducer on top of the clustered data to predict words to realize the empty paradigm slots', 'An error analysis of our system suggests clustering by cell across different inflection classes is the most pressing challenge for future work.'], ['Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted)', 'Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis', 'We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches', 'We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries', 'Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification.'], ['In this theme paper, we focus on Automated Writing Evaluation (AWE), using Ellis Page’s seminal 1966 paper to frame the presentation', 'We discuss some of the current frontiers in the field and offer some thoughts on the emergent uses of this technology.'], ['Building on Petroni et al', '2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs)', '(1) Negation', 'We find that PLMs do not distinguish between negated (‘‘Birds cannot [MASK]”) and non-negated (‘‘Birds can [MASK]”) cloze questions', '(2) Mispriming', 'Inspired by priming methods in human psychology, we add “misprimes” to cloze questions (‘‘Talk? Birds can [MASK]”)', 'We find that PLMs are easily distracted by misprimes', 'These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.'], ['The field of natural language processing is experiencing a period of unprecedented growth, and with it a surge of published papers', 'This represents an opportunity for us to take stock of how we cite the work of other researchers, and whether this growth comes at the expense of “forgetting” about older literature', 'In this paper, we address this question through bibliographic analysis', 'By looking at the age of outgoing citations in papers published at selected ACL venues between 2010 and 2019, we find that there is indeed a tendency for recent papers to cite more recent work, but the rate at which papers older than 15 years are cited has remained relatively stable.'], ['Most NLP models today treat language as universal, even though socio- and psycholingustic research shows that the communicated message is influenced by the characteristics of the speaker as well as the target audience', 'This paper surveys the landscape of personalization in natural language processing and related fields, and offers a path forward to mitigate the decades of deviation of the NLP tools from sociolingustic findings, allowing to flexibly process the “natural” language of each user rather than enforcing a uniform NLP treatment', 'It outlines a possible direction to incorporate these aspects into neural NLP models by means of socially contextual personalization, and proposes to shift the focus of our evaluation strategies accordingly.'], ['Many tasks aim to measure machine reading comprehension (MRC), often focusing on question types presumed to be difficult', 'Rarely, however, do task designers start by considering what systems should in fact comprehend', 'In this paper we make two key contributions', 'First, we argue that existing approaches do not adequately define comprehension; they are too unsystematic about what content is tested', 'Second, we present a detailed definition of comprehension—a “Template of Understanding”—for a widely useful class of texts, namely short narratives', 'We then conduct an experiment that strongly suggests existing systems are not up to the task of narrative understanding as we define it.'], ['Disparities in authorship and citations across genders can have substantial adverse consequences not just on the disadvantaged gender, but also on the field of study as a whole', 'In this work, we examine female first author percentages and the citations to their papers in Natural Language Processing', 'We find that only about 29% of first authors are female and only about 25% of last authors are female', 'Notably, this percentage has not improved since the mid 2000s', 'We also show that, on average, female first authors are cited less than male first authors, even when controlling for experience and area of research', 'We hope that recording citation and participation gaps across demographic groups will improve awareness of gender gaps and encourage more inclusiveness and fairness in research.'], ['We present BART, a denoising autoencoder for pretraining sequence-to-sequence models', 'BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text', 'It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes', 'We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token', 'BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks', 'It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE', 'BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining', 'We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.'], ['Text generation has made significant advances in the last few years', 'Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment', 'We propose BLEURT, a learned evaluation metric for English based on BERT', 'BLEURT can model human judgment with a few thousand possibly biased training examples', 'A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize', 'BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set', 'In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.'], ['Large-scale pre-trained language model such as BERT has achieved great success in language understanding tasks', 'However, it remains an open question how to utilize BERT for language generation', 'In this paper, we present a novel approach, Conditional Masked Language Modeling (C-MLM), to enable the finetuning of BERT on target generation tasks', 'The finetuned BERT (teacher) is exploited as extra supervision to improve conventional Seq2Seq models (student) for better text generation performance', 'By leveraging BERT’s idiosyncratic bidirectional nature, distilling knowledge learned in BERT can encourage auto-regressive Seq2Seq models to plan ahead, imposing global sequence-level supervision for coherent text generation', 'Experiments show that the proposed approach significantly outperforms strong Transformer baselines on multiple language generation tasks such as machine translation and text summarization', 'Our proposed model also achieves new state of the art on IWSLT German-English and English-Vietnamese MT datasets.'], ['Neural networks lack the ability to reason about qualitative physics and so cannot generalize to scenarios and tasks unseen during training', 'We propose ESPRIT, a framework for commonsense reasoning about qualitative physics in natural language that generates interpretable descriptions of physical events', 'We use a two-step approach of first identifying the pivotal physical events in an environment and then generating natural language descriptions of those events using a data-to-text approach', 'Our framework learns to generate explanations of how the physical simulation will causally evolve so that an agent or a human can easily reason about a solution using those interpretable descriptions', 'Human evaluations indicate that ESPRIT produces crucial fine-grained details and has high coverage of physical concepts compared to even human annotations', 'Dataset, code and documentation are available at https://github.com/salesforce/esprit.'], ['We present a novel iterative, edit-based approach to unsupervised sentence simplification', 'Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation', 'Then, we iteratively perform word and phrase-level edits on the complex sentence', 'Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable', 'Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches.'], ['Neural natural language generation (NLG) models have recently shown remarkable progress in fluency and coherence', 'However, existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language', 'In this paper, we suggest a new NLG task where a model is tasked with generating natural language statements that can be logically entailed by the facts in an open-domain semi-structured table', 'To facilitate the study of the proposed logical NLG problem, we use the existing TabFact dataset~(CITATION) featured with a wide range of logical/symbolic inferences as our testbed, and propose new automatic metrics to evaluate the fidelity of generation models w.r.t', 'logical inference', 'The new task poses challenges to the existing monotonic generation frameworks due to the mismatch between sequence order and logical order', 'In our experiments, we comprehensively survey different generation architectures (LSTM, Transformer, Pre-Trained LM) trained with different algorithms (RL, Adversarial Training, Coarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained LM can significantly boost both the fluency and logical fidelity metrics, 2) RL and Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine generation can help partially alleviate the fidelity issue while maintaining high language fluency', 'The code and data are available at https://github.com/wenhuchen/LogicNLG.'], ['The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles', 'To evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, Newsela and Wikipedia', 'We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity', 'Experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in F1', 'We apply our CRF aligner to construct two new text simplification datasets, Newsela-Auto and Wiki-Auto, which are much larger and of better quality compared to the existing datasets', 'A Transformer-based seq2seq model trained on our datasets establishes a new state-of-the-art for text simplification in both automatic and human evaluation.'], ['Different texts shall by nature correspond to different number of keyphrases', 'This desideratum is largely missing from existing neural keyphrase generation models', 'In this study, we address this problem from both modeling and evaluation perspectives', 'We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences', 'Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states', 'In contrast to previous approaches, our model is capable of generating diverse keyphrases and controlling number of outputs', 'We further propose two evaluation metrics tailored towards the variable-number generation', 'We also introduce a new dataset StackEx that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks', 'With both previous and new evaluation metrics, our model outperforms strong baselines on all datasets.'], ['We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence', 'Our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity with the context, which could include shared commonsense or world knowledge between the speaker and the listener', 'While prior works on sarcasm generation predominantly focus on context incongruity, we show that combining valence reversal and semantic incongruity based on the commonsense knowledge generates sarcasm of higher quality', 'Human evaluation shows that our system generates sarcasm better than humans 34% of the time, and better than a reinforced hybrid baseline 90% of the time.'], ['The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs', 'As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs', 'We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information', 'In particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a', 'views) of input graphs', 'The losses are then back-propagated to better calibrate our model via multi-task training', 'Experiments on two benchmarks for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline.'], ['Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions', 'For example, a victim of a die event is likely to be a victim of an attack event in the same sentence', 'In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence', 'OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder', 'At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions', 'Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks', 'In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.'], ['Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions', 'This is problematic when the information needed to recognize an event argument is spread across multiple sentences', 'We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers', 'We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance', 'To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader', 'We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work', 'We also report findings on the relationship between context length and neural model performance on the task.'], ['This paper studies the task of Relation Extraction (RE) that aims to identify the semantic relations between two entity mentions in text', 'In the deep learning models for RE, it has been beneficial to incorporate the syntactic structures from the dependency trees of the input sentences', 'In such models, the dependency trees are often used to directly structure the network architectures or to obtain the dependency relations between the word pairs to inject the syntactic information into the models via multi-task learning', 'The major problem with these approaches is the lack of generalization beyond the syntactic structures in the training data or the failure to capture the syntactic importance of the words for RE', 'In order to overcome these issues, we propose a novel deep learning model for RE that uses the dependency trees to extract the syntax-based importance scores for the words, serving as a tree representation to introduce syntactic information into the models with greater generalization', 'In particular, we leverage Ordered-Neuron Long-Short Term Memory Networks (ON-LSTM) to infer the model-based importance scores for RE for every word in the sentences that are then regulated to be consistent with the syntax-based scores to enable syntactic information injection', 'We perform extensive experiments to demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance on three RE benchmark datasets.'], ['Linguistic Code-switching (CS) is still an understudied phenomenon in natural language processing', 'The NLP community has mostly focused on monolingual and multi-lingual scenarios, but little attention has been given to CS in particular', 'This is partly because of the lack of resources and annotated data, despite its increasing occurrence in social media platforms', 'In this paper, we aim at adapting monolingual models to code-switched text in various tasks', 'Specifically, we transfer English knowledge from a pre-trained ELMo model to different code-switched language pairs (i.e., Nepali-English, Spanish-English, and Hindi-English) using the task of language identification', 'Our method, CS-ELMo, is an extension of ELMo with a simple yet effective position-aware attention mechanism inside its character convolutions', 'We show the effectiveness of this transfer learning step by outperforming multilingual BERT and homologous CS-unaware ELMo models and establishing a new state of the art in CS tasks, such as NER and POS tagging', 'Our technique can be expanded to more English-paired code-switched languages, providing more resources to the CS community.'], ['Concept graphs are created as universal taxonomies for text understanding in the open-domain knowledge', 'The nodes in concept graphs include both entities and concepts', 'The edges are from entities to concepts, showing that an entity is an instance of a concept', 'In this paper, we propose the task of learning interpretable relationships from open-domain facts to enrich and refine concept graphs', 'The Bayesian network structures are learned from open-domain facts as the interpretable relationships between relations of facts and concepts of entities', 'We conduct extensive experiments on public English and Chinese datasets', 'Compared to the state-of-the-art methods, the learned network structures help improving the identification of concepts for entities based on the relations of entities on both datasets.'], ['We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution', 'Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types', 'We demonstrate strong performance of our model on RAMS and other event-related datasets.'], ['Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain', 'Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition', 'Given the corpus-level statistics, i.e., a global co-occurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated with the target entities, and then recognize relational interactions between these contexts to form model rationales, which will contribute to the final prediction', 'We conduct experiments on a real-world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models, but also present rationales to justify its prediction', 'We further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making.'], ['Named-entities are inherently multilingual, and annotations in any given language may be limited', 'This motivates us to consider polyglot named-entity recognition (NER), where one model is trained using annotated data drawn from more than one language', 'However, a straightforward implementation of this simple idea does not always work in practice: naive training of NER models using annotated data drawn from multiple languages consistently underperforms models trained on monolingual data alone, despite having access to more training data', 'The starting point of this paper is a simple solution to this problem, in which polyglot models are fine-tuned on monolingual data to consistently and significantly outperform their monolingual counterparts', 'To explain this phenomena, we explore the sources of multilingual transfer in polyglot NER models and examine the weight structure of polyglot models compared to their monolingual counterparts', 'We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters.'], ['In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color', 'Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template', 'In this work, we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals', 'Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates', 'Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical.'], ['Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance', 'Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data', 'However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages', 'To address this problem, we propose a method of “soft gazetteers” that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking', 'Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.'], ['We reframe suicide risk assessment from social media as a ranking problem whose goal is maximizing detection of severely at-risk individuals given the time available', 'Building on measures developed for resource-bounded document retrieval, we introduce a well founded evaluation paradigm, and demonstrate using an expert-annotated test collection that meaningful improvements over plausible cascade model baselines can be achieved using an approach that jointly ranks individuals and their social media posts.'], ['Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration', 'Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the “ideal” number of topics and depth of the hierarchy', 'In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM', 'CluHTM’s novel contributions include: (i) the exploration of richer text representation that encapsulates both, global (dataset level) and local semantic information – when combined, these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure; (ii) the exploitation of a stability analysis metric for defining the number of topics and the “shape” the hierarchical structure', 'In our evaluation, considering twelve datasets and seven state-of-the-art baselines, CluHTM outperformed the baselines in the vast majority of the cases, with gains of around 500% over the strongest state-of-the-art baselines', 'We also provide qualitative and quantitative statistical analyses of why our solution works so well.'], ['Entity set expansion, aiming at expanding a small seed entity set with new entities belonging to the same semantic class, is a critical task that benefits many downstream NLP and IR applications, such as question answering, query understanding, and taxonomy construction', 'Existing set expansion methods bootstrap the seed entity set by adaptively selecting context features and extracting new entities', 'A key challenge for entity set expansion is to avoid selecting ambiguous context features which will shift the class semantics and lead to accumulative errors in later iterations', 'In this study, we propose a novel iterative set expansion framework that leverages automatically generated class names to address the semantic drift issue', 'In each iteration, we select one positive and several negative class names by probing a pre-trained language model, and further score each candidate entity based on selected class names', 'Experiments on two datasets show that our framework generates high-quality class names and outperforms previous state-of-the-art methods significantly.'], ['In classification, there are usually some good features that are indicative of class labels', 'For example, in sentiment classification, words like good and nice are indicative of the positive sentiment and words like bad and terrible are indicative of the negative sentiment', 'However, there are also many common features (e.g., words) that are not indicative of any specific class (e.g., voice and screen, which are common to both sentiment classes and are not discriminative for classification)', 'Although deep learning has made significant progresses in generating discriminative features through its powerful representation learning, we believe there is still room for improvement', 'In this paper, we propose a novel angle to further improve this representation learning, i.e., feature projection', 'This method projects existing features into the orthogonal space of the common features', 'The resulting projection is thus perpendicular to the common features and more discriminative for classification', 'We apply this new method to improve CNN, RNN, Transformer, and Bert based text classification and obtain markedly better results.'], ['Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons', 'To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains', 'However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors', 'For instance, we find that it is not actually necessary to provide proper, human-based cues; random, insensible cues also result in similar improvements', 'Based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on VQA-CPv2.'], ['Visual Dialogue involves “understanding” the dialogue history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to accurately generate the correct response', 'In this paper, we show that co-attention models which explicitly encode dialoh history outperform models that don’t, achieving state-of-the-art performance (72 % NDCG on val set)', 'However, we also expose shortcomings of the crowdsourcing dataset collection procedure, by showing that dialogue history is indeed only required for a small amount of the data, and that the current evaluation metric encourages generic replies', 'To that end, we propose a challenging subset (VisdialConv) of the VisdialVal set and the benchmark NDCG of 63%.'], ['We present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it', 'For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator', 'To scale training, we decouple the language and action data by (a) annotating action phrase spans in How-To instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces', 'We use a Transformer to extract action phrase tuples from long-range natural language instructions', 'A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions', 'Given a starting screen and instruction, our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in PixelHelp.'], ['We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos', 'We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers', 'We name this augmented version as TVQA+', 'We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos', 'Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task', 'Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations.'], ['Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only', 'However, it is still challenging to associate source-target sentences in the latent space', 'As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT)', 'In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT', 'Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision', 'The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.'], ['In many languages like Arabic, diacritics are used to specify pronunciations as well as meanings', 'Such diacritics are often omitted in written text, increasing the number of possible pronunciations and meanings for a word', 'This results in a more ambiguous text making computational processing on such text more difficult', 'Diacritic restoration is the task of restoring missing diacritics in the written text', 'Most state-of-the-art diacritic restoration models are built on character level information which helps generalize the model to unseen data, but presumably lose useful information at the word level', 'Thus, to compensate for this loss, we investigate the use of multi-task learning to jointly optimize diacritic restoration with related NLP problems namely word segmentation, part-of-speech tagging, and syntactic diacritization', 'We use Arabic as a case study since it has sufficient data resources for tasks that we consider in our joint modeling', 'Our joint models significantly outperform the baselines and are comparable to the state-of-the-art models that are more complex relying on morphological analyzers and/or a lot more data (e.g', 'dialectal data).'], ['Lexica distinguishing all morphologically related forms of each lexeme are crucial to many language technologies, yet building them is expensive', 'We propose a frugal paradigm completion approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible', 'It induces typological information during training which it uses to determine the best sources at test time', 'We evaluate our language-agnostic approach on 7 diverse languages', 'Compared to popular alternative approaches, ours reduces manual labor by 16-63% and is the most robust to typological variation.'], ['Contextual features always play an important role in Chinese word segmentation (CWS)', 'Wordhood information, being one of the contextual features, is proved to be useful in many conventional character-based segmenters', 'However, this feature receives less attention in recent neural models and it is also challenging to design a framework that can properly integrate wordhood information from different wordhood measures to existing neural frameworks', 'In this paper, we therefore propose a neural framework, WMSeg, which uses memory networks to incorporate wordhood information with several popular encoder-decoder combinations for CWS', 'Experimental results on five benchmark datasets indicate the memory mechanism successfully models wordhood information for neural segmenters and helps WMSeg achieve state-of-the-art performance on all those datasets', 'Further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments.'], ['Chinese word segmentation (CWS) and part-of-speech (POS) tagging are important fundamental tasks for Chinese language processing, where joint learning of them is an effective one-step solution for both tasks', 'Previous studies for joint CWS and POS tagging mainly follow the character-based tagging paradigm with introducing contextual information such as n-gram features or sentential representations from recurrent neural models', 'However, for many cases, the joint tagging needs not only modeling from context features but also knowledge attached to them (e.g., syntactic relations among words); limited efforts have been made by existing research to meet such needs', 'In this paper, we propose a neural model named TwASP for joint CWS and POS tagging following the character-based sequence labeling paradigm, where a two-way attention mechanism is used to incorporate both context feature and their corresponding syntactic knowledge for each input character', 'Particularly, we use existing language processing toolkits to obtain the auto-analyzed syntactic knowledge for the context, and the proposed attention module can learn and benefit from them although their quality may not be perfect', 'Our experiments illustrate the effectiveness of the two-way attentions for joint CWS and POS tagging, where state-of-the-art performance is achieved on five benchmark datasets.'], ['The written forms of Semitic languages are both highly ambiguous and morphologically rich: a word can have multiple interpretations and is one of many inflected forms of the same concept or lemma', 'This is further exacerbated for dialectal content, which is more prone to noise and lacks a standard orthography', 'The morphological features can be lexicalized, like lemmas and diacritized forms, or non-lexicalized, like gender, number, and part-of-speech tags, among others', 'Joint modeling of the lexicalized and non-lexicalized features can identify more intricate morphological patterns, which provide better context modeling, and further disambiguate ambiguous lexical choices', 'However, the different modeling granularity can make joint modeling more difficult', 'Our approach models the different features jointly, whether lexicalized (on the character-level), or non-lexicalized (on the word-level)', 'We use Arabic as a test case, and achieve state-of-the-art results for Modern Standard Arabic with 20% relative error reduction, and Egyptian Arabic with 11% relative error reduction.'], ['Informal romanization is an idiosyncratic process used by humans in informal digital communication to encode non-Latin script languages into Latin character sets found on common keyboards', 'Character substitution choices differ between users but have been shown to be governed by the same main principles observed across a variety of languages—namely, character pairs are often associated through phonetic or visual similarity', 'We propose a noisy-channel WFST cascade model for deciphering the original non-Latin script from observed romanized text in an unsupervised fashion', 'We train our model directly on romanized data from two languages: Egyptian Arabic and Russian', 'We demonstrate that adding inductive bias through phonetic and visual priors on character mappings substantially improves the model’s performance on both languages, yielding results much closer to the supervised skyline', 'Finally, we introduce a new dataset of romanized Russian, collected from a Russian social network website and partially annotated for our experiments.'], ['We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent', 'This simple modification, when combined with a novel mention clustering algorithm for selecting which examples to label, is much more efficient in terms of the performance obtained per annotation budget', 'In experiments with existing benchmark coreference datasets, we show that the signal from this additional question leads to significant performance gains per human-annotation hour', 'Future work can use our annotation protocol to effectively develop coreference models for new domains', 'Our code is publicly available.'], ['This paper introduces two tasks: determining (a) the duration of possession relations and (b) co-possessions, i.e., whether multiple possessors possess a possessee at the same time', 'We present new annotations on top of corpora annotating possession existence and experimental results', 'Regarding possession duration, we derive the time spans we work with empirically from annotations indicating lower and upper bounds', 'Regarding co-possessions, we use a binary label', 'Cohen’s kappa coefficients indicate substantial agreement, and experimental results show that text is more useful than the image for solving these tasks.'], ['Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP', 'In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task', 'We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings', 'Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining', 'Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable', 'Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.'], ['Word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity (STS) tasks', 'Recent work has increasingly adopted a statistical view on these embeddings, with some of the top approaches being essentially various correlations (which include the famous cosine similarity)', 'Another excellent candidate for a similarity measure is mutual information (MI), which can capture arbitrary dependencies between the variables and has a simple and intuitive expression', 'Unfortunately, its use in the context of dense word embeddings has so far been avoided due to difficulties with estimating MI for continuous data', 'In this work we go through a vast literature on estimating MI in such cases and single out the most promising methods, yielding a simple and elegant similarity measure for word embeddings', 'We show that mutual information is a viable alternative to correlations, gives an excellent signal that correlates well with human judgements of similarity and rivals existing state-of-the-art unsupervised methods.'], ['We study the task of cross-database semantic parsing (XSP), where a system that maps natural language utterances to executable SQL queries is evaluated on databases unseen during training', 'Recently, several datasets, including Spider, were proposed to support development of XSP systems', 'We propose a challenging evaluation setup for cross-database semantic parsing, focusing on variation across database schemas and in-domain language use', 'We re-purpose eight semantic parsing datasets that have been well-studied in the setting where in-domain training data is available, and instead use them as additional evaluation data for XSP systems instead', 'We build a system that performs well on Spider, and find that it struggles to generalize to our re-purposed set', 'Our setup uncovers several generalization challenges for cross-database semantic parsing, demonstrating the need to use and develop diverse training and evaluation datasets.'], ['The focus of a negation is the set of tokens intended to be negated, and a key component for revealing affirmative alternatives to negated utterances', 'In this paper, we experiment with neural networks to predict the focus of negation', 'Our main novelty is leveraging a scope detector to introduce the scope of negation as an additional input to the network', 'Experimental results show that doing so obtains the best results to date', 'Additionally, we perform a detailed error analysis providing insights into the main error categories, and analyze errors depending on whether the model takes into account scope and context information.'], ['Recent neural network-driven semantic role labeling (SRL) systems have shown impressive improvements in F1 scores', 'These improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear SRL models', 'Introducing the benefits of structure to inform neural models presents a methodological challenge', 'In this paper, we present a structured tuning framework to improve models using softened constraints only at training time', 'Our framework leverages the expressiveness of neural networks and provides supervision with structured loss components', 'We start with a strong baseline (RoBERTa) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints', 'Additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios.'], ['Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks', 'Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables)', 'In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables', 'TaBERT is trained on a large corpus of 26 million tables and their English contexts', 'In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider.'], ['We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores', 'We also introduce a strong pipeline model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction', 'By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups.'], ['This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks', 'We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data', 'Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER', 'XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models', 'We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale', 'Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks', 'We will make our code and models publicly available.'], ['Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large', 'In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology', 'In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT', 'The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training', 'We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training', 'Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets.'], ['We propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction', 'At training, our novel multi-level learning-to-rank loss compares positive types against negative siblings according to the type tree', 'During prediction, we define a coarse-to-fine decoder that restricts viable candidates at each level of the ontology based on already predicted parent type(s)', 'Our approach significantly outperform prior work on strict accuracy, demonstrating the effectiveness of our method.'], ['Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input', 'However, domain transfer of NER models with data from multiple genres has not been widely studied', 'To this end, we conduct NER experiments in three predictive setups on data from: a) multiple domains; b) multiple domains where the genre label is unknown at inference time; c) domains not encountered in training', 'We introduce a new architecture tailored to this task by using shared and private domain parameters and multi-task learning', 'This consistently outperforms all other baseline and competitive methods on all three experimental setups, with differences ranging between +1.95 to +3.11 average F1 across multiple genres when compared to standard approaches', 'These results illustrate the challenges that need to be taken into account when building real-world NLP applications that are robust to various types of text and the methods that can help, at least partially, alleviate these issues.'], ['Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce', 'State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories', 'This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy', 'Through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts category-specific attribute values', 'Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10% in F1 and 15% in coverage across all categories.'], ['Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect', 'Thus, a crucial research question is how to obtain supervision in a cost-effective way', 'In this paper, we introduce “entity triggers,” an effective proxy of human explanations for facilitating label-efficient learning of NER models', 'An entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence', 'We crowd-sourced 14k entity triggers for two well-studied NER datasets', 'Our proposed model, Trigger Matching Network, jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging', 'Our framework is significantly more cost-effective than the traditional neural NER frameworks', 'Experiments show that using only 20% of the trigger-annotated sentences results in a comparable performance as using 70% of conventional annotated sentences.'], ['This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs)', 'It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings', 'Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task', 'As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro↔En and De↔En', 'With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU).'], ['When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others', 'Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance', 'In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages', 'Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.'], ['Neural Machine Translation (NMT) models are sensitive to small perturbations in the input', 'Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input', 'This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input', 'We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed', 'Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.'], ['Web-crawled data provides a good source of parallel corpora for training machine translation models', 'It is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods', 'In this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models', 'We measure sentence parallelism by leveraging the multilingual capability of BERT and use the Generative Pre-training (GPT) language model as a domain filter to balance data domains', 'We evaluate the proposed method on the WMT 2018 Parallel Corpus Filtering shared task, and on our own web-crawled Japanese-Chinese parallel corpus', 'Our method significantly outperforms baselines and achieves a new state-of-the-art', 'In an unsupervised setting, our method achieves comparable performance to the top-1 supervised method', 'We also evaluate on a web-crawled Japanese-Chinese parallel corpus that we make publicly available.'], ['Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT)', 'However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN', 'This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer', 'In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information', 'Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline.'], ['The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories', 'In this paper, we propose a novel multi-perspective cross-lingual neural framework for code–text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities', 'Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space.'], ['While automated essay scoring (AES) can reliably grade essays at scale, automated writing evaluation (AWE) additionally provides formative feedback to guide essay revision', 'However, a neural AES typically does not provide useful feature representations for supporting AWE', 'This paper presents a method for linking AWE and neural AES, by extracting Topical Components (TCs) representing evidence from a source text using the intermediate output of attention layers', 'We evaluate performance using a feature-based AES requiring TCs', 'Results show that performance is comparable whether using automatically or manually constructed TCs for 1) representing essays as rubric-based features, 2) grading essays.'], ['In traditional approaches to entity linking, linking decisions are based on three sources of information – the similarity of the mention string to an entity’s name, the similarity of the context of the document to the entity, and broader information about the knowledge base (KB)', 'In some domains, there is little contextual information present in the KB and thus we rely more heavily on mention string similarity', 'We consider one example of this, concept linking, which seeks to link mentions of medical concepts to a medical concept ontology', 'We propose an approach to concept linking that leverages recent work in contextualized neural models, such as ELMo (Peters et al', '2018), which create a token representation that integrates the surrounding context of the mention and concept name', 'We find a neural ranking approach paired with contextualized embeddings provides gains over a competitive baseline (Leaman et al', '2013)', 'Additionally, we find that a pre-training step using synonyms from the ontology offers a useful initialization for the ranker.'], ['The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence', 'The Fact Extraction and VERification (FEVER) dataset provides such a resource for evaluating endto- end fact-checking, requiring retrieval of evidence from Wikipedia to validate a veracity prediction', 'We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking – multiple propositions, temporal reasoning, and ambiguity and lexical variation – and introduce a resource with these types of claims', 'Then we present a system designed to be resilient to these “attacks” using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions', 'We find that in handling these attacks we obtain state-of-the-art results on FEVER, largely due to improved evidence retrieval.'], ['In this paper, we aim to learn associations between visual attributes of fonts and the verbal context of the texts they are typically applied to', 'Compared to related work leveraging the surrounding visual context, we choose to focus only on the input text, which can enable new applications for which the text is the only visual element in the document', 'We introduce a new dataset, containing examples of different topics in social media posts and ads, labeled through crowd-sourcing', 'Due to the subjective nature of the task, multiple fonts might be perceived as acceptable for an input text, which makes this problem challenging', 'To this end, we investigate different end-to-end models to learn label distributions on crowd-sourced data, to capture inter-subjectivity across all annotations.'], ['News framing refers to the practice in which aspects of specific issues are highlighted in the news to promote a particular interpretation', 'In NLP, although recent works have studied framing in English news, few have studied how the analysis can be extended to other languages and in a multi-label setting', 'In this work, we explore multilingual transfer learning to detect multiple frames from just the news headline in a genuinely low-resource context where there are few/no frame annotations in the target language', 'We propose a novel method that can leverage elementary resources consisting of a dictionary and few annotations to detect frames in the target language', 'Our method performs comparably or better than translating the entire target language headline to the source language for which we have annotated data', 'This work opens up an exciting new capability of scaling up frame analysis to many languages, even those without existing translation technologies', 'Lastly, we apply our method to detect frames on the issue of U.S', 'gun violence in multiple languages and obtain exciting insights on the relationship between different frames of the same problem across different countries with different languages.'], ['Given the complexity of combinations of tasks, languages, and domains in natural language processing (NLP) research, it is computationally prohibitive to exhaustively test newly proposed models on each possible experimental setting', 'In this work, we attempt to explore the possibility of gaining plausible judgments of how well an NLP model can perform under an experimental setting, without actually training or testing the model', 'To do so, we build regression models to predict the evaluation score of an NLP experiment given the experimental settings as input', 'Experimenting on~9 different NLP tasks, we find that our predictors can produce meaningful predictions over unseen languages and different modeling architectures, outperforming reasonable baselines as well as human experts', '%we represent experimental settings using an array of features', 'Going further, we outline how our predictor can be used to find a small subset of representative experiments that should be run in order to obtain plausible predictions for all other experimental settings.'], ['It is appealing to have a system that generates a story or scripts automatically from a storyline, even though this is still out of our reach', 'In dialogue systems, it would also be useful to drive dialogues by a dialogue plan', 'In this paper, we address a key problem involved in these applications - guiding a dialogue by a narrative', 'The proposed model ScriptWriter selects the best response among the candidates that fit the context as well as the given narrative', 'It keeps track of what in the narrative has been said and what is to be said', 'A narrative plays a different role than the context (i.e., previous utterances), which is generally used in current dialogue systems', 'Due to the unavailability of data for this new application, we construct a new large-scale data collection GraphMovie from a movie website where end- users can upload their narratives freely when watching a movie', 'Experimental results on the dataset show that our proposed approach based on narratives significantly outperforms the baselines that simply use the narrative as a kind of context.'], ['Most of recent work in cross-lingual word embeddings is severely Anglocentric', 'The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting', 'With this work, however, we challenge these practices', 'First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance', 'Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages', 'Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field', 'Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.'], ['Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks', 'In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails', 'We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action', 'We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0.23 and 0.63 for this task', 'To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails.'], ['Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another', 'However, the ability of NLI models to make pragmatic inferences remains understudied', 'We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types', 'We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences', 'Although MultiNLI appears to contain very few pairs illustrating these inference types, we find that BERT learns to draw pragmatic inferences', 'It reliably treats scalar implicatures triggered by “some” as entailments', 'For some presupposition triggers like “only”, BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation', 'BOW and InferSent show weaker evidence of pragmatic reasoning', 'We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.'], ['Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios', 'We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets', 'The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases', 'During training, the bias-only models’ predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples', 'We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data', 'Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets', 'Our code and data are publicly available in https://github.com/rabeehk/robust-nli.'], ['Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution', 'Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance', 'However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity', 'This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the out-of-distribution data', 'In this paper, we address this trade-off by introducing a novel debiasing method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples', 'We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy.'], ['The recent growth in the popularity and success of deep learning models on NLP classification tasks has accompanied the need for generating some form of natural language explanation of the predicted labels', 'Such generated natural language (NL) explanations are expected to be faithful, i.e., they should correlate well with the model’s internal decision making', 'In this work, we focus on the task of natural language inference (NLI) and address the following question: can we build NLI systems which produce labels with high accuracy, while also generating faithful explanations of its decisions? We propose Natural-language Inference over Label-specific Explanations (NILE), a novel NLI method which utilizes auto-generated label-specific NL explanations to produce labels along with its faithful explanation', 'We demonstrate NILE’s effectiveness over previously reported methods through automated and human evaluation of the produced labels and explanations', 'Our evaluation of NILE also supports the claim that accurate systems capable of providing testable explanations of their decisions can be designed', 'We discuss the faithfulness of NILE’s explanations in terms of sensitivity of the decisions to the corresponding explanations', 'We argue that explicit evaluation of faithfulness, in addition to label and explanation accuracy, is an important step in evaluating model’s explanations', 'Further, we demonstrate that task-specific probes are necessary to establish such sensitivity.'], ['Question-answering (QA) data often encodes essential information in many facets', 'This paper studies a natural question: Can we get supervision from QA data for other tasks (typically, non-QA ones)? For example, can we use QAMR (Michael et al., 2017) to improve named entity recognition? We suggest that simply further pre-training BERT is often not the best option, and propose the question-answer driven sentence encoding (QuASE) framework', 'QuASE learns representations from QA data, using BERT or other state-of-the-art contextual language models', 'In particular, we observe the need to distinguish between two types of sentence encodings, depending on whether the target task is a single- or multi-sentence input; in both cases, the resulting encoding is shown to be an easy-to-use plugin for many downstream tasks', 'This work may point out an alternative way to supervise NLP tasks.'], ['While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics', 'Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases', 'First, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method', 'Next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance', 'The first approach aims to remove the label bias at the embedding level', 'The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models', 'We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy.'], ['We introduce Uncertain Natural Language Inference (UNLI), a refinement of Natural Language Inference (NLI) that shifts away from categorical labels, targeting instead the direct prediction of subjective probability assessments', 'We demonstrate the feasibility of collecting annotations for UNLI by relabeling a portion of the SNLI dataset under a probabilistic scale, where items even with the same categorical label differ in how likely people judge them to be true given a premise', 'We describe a direct scalar regression modeling approach, and find that existing categorically-labeled NLI data can be used in pre-training', 'Our best models correlate well with humans, demonstrating models are capable of more subtle inferences than the categorical bin assignment employed in current NLI tasks.'], ['An interesting and frequent type of multi-word expression (MWE) is the headless MWE, for which there are no true internal syntactic dominance relations; examples include many named entities (“Wells Fargo”) and dates (“July 5, 2020”) as well as certain productive constructions (“blow for blow”, “day after day”)', 'Despite their special status and prevalence, current dependency-annotation schemes require treating such flat structures as if they had internal syntactic heads, and most current parsers handle them in the same fashion as headed constructions', 'Meanwhile, outside the context of parsing, taggers are typically used for identifying MWEs, but taggers might benefit from structural information', 'We empirically compare these two common strategies—parsing and tagging—for predicting flat MWEs', 'Additionally, we propose an efficient joint decoding algorithm that combines scores from both strategies', 'Experimental results on the MWE-Aware English Dependency Corpus and on six non-English dependency treebanks with frequent flat structures show that: (1) tagging is more accurate than parsing for identifying flat-structure MWEs, (2) our joint decoder reconciles the two different views and, for non-BERT features, leads to higher accuracies, and (3) most of the gains result from feature sharing between the parsers and taggers.'], ['Neural encoders have allowed dependency parsers to shift from higher-order structured models to simpler first-order ones, making decoding faster and still achieving better accuracy than non-neural parsers', 'This has led to a belief that neural encoders can implicitly encode structural constraints, such as siblings and grandparents in a tree', 'We tested this hypothesis and found that neural parsers may benefit from higher-order features, even when employing a powerful pre-trained encoder, such as BERT', 'While the gains of higher-order features are small in the presence of a powerful encoder, they are consistent for long-range dependencies and long sentences', 'In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures.'], ['Virtual adversarial training (VAT) is a powerful technique to improve model robustness in both supervised and semi-supervised settings', 'It is effective and can be easily adopted on lots of image classification and text classification tasks', 'However, its benefits to sequence labeling tasks such as named entity recognition (NER) have not been shown as significant, mostly, because the previous approach can not combine VAT with the conditional random field (CRF)', 'CRF can significantly boost accuracy for sequence models by putting constraints on label transitions, which makes it an essential component in most state-of-the-art sequence labeling model architectures', 'In this paper, we propose SeqVAT, a method which naturally applies VAT to sequence labeling models with CRF', 'Empirical studies show that SeqVAT not only significantly improves the sequence labeling performance over baselines under supervised settings, but also outperforms state-of-the-art approaches under semi-supervised settings.'], ['A recent advance in monolingual dependency parsing is the idea of a treebank embedding vector, which allows all treebanks for a particular language to be used as training data while at the same time allowing the model to prefer training data from one treebank over others and to select the preferred treebank at test time', 'We build on this idea by 1) introducing a method to predict a treebank vector for sentences that do not come from a treebank used in training, and 2) exploring what happens when we move away from predefined treebank embedding vectors during test time and instead devise tailored interpolations', 'We show that 1) there are interpolated vectors that are superior to the predefined ones, and 2) treebank vectors can be predicted with sufficient accuracy, for nine out of ten test languages, to match the performance of an oracle approach that knows the most suitable predefined treebank embedding for the test set.'], ['This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multimodal software robot equipped with four inte- gral capabilities: news generation, news translation, news reading and avatar animation', 'Its system summarizes Chinese news that it automatically generates from data tables', 'Next, it translates the summary or the full article into multiple languages, and reads the multi- lingual rendition through synthesized speech', 'Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the speech trained from a real person’s voice data in one input language', 'The proposed system enjoys several merits: it has an animated avatar, and is able to generate and read multilingual news', 'Since it was put into practice, Xiaomingbot has written over 600,000 articles, and gained over 150,000 followers on social media platforms.'], ['In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for natural language processing', 'It works with different neural network models and supports various kinds of supervised learning tasks, such as text classification, reading comprehension, sequence labeling', 'TextBrewer provides a simple and uniform workflow that enables quick setting up of distillation experiments with highly flexible configurations', 'It offers a set of predefined distillation methods and can be extended with custom code', 'As a case study, we use TextBrewer to distill BERT on several typical NLP tasks', 'With simple configurations, we achieve results that are comparable with or even higher than the public distilled BERT models with similar numbers of parameters.'], ['We present a system that allows a user to search a large linguistically annotated corpus using syntactic patterns over dependency graphs', 'In contrast to previous attempts to this effect, we introduce a light-weight query language that does not require the user to know the details of the underlying syntactic representations, and instead to query the corpus by providing an example sentence coupled with simple markup', 'Search is performed at an interactive speed due to efficient linguistic graph-indexing and retrieval engine', 'This allows for rapid exploration, development and refinement of syntax-based queries', 'We demonstrate the system using queries over two corpora: the English wikipedia, and a collection of English pubmed abstracts', 'A demo of the wikipedia system is available at https://allenai.github.io/spike/ .'], ['We present Tabouid, a word-guessing game automatically generated from Wikipedia', 'Tabouid contains 10,000 (virtual) cards in English, and as many in French, covering not only words and linguistic expressions but also a variety of topics including artists, historical events or scientific concepts', 'Each card corresponds to a Wikipedia article, and conversely, any article could be turned into a card', 'A range of relatively simple NLP and machine-learning techniques are effectively integrated into a two-stage process', 'First, a large subset of Wikipedia articles are scored - this score estimates the difficulty, or alternatively, the playability of the page', 'Then, the best articles are turned into cards by selecting, for each of them, a list of banned words based on its content', 'We believe that the game we present is more than mere entertainment and that, furthermore, this paper has pedagogical potential.'], ['We introduce Talk to Papers, which exploits the recent open-domain question answering (QA) techniques to improve the current experience of academic search', 'It’s designed to enable researchers to use natural language queries to find precise answers and extract insights from a massive amount of academic papers', 'We present a large improvement over classic search engine baseline on several standard QA datasets and provide the community a collaborative data collection tool to curate the first natural language processing research QA dataset via a community effort.'], ['Exploiting syntagmatic information is an encouraging research focus to be pursued in an effort to close the gap between knowledge-based and supervised Word Sense Disambiguation (WSD) performance', 'We follow this direction in our next-generation knowledge-based WSD system, SyntagRank, which we make available via a Web interface and a RESTful API', 'SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a multilingual setting', 'Our service provides both a user-friendly interface, available at http://syntagnet.org/, and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/).'], ['Syntactic dependencies can be predicted with high accuracy, and are useful for both machine-learned and pattern-based information extraction tasks', 'However, their utility can be improved', 'These syntactic dependencies are designed to accurately reflect syntactic relations, and they do not make semantic relations explicit', 'Therefore, these representations lack many explicit connections between content words, that would be useful for downstream applications', 'Proposals like English Enhanced UD improve the situation by extending universal dependency trees with additional explicit arcs', 'However, they are not available to Python users, and are also limited in coverage', 'We introduce a broad-coverage, data-driven and linguistically sound set of transformations, that makes event-structure and many lexical relations explicit', 'We present pyBART, an easy-to-use open-source Python library for converting English UD trees either to Enhanced UD graphs or to our representation', 'The library can work as a standalone package or be integrated within a spaCy NLP pipeline', 'When evaluated in a pattern-based relation extraction scenario, our representation results in higher extraction scores than Enhanced UD, while requiring fewer patterns.'], ['Traditional search engines for life sciences (e.g., PubMed) are designed for document retrieval and do not allow direct retrieval of specific statements', 'Some of these statements may serve as textual evidence that is key to tasks such as hypothesis generation and new finding validation', 'We present EVIDENCEMINER, a web-based system that lets users query a natural language statement and automatically retrieves textual evidence from a background corpora for life sciences', 'EVIDENCEMINER is constructed in a completely automated way without any human effort for training data annotation', 'It is supported by novel data-driven methods for distantly supervised named entity recognition and open information extraction', 'The entities and patterns are pre-computed and indexed offline to support fast online evidence retrieval', 'The annotation results are also highlighted in the original document for better visualization', 'EVIDENCEMINER also includes analytic functionalities such as the most frequent entity and relation summarization', 'EVIDENCEMINER can help scientists uncover important research issues, leading to more effective research and more in-depth quantitative analysis', 'The system of EVIDENCEMINER is available at https://evidenceminer.firebaseapp.com/.'], ['We introduce Trialstreamer, a living database of clinical trial reports', 'Here we mainly describe the evidence extraction component; this extracts from biomedical abstracts key pieces of information that clinicians need when appraising the literature, and also the relations between these', 'Specifically, the system extracts descriptions of trial participants, the treatments compared in each arm (the interventions), and which outcomes were measured', 'The system then attempts to infer which interventions were reported to work best by determining their relationship with identified trial outcome measures', 'In addition to summarizing individual trials, these extracted data elements allow automatic synthesis of results across many trials on the same topic', 'We apply the system at scale to all reports of randomized controlled trials indexed in MEDLINE, powering the automatic generation of evidence maps, which provide a global view of the efficacy of different interventions combining data from all relevant clinical trials on a topic', 'We make all code and models freely available alongside a demonstration of the web interface.'], ['Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models', 'However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models', 'We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design', 'This paper releases two tools of independent value for the computational linguistics community: 1', 'A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2', 'Two command-line tools, ‘syntaxgym‘ and ‘lm-zoo‘, which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine.'], ['We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology', 'Our system, GAIA, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos', 'GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation', 'The system is publicly available at GitHub and DockerHub, with a narrated video that documents the system.'], ['We present easy-to-use retrieval focused multilingual sentence embedding models, made available on TensorFlow Hub', 'The models embed text from 16 languages into a shared semantic space using a multi-task trained dual-encoder that learns tied cross-lingual representations via translation bridge tasks (Chidambaram et al., 2018)', 'The models achieve a new state-of-the-art in performance on monolingual and cross-lingual semantic retrieval (SR)', 'Competitive performance is obtained on the related tasks of translation pair bitext retrieval (BR) and retrieval question answering (ReQA)', 'On transfer learning tasks, our multilingual embeddings approach, and in some cases exceed, the performance of English only sentence embeddings.'], ['CodaLab is an open-source web-based platform for collaborative computational research', 'Although CodaLab has gained popularity in the research community, its interface has limited support for creating reusable tools that can be easily applied to new datasets and composed into pipelines', 'In clinical domain, natural language processing (NLP) on medical notes generally involves multiple steps, like tokenization, named entity recognition, etc', 'Since these steps require different tools which are usually scattered in different publications, it is not easy for researchers to use them to process their own datasets', 'In this paper, we present BENTO, a workflow management platform with a graphic user interface (GUI) that is built on top of CodaLab, to facilitate the process of building clinical NLP pipelines', 'BENTO comes with a number of clinical NLP tools that have been pre-trained using medical notes and expert annotations and can be readily used for various clinical NLP tasks', 'It also allows researchers and developers to create their custom tools (e.g., pre-trained NLP models) and use them in a controlled and reproducible way', 'In addition, the GUI interface enables researchers with limited computer background to compose tools into NLP pipelines and then apply the pipelines on their own datasets in a “what you see is what you get” (WYSIWYG) way', 'Although BENTO is designed for clinical NLP applications, the underlying architecture is flexible to be tailored to any other domains.'], ['We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages', 'Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition', 'We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested', 'Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction', 'Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.'], ['We introduce jiant, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks', 'jiant enables modular and configuration driven experimentation with state-of-the-art models and a broad set of tasks for probing, transfer learning, and multitask training experiments', 'jiant implements over 50 NLU tasks, including all GLUE and SuperGLUE benchmark tasks', 'We demonstrate that jiant reproduces published performance on a variety of tasks and models, e.g., RoBERTa and BERT.'], ['We present MT-DNN, an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models', 'Built upon PyTorch and Transformers, MT-DNN is designed to facilitate rapid customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and text encoders (e.g., RNNs, BERT, RoBERTa, UniLM)', 'A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm', 'To enable efficient production deployment, MT-DNN supports multi-task knowledge distillation, which can substantially compress a deep neural model without significant performance drop', 'We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains', 'The software and pre-trained models will be publicly available at https://github.com/namisan/mt-dnn.'], ['This paper presents LinggleWrite, a writing coach that provides writing suggestions, assesses writing proficiency levels, detects grammatical errors, and offers corrective feedback in response to user’s essay', 'The method involves extracting grammar patterns, training models for automated essay scoring (AES) and grammatical error detection (GED), and finally retrieving plausible corrections from a n-gram search engine', 'Experiments on public test sets indicate that both AES and GED models achieve state-of-the-art performance', 'These results show that LinggleWrite is potentially useful in helping learners improve their writing skills.'], ['We present CLIReval, an easy-to-use toolkit for evaluating machine translation (MT) with the proxy task of cross-lingual information retrieval (CLIR)', 'Contrary to what the project name might suggest, CLIReval does not actually require any annotated CLIR dataset', 'Instead, it automatically transforms translations and references used in MT evaluations into a synthetic CLIR dataset; it then sets up a standard search engine (Elasticsearch) and computes various information retrieval metrics (e.g., mean average precision) by treating the translations as documents to be retrieved', 'The idea is to gauge the quality of MT by its impact on the document translation approach to CLIR', 'As a case study, we run CLIReval on the “metrics shared task” of WMT2019; while this extrinsic metric is not intended to replace popular intrinsic metrics such as BLEU, results suggest CLIReval is competitive in many language pairs in terms of correlation to human judgments of quality', 'CLIReval is publicly available at https://github.com/ssun32/CLIReval.'], ['We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems', 'As the successor of ConvLab, ConvLab-2 inherits ConvLab’s framework but integrates more powerful dialogue models and supports more datasets', 'Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems', 'The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement', 'The interactive tool provides an user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component.'], ['This paper introduces OpusFilter, a flexible and modular toolbox for filtering parallel corpora', 'It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters', 'Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data', 'We demonstrate the effectiveness of OpusFilter on the example of a Finnish-English news translation task based on noisy web-crawled training data', 'Applying our tool leads to improved translation quality while significantly reducing the size of the training data, also clearly outperforming an alternative ranking given in the crawled data set', 'Furthermore, we show the ability of OpusFilter to perform data selection for domain adaptation.'], ['Label noise—incorrectly or ambiguously labeled training examples—can negatively impact model performance', 'Although noise detection techniques have been around for decades, practitioners rarely apply them, as manual noise remediation is a tedious process', 'Examples incorrectly flagged as noise waste reviewers’ time, and correcting label noise without guidance can be difficult', 'We propose LNIC, a noise-detection method that uses an example’s neighborhood within the training set to (a) reduce false positives and (b) provide an explanation as to why the ex- ample was flagged as noise', 'We demonstrate on several short-text classification datasets that LNIC outperforms the state of the art on measures of precision and F0.5-score', 'We also show how LNIC’s training set context helps a reviewer to understand and correct label noise in a dataset', 'The LNIC tool lowers the barriers to label noise remediation, increasing its utility for NLP practitioners.'], ['Large Transformer-based language models can route and reshape complex information via their multi-headed attention mechanism', 'Although the attention never receives explicit supervision, it can exhibit recognizable patterns following linguistic or positional information', 'Analyzing the learned representations and attentions is paramount to furthering our understanding of the inner workings of these models', 'However, analyses have to catch up with the rapid release of new models and the growing diversity of investigation techniques', 'To support analysis for a wide variety of models, we introduce exBERT, a tool to help humans conduct flexible, interactive investigations and formulate hypotheses for the model-internal reasoning process', 'exBERT provides insights into the meaning of the contextual representations and attention by matching a human-specified input to similar contexts in large annotated datasets', 'By aggregating the annotations of the matched contexts, exBERT can quickly replicate findings from literature and extend them to previously not analyzed models.']]\n"
     ]
    }
   ],
   "source": [
    "list=[]\n",
    "for i in range(len(data['b'])):\n",
    "    list.append(data['b'][i].split('. '))\n",
    "print(list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method',\n",
       " 'As a step in this direction we study the case of representations of phonology in neural network models of spoken language',\n",
       " 'We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences',\n",
       " 'We manipulate two factors that can affect the outcome of analysis',\n",
       " 'First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models',\n",
       " 'Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance',\n",
       " 'We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ls in list:\n",
    "    for i in range(len(ls)-1):\n",
    "        ls[i]=ls[i]+'.'\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method.',\n",
       "  'As a step in this direction we study the case of representations of phonology in neural network models of spoken language.',\n",
       "  'We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences.',\n",
       "  'We manipulate two factors that can affect the outcome of analysis.',\n",
       "  'First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models.',\n",
       "  'Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance.',\n",
       "  'We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.'],\n",
       " ['To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions.',\n",
       "  'In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations.',\n",
       "  'We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations.',\n",
       "  'Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks.',\n",
       "  'Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions.',\n",
       "  'Our framework shows that this model is capable of generating a significant number of inconsistent explanations.'],\n",
       " ['By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings).',\n",
       "  'The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge.',\n",
       "  'However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself.',\n",
       "  'Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT).',\n",
       "  'Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process.',\n",
       "  'Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines.',\n",
       "  'We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.'],\n",
       " ['Language models keep track of complex information about the preceding context – including, e.g., syntactic relations in a sentence.',\n",
       "  'We investigate whether they also capture information beneficial for resolving pronominal anaphora in English.',\n",
       "  'We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus.',\n",
       "  'The Transformer outperforms the LSTM in all analyses.',\n",
       "  'Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world.',\n",
       "  'However, we find traces of the latter aspect, too.'],\n",
       " ['In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer.',\n",
       "  'Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed.',\n",
       "  'This makes attention weights unreliable as explanations probes.',\n",
       "  'In this paper, we consider the problem of quantifying this flow of information through self-attention.',\n",
       "  'We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens.',\n",
       "  'We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.'],\n",
       " ['With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems.',\n",
       "  'But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research.',\n",
       "  'We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria.',\n",
       "  'We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is “defined” by the community.',\n",
       "  'We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted.',\n",
       "  'Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful.',\n",
       "  'We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.'],\n",
       " ['Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model’s predictions.',\n",
       "  'Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model’s prediction.',\n",
       "  'They can be considered a plausible explanation if they provide a human-understandable justification for the model’s predictions.',\n",
       "  'In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model’s predictions.',\n",
       "  'We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model’s predictions.',\n",
       "  'Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model’s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions.',\n",
       "  'To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse.',\n",
       "  'We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model’s predictions (iii) correlate better with gradient-based attribution methods.',\n",
       "  'Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model’s predictions.',\n",
       "  'Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention'],\n",
       " ['Multi-task Learning methods have achieved great progress in text classification.',\n",
       "  'However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications.',\n",
       "  'To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption.',\n",
       "  'The extensive experiments back up our theoretical analysis and validate the superiority of our proposals.'],\n",
       " ['This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology.',\n",
       "  'Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation.',\n",
       "  'The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.'],\n",
       " ['Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training.',\n",
       "  'However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading.',\n",
       "  'In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances).',\n",
       "  'We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills.',\n",
       "  'We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.'],\n",
       " ['This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC).',\n",
       "  'The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC.',\n",
       "  'For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods.',\n",
       "  'Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM.',\n",
       "  'The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks.',\n",
       "  'Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.'],\n",
       " ['With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents.',\n",
       "  'Most existing methods usually learn the representations of users and news from news contents for recommendation.',\n",
       "  'However, they seldom consider high-order connectivity underlying the user-news interactions.',\n",
       "  'Moreover, existing methods failed to disentangle a user’s latent preference factors which cause her clicks on different news.',\n",
       "  'In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD.',\n",
       "  'Our model can encode high-order relationships into user and news representations by information propagation along the graph.',\n",
       "  'Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability.',\n",
       "  'A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations.',\n",
       "  'Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.'],\n",
       " ['In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case.',\n",
       "  'We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story.',\n",
       "  'We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework.',\n",
       "  'Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants’ impacts in a complex case.'],\n",
       " ['The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online.',\n",
       "  'Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection.',\n",
       "  'While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language.',\n",
       "  'The latter is, however, inextricably linked to abusive behaviour.',\n",
       "  'In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other.',\n",
       "  'Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.'],\n",
       " ['The key to effortless end-user programming is natural language.',\n",
       "  'We examine how to teach intelligent systems new functions, expressed in natural language.',\n",
       "  'As a first step, we collected 3168 samples of teaching efforts in plain English.',\n",
       "  'Then we built fuSE, a novel system that translates English function descriptions into code.',\n",
       "  'Our approach is three-tiered and each task is evaluated separately.',\n",
       "  'We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT).',\n",
       "  'Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM).',\n",
       "  'Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods).',\n",
       "  'In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.'],\n",
       " ['Moderation is crucial to promoting healthy online discussions.',\n",
       "  'Although several ‘toxicity’ detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently.',\n",
       "  'We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems? We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title.',\n",
       "  'We find that context can both amplify or mitigate the perceived toxicity of posts.',\n",
       "  'Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context.',\n",
       "  'Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware.',\n",
       "  'This points to the need for larger datasets of comments annotated in context.',\n",
       "  'We make our code and data publicly available.'],\n",
       " ['Abstract Meaning Representations (AMRs) capture sentence-level semantics structural representations to broad-coverage natural sentences.',\n",
       "  'We investigate parsing AMR with explicit dependency structures and interpretable latent structures.',\n",
       "  'We generate the latent soft structure without additional annotations, and fuse both dependency and latent structure via an extended graph neural networks.',\n",
       "  'The fused structural information helps our experiments results to achieve the best reported results on both AMR 2.0 (77.5% Smatch F1 on LDC2017T10) and AMR 1.0 ((71.8% Smatch F1 on LDC2014T12).'],\n",
       " ['Answering natural language questions over tables is usually seen as a semantic parsing task.',\n",
       "  'To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms.',\n",
       "  'However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation.',\n",
       "  'In this paper, we present TaPas, an approach to question answering over tables without generating logical forms.',\n",
       "  'TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection.',\n",
       "  'TaPas extends BERT’s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end.',\n",
       "  'We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture.',\n",
       "  'We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.'],\n",
       " ['In argumentation, people state premises to reason towards a conclusion.',\n",
       "  'The conclusion conveys a stance towards some target, such as a concept or statement.',\n",
       "  'Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons.',\n",
       "  'However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation.',\n",
       "  'We thus study the question to what extent an argument’s conclusion can be reconstructed from its premises.',\n",
       "  'In particular, we argue here that a decisive step is to infer a conclusion’s target, and we hypothesize that this target is related to the premises’ targets.',\n",
       "  'We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network.',\n",
       "  'Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines.',\n",
       "  'According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.'],\n",
       " ['Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality.',\n",
       "  'Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities.',\n",
       "  'Equally treating all modalities may encode too much useless information from less important modalities.',\n",
       "  'In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT.',\n",
       "  'The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images.',\n",
       "  'Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.'],\n",
       " ['In this paper, we hypothesize that sarcasm is closely related to sentiment and emotion, and thereby propose a multi-task deep learning framework to solve all these three problems simultaneously in a multi-modal conversational scenario.',\n",
       "  'We, at first, manually annotate the recently released multi-modal MUStARD sarcasm dataset with sentiment and emotion classes, both implicit and explicit.',\n",
       "  'For multi-tasking, we propose two attention mechanisms, viz.',\n",
       "  'Inter-segment Inter-modal Attention (Ie-Attention) and Intra-segment Inter-modal Attention (Ia-Attention).',\n",
       "  'The main motivation of Ie-Attention is to learn the relationship between the different segments of the sentence across the modalities.',\n",
       "  'In contrast, Ia-Attention focuses within the same segment of the sentence across the modalities.',\n",
       "  'Finally, representations from both the attentions are concatenated and shared across the five classes (i.e., sarcasm, implicit sentiment, explicit sentiment, implicit emotion, explicit emotion) for multi-tasking.',\n",
       "  'Experimental results on the extended version of the MUStARD dataset show the efficacy of our proposed approach for sarcasm detection over the existing state-of-the-art systems.',\n",
       "  'The evaluation also shows that the proposed multi-task framework yields better performance for the primary task, i.e., sarcasm detection, with the help of two secondary tasks, emotion and sentiment analysis.'],\n",
       " ['The task of Dialogue Act Classification (DAC) that purports to capture communicative intent has been studied extensively.',\n",
       "  'But these studies limit themselves to text.',\n",
       "  'Non-verbal features (change of tone, facial expressions etc.) can provide cues to identify DAs, thus stressing the benefit of incorporating multi-modal inputs in the task.',\n",
       "  'Also, the emotional state of the speaker has a substantial effect on the choice of the dialogue act, since conversations are often influenced by emotions.',\n",
       "  'Hence, the effect of emotion too on automatic identification of DAs needs to be studied.',\n",
       "  'In this work, we address the role of both multi-modality and emotion recognition (ER) in DAC.',\n",
       "  'DAC and ER help each other by way of multi-task learning.',\n",
       "  'One of the major contributions of this work is a new dataset- multimodal Emotion aware Dialogue Act dataset called EMOTyDA, collected from open-sourced dialogue datasets.',\n",
       "  'To demonstrate the utility of EMOTyDA, we build an attention based (self, inter-modal, inter-task) multi-modal, multi-task Deep Neural Network (DNN) for joint learning of DAs and emotions.',\n",
       "  'We show empirically that multi-modality and multi-tasking achieve better performance of DAC compared to uni-modal and single task DAC variants.'],\n",
       " ['Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.',\n",
       "  'In this paper, we present the first computational study of parody.',\n",
       "  'We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts.',\n",
       "  'We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.',\n",
       "  'Our results show that political parody tweets can be predicted with an accuracy up to 90%.',\n",
       "  'Finally, we identify the markers of parody through a linguistic analysis.',\n",
       "  'Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.'],\n",
       " ['A central concern in Computational Social Sciences (CSS) is fairness: where the role of NLP is to scale up text analysis to large corpora, the quality of automatic analyses should be as independent as possible of textual properties.',\n",
       "  'We analyze the performance of a state-of-the-art neural model on the task of political claims detection (i.e., the identification of forward-looking statements made by political actors) and identify a strong frequency bias: claims made by frequent actors are recognized better.',\n",
       "  'We propose two simple debiasing methods which mask proper names and pronouns during training of the model, thus removing personal information bias.',\n",
       "  'We find that (a) these methods significantly decrease frequency bias while keeping the overall performance stable; and (b) the resulting models improve when evaluated in an out-of-domain setting.'],\n",
       " ['Social biases are encoded in word embeddings.',\n",
       "  'This presents a unique opportunity to study society historically and at scale, and a unique danger when embeddings are used in downstream applications.',\n",
       "  'Here, we investigate the extent to which publicly-available word embeddings accurately reflect beliefs about certain kinds of people as measured via traditional survey methods.',\n",
       "  'We find that biases found in word embeddings do, on average, closely mirror survey data across seventeen dimensions of social meaning.',\n",
       "  'However, we also find that biases in embeddings are much more reflective of survey data for some dimensions of meaning (e.g.',\n",
       "  'gender) than others (e.g.',\n",
       "  'race), and that we can be highly confident that embedding-based measures reflect survey data only for the most salient biases.'],\n",
       " ['In an era where generating content and publishing it is so easy, we are bombarded with information and are exposed to all kinds of claims, some of which do not always rank high on the truth scale.',\n",
       "  'This paper suggests that the key to a longer-term, holistic, and systematic approach to navigating this information pollution is capturing the provenance of claims.',\n",
       "  'To do that, we develop a formal definition of provenance graph for a given natural language claim, aiming to understand where the claim may come from and how it has evolved.',\n",
       "  'To construct the graph, we model provenance inference, formulated mainly as an information extraction task and addressed via a textual entailment model.',\n",
       "  'We evaluate our approach using two benchmark datasets, showing initial success in capturing the notion of provenance and its effectiveness on the application of claim verification.'],\n",
       " ['Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality.',\n",
       "  'In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality.',\n",
       "  'Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts.',\n",
       "  'Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize.',\n",
       "  'Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents.',\n",
       "  'We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.'],\n",
       " ['State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions.',\n",
       "  'This limitation has increased interest in designing more interpretable deep models for NLP that reveal the ‘reasoning’ behind model outputs.',\n",
       "  'But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress.',\n",
       "  'We propose the Evaluating Rationales And Simple English Reasoning (ERASER a benchmark to advance research on interpretable models in NLP.',\n",
       "  'This benchmark comprises multiple datasets and tasks for which human annotations of “rationales” (supporting evidence) have been collected.',\n",
       "  'We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions).',\n",
       "  'Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems.',\n",
       "  'The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/'],\n",
       " ['In many settings it is important for one to be able to understand why a model made a particular prediction.',\n",
       "  'In NLP this often entails extracting snippets of an input text ‘responsible for’ corresponding model output; when such a snippet comprises tokens that indeed informed the model’s prediction, it is a faithful explanation.',\n",
       "  'In some settings, faithfulness may be critical to ensure transparency.',\n",
       "  'Lei et al.',\n",
       "  '(2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules.',\n",
       "  'However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning.',\n",
       "  'We propose a simpler variant of this approach that provides faithful explanations by construction.',\n",
       "  'In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict.',\n",
       "  'An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex.',\n",
       "  'In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to ‘end-to-end’ approaches, while being more general and easier to train.',\n",
       "  'Code is available at https://github.com/successar/FRESH.'],\n",
       " ['Machine reading comprehension has made great progress in recent years owing to large-scale annotated datasets.',\n",
       "  'In the clinical domain, however, creating such datasets is quite difficult due to the domain expertise required for annotation.',\n",
       "  'Recently, Pampari et al.',\n",
       "  '(EMNLP’18) tackled this issue by using expert-annotated question templates and existing i2b2 annotations to create emrQA, the first large-scale dataset for question answering (QA) based on clinical notes.',\n",
       "  'In this paper, we provide an in-depth analysis of this dataset and the clinical reading comprehension (CliniRC) task.',\n",
       "  'From our qualitative analysis, we find that (i) emrQA answers are often incomplete, and (ii) emrQA questions are often answerable without using domain knowledge.',\n",
       "  'From our quantitative experiments, surprising results include that (iii) using a small sampled subset (5%-20%), we can obtain roughly equal performance compared to the model trained on the entire dataset, (iv) this performance is close to human expert’s performance, and (v) BERT models do not beat the best performing base model.',\n",
       "  'Following our analysis of the emrQA, we further explore two desired aspects of CliniRC systems: the ability to utilize clinical domain knowledge and to generalize to unseen questions and contexts.',\n",
       "  'We argue that both should be considered when creating future datasets.'],\n",
       " ['Transformer-based QA models use input-wide self-attention – i.e.',\n",
       "  'across both the question and the input passage – at all layers, causing them to be slow and memory-intensive.',\n",
       "  'It turns out that we can get by without input-wide self-attention at all layers, especially in the lower layers.',\n",
       "  'We introduce DeFormer, a decomposed transformer, which substitutes the full self-attention with question-wide and passage-wide self-attentions in the lower layers.',\n",
       "  'This allows for question-independent processing of the input text representations, which in turn enables pre-computing passage representations reducing runtime compute drastically.',\n",
       "  'Furthermore, because DeFormer is largely similar to the original model, we can initialize DeFormer with the pre-training weights of a standard transformer, and directly fine-tune on the target QA dataset.',\n",
       "  'We show DeFormer versions of BERT and XLNet can be used to speed up QA by over 4.3x and with simple distillation-based losses they incur only a 1% drop in accuracy.',\n",
       "  'We open source the code at https://github.com/StonyBrookNLP/deformer.'],\n",
       " ['Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges.',\n",
       "  'Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG.',\n",
       "  'Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer.',\n",
       "  'KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA.',\n",
       "  'Recent research on multi-hop KGQA has attempted to handle KG sparsity using relevant external text, which isn’t always readily available.',\n",
       "  'In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction.',\n",
       "  'Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far.',\n",
       "  'We fill this gap in this paper and propose EmbedKGQA.',\n",
       "  'EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs.',\n",
       "  'EmbedKGQA also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods.',\n",
       "  'Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA’s effectiveness over other state-of-the-art baselines.'],\n",
       " ['Question Answering (QA) is in increasing demand as the amount of information available online and the desire for quick access to this content grows.',\n",
       "  'A common approach to QA has been to fine-tune a pretrained language model on a task-specific labeled dataset.',\n",
       "  'This paradigm, however, relies on scarce, and costly to obtain, large-scale human-labeled data.',\n",
       "  'We propose an unsupervised approach to training QA models with generated pseudo-training data.',\n",
       "  'We show that generating questions for QA training by applying a simple template on a related, retrieved sentence rather than the original context sentence improves downstream QA performance by allowing the model to learn more complex context-question relationships.',\n",
       "  'Training a QA model on this data gives a relative improvement over a previous unsupervised model in F1 score on the SQuAD dataset by about 14%, and 20% when the answer is a named entity, achieving state-of-the-art performance on SQuAD for unsupervised QA.'],\n",
       " ['Evidence retrieval is a critical stage of question answering (QA), necessary not only to improve performance, but also to explain the decisions of the QA method.',\n",
       "  'We introduce a simple, fast, and unsupervised iterative evidence retrieval method, which relies on three ideas: (a) an unsupervised alignment approach to soft-align questions and answers with justification sentences using only GloVe embeddings, (b) an iterative process that reformulates queries focusing on terms that are not covered by existing justifications, which (c) stops when the terms in the given question and candidate answers are covered by the retrieved justifications.',\n",
       "  'Despite its simplicity, our approach outperforms all the previous methods (including supervised methods) on the evidence selection task on two datasets: MultiRC and QASC.',\n",
       "  'When these evidence sentences are fed into a RoBERTa answer classification component, we achieve state-of-the-art QA performance on these two datasets.'],\n",
       " ['A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions.',\n",
       "  'We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants.',\n",
       "  'Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages.',\n",
       "  'However, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available.',\n",
       "  'We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings.',\n",
       "  'Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io.'],\n",
       " ['Discourse representation structures (DRSs) are scoped semantic representations for texts of arbitrary length.',\n",
       "  'Evaluating the accuracy of predicted DRSs plays a key role in developing semantic parsers and improving their performance.',\n",
       "  'DRSs are typically visualized as boxes which are not straightforward to process automatically.',\n",
       "  'Counter transforms DRSs to clauses and measures clause overlap by searching for variable mappings between two DRSs.',\n",
       "  'However, this metric is computationally costly (with respect to memory and CPU time) and does not scale with longer texts.',\n",
       "  'We introduce Dscorer, an efficient new metric which converts box-style DRSs to graphs and then measures the overlap of n-grams.',\n",
       "  'Experiments show that Dscorer computes accuracy scores that are correlated with Counter at a fraction of the time.'],\n",
       " ['We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software.',\n",
       "  'We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering.',\n",
       "  'We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.'],\n",
       " ['Correctly resolving textual mentions of people fundamentally entails making inferences about those people.',\n",
       "  'Such inferences raise the risk of systemic biases in coreference resolution systems, including biases that can harm binary and non-binary trans and cis stakeholders.',\n",
       "  'To better understand such biases, we foreground nuanced conceptualizations of gender from sociology and sociolinguistics, and develop two new datasets for interrogating bias in crowd annotations and in existing coreference resolution systems.',\n",
       "  'Through these studies, conducted on English text, we confirm that without acknowledging and building systems that recognize the complexity of gender, we build systems that lead to many potential harms.'],\n",
       " ['Motivated by human attention, computational attention mechanisms have been designed to help neural networks adjust their focus on specific parts of the input data.',\n",
       "  'While attention mechanisms are claimed to achieve interpretability, little is known about the actual relationships between machine and human attention.',\n",
       "  'In this work, we conduct the first quantitative assessment of human versus computational attention mechanisms for the text classification task.',\n",
       "  'To achieve this, we design and conduct a large-scale crowd-sourcing study to collect human attention maps that encode the parts of a text that humans focus on when conducting text classification.',\n",
       "  'Based on this new resource of human attention dataset for text classification, YELP-HAT, collected on the publicly available YELP dataset, we perform a quantitative comparative analysis of machine attention maps created by deep learning models and human attention maps.',\n",
       "  'Our analysis offers insights into the relationships between human versus machine attention maps along three dimensions: overlap in word selections, distribution over lexical categories, and context-dependency of sentiment polarity.',\n",
       "  'Our findings open promising future research opportunities ranging from supervised attention to the design of human-centric attention-based explanations.'],\n",
       " ['The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually “know” about natural language.',\n",
       "  'Probes are a natural way of assessing this.',\n",
       "  'When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network’s learned representations.',\n",
       "  'If the probe does well, the researcher may conclude that the representations encode knowledge related to the task.',\n",
       "  'A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself.',\n",
       "  'We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation.',\n",
       "  'The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines.',\n",
       "  'We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research—plus English—totalling eleven languages.',\n",
       "  'Our implementation is available in https://github.com/rycolab/info-theoretic-probing.'],\n",
       " ['State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting.',\n",
       "  'This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions.',\n",
       "  'We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level.',\n",
       "  'More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers.',\n",
       "  'This approach does not rely on a shared vocabulary or joint training.',\n",
       "  'However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD).',\n",
       "  'Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages.',\n",
       "  'We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.'],\n",
       " ['This paper investigates contextual word representation models from the lens of similarity analysis.',\n",
       "  'Given a collection of trained models, we measure the similarity of their internal representations and attention.',\n",
       "  'Critically, these models come from vastly different architectures.',\n",
       "  'We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation.',\n",
       "  'The analysis reveals that models within the same family are more similar to one another, as may be expected.',\n",
       "  'Surprisingly, different architectures have rather similar representations, but different individual neurons.',\n",
       "  'We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.'],\n",
       " ['The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding.',\n",
       "  'However, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content.',\n",
       "  'This paper proposes a method to employ weak-supervision directly at the word sense level.',\n",
       "  'Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses.',\n",
       "  'Accordingly, we attain a lexical-semantic level language model, without the use of human annotation.',\n",
       "  'SenseBERT achieves significantly improved lexical understanding, as we demonstrate by experimenting on SemEval Word Sense Disambiguation, and by attaining a state of the art result on the ‘Word in Context’ task.'],\n",
       " ['In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e.',\n",
       "  'replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary.',\n",
       "  'Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting.',\n",
       "  'This makes it impossible to understand the ability of simplification models in more realistic settings.',\n",
       "  'To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English.',\n",
       "  'ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations.',\n",
       "  'Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task.',\n",
       "  'Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed.'],\n",
       " ['Thanks to the wealth of high-quality annotated images available in popular repositories such as ImageNet, multimodal language-vision research is in full bloom.',\n",
       "  'However, events, feelings and many other kinds of concepts which can be visually grounded are not well represented in current datasets.',\n",
       "  'Nevertheless, we would expect a wide-coverage language understanding system to be able to classify images depicting recess and remorse, not just cats, dogs and bridges.',\n",
       "  'We fill this gap by presenting BabelPic, a hand-labeled dataset built by cleaning the image-synset association found within the BabelNet Lexical Knowledge Base (LKB).',\n",
       "  'BabelPic explicitly targets non-concrete concepts, thus providing refreshing new data for the community.',\n",
       "  'We also show that pre-trained language-vision systems can be used to further expand the resource by exploiting natural language knowledge available in the LKB.',\n",
       "  'BabelPic is available for download at http://babelpic.org.'],\n",
       " ['Predicting how events induce emotions in the characters of a story is typically seen as a standard multi-label classification task, which usually treats labels as anonymous classes to predict.',\n",
       "  'They ignore information that may be conveyed by the emotion labels themselves.',\n",
       "  'We propose that the semantics of emotion labels can guide a model’s attention when representing the input story.',\n",
       "  'Further, we observe that the emotions evoked by an event are often related: an event that evokes joy is unlikely to also evoke sadness.',\n",
       "  'In this work, we explicitly model label classes via label embeddings, and add mechanisms that track label-label correlations both during training and inference.',\n",
       "  'We also introduce a new semi-supervision strategy that regularizes for the correlations on unlabeled data.',\n",
       "  'Our empirical evaluations show that modeling label semantics yields consistent benefits, and we advance the state-of-the-art on an emotion inference task.'],\n",
       " ['We propose a semantic parsing dataset focused on instruction-driven communication with an agent in the game Minecraft.',\n",
       "  'The dataset consists of 7K human utterances and their corresponding parses.',\n",
       "  'Given proper world state, the parses can be interpreted and executed in game.',\n",
       "  'We report the performance of baseline models, and analyze their successes and failures.'],\n",
       " ['Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address.',\n",
       "  'They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws.In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019) to these cases.',\n",
       "  'We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues.',\n",
       "  'For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability.',\n",
       "  'We demonstrate the efficacy of our approach across several dialogue tasks.'],\n",
       " ['Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems.',\n",
       "  'Yet explaining their decisions is difficult despite recent work probing their internal representations.',\n",
       "  'We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency.',\n",
       "  'We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue.',\n",
       "  'We find that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models.',\n",
       "  'However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.'],\n",
       " ['LSTM-based recurrent neural networks are the state-of-the-art for many natural language processing (NLP) tasks.',\n",
       "  'Despite their performance, it is unclear whether, or how, LSTMs learn structural features of natural languages such as subject-verb number agreement in English.',\n",
       "  'Lacking this understanding, the generality of LSTM performance on this task and their suitability for related tasks remains uncertain.',\n",
       "  'Further, errors cannot be properly attributed to a lack of structural capability, training data omissions, or other exceptional faults.',\n",
       "  'We introduce *influence paths*, a causal account of structural properties as carried by paths across gates and neurons of a recurrent neural network.',\n",
       "  'The approach refines the notion of influence (the subject’s grammatical number has influence on the grammatical number of the subsequent verb) into a set of gate or neuron-level paths.',\n",
       "  'The set localizes and segments the concept (e.g., subject-verb agreement), its constituent elements (e.g., the subject), and related or interfering elements (e.g., attractors).',\n",
       "  'We exemplify the methodology on a widely-studied multi-layer LSTM language model, demonstrating its accounting for subject-verb number agreement.',\n",
       "  'The results offer both a finer and a more complete view of an LSTM’s handling of this structural aspect of the English language than prior results based on diagnostic classifiers and ablation.'],\n",
       " ['Contextualized representations (e.g.',\n",
       "  'ELMo, BERT) have become the default pretrained representations for downstream NLP applications.',\n",
       "  'In some settings, this transition has rendered their static embedding predecessors (e.g.',\n",
       "  'Word2Vec, GloVe) obsolete.',\n",
       "  'As a side-effect, we observe that older interpretability methods for static embeddings — while more diverse and mature than those available for their dynamic counterparts — are underutilized in studying newer contextualized representations.',\n",
       "  'Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights.',\n",
       "  'Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation.',\n",
       "  'Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data.',\n",
       "  'Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.'],\n",
       " ['Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing.',\n",
       "  'In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders.',\n",
       "  'We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks.',\n",
       "  'Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions.',\n",
       "  'Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy.',\n",
       "  'Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender.',\n",
       "  'Consequently, our results cast doubt on attention’s reliability as a tool for auditing algorithms in the context of fairness and accountability.'],\n",
       " ['We propose a general framework to study language emergence through signaling games with neural agents.',\n",
       "  'Using a continuous latent space, we are able to (i) train using backpropagation, (ii) show that discrete messages nonetheless naturally emerge.',\n",
       "  'We explore whether categorical perception effects follow and show that the messages are not compositional.'],\n",
       " ['Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect.',\n",
       "  'To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words.',\n",
       "  'We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability—but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance.',\n",
       "  'We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.'],\n",
       " ['Videos convey rich information.',\n",
       "  'Dynamic spatio-temporal relationships between people/objects, and diverse multimodal events are present in a video clip.',\n",
       "  'Hence, it is important to develop automated models that can accurately extract such information from videos.',\n",
       "  'Answering questions on videos is one of the tasks which can evaluate such AI abilities.',\n",
       "  'In this paper, we propose a video question answering model which effectively integrates multi-modal input sources and finds the temporally relevant information to answer questions.',\n",
       "  'Specifically, we first employ dense image captions to help identify objects and their detailed salient regions and actions, and hence give the model useful extra information (in explicit textual format to allow easier matching) for answering questions.',\n",
       "  'Moreover, our model is also comprised of dual-level attention (word/object and frame level), multi-head self/cross-integration for different sources (video and dense captions), and gates which pass more relevant information to the classifier.',\n",
       "  'Finally, we also cast the frame selection problem as a multi-label classification task and introduce two loss functions, In-andOut Frame Score Margin (IOFSM) and Balanced Binary Cross-Entropy (BBCE), to better supervise the model with human importance annotations.',\n",
       "  'We evaluate our model on the challenging TVQA dataset, where each of our model components provides significant gains, and our overall model outperforms the state-of-the-art by a large margin (74.09% versus 70.52%).',\n",
       "  'We also present several word, object, and frame level visualization studies.'],\n",
       " ['By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models.',\n",
       "  'We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time.',\n",
       "  'Existing models for this setting sample new descriptions at test time and use those to classify images.',\n",
       "  'Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language.',\n",
       "  'LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.'],\n",
       " ['While much work on deep latent variable models of text uses continuous latent variables, discrete latent variables are interesting because they are more interpretable and typically more space efficient.',\n",
       "  'We consider several approaches to learning discrete latent variable models for text in the case where exact marginalization over these variables is intractable.',\n",
       "  'We compare the performance of the learned representations as features for low-resource document and sentence classification.',\n",
       "  'Our best models outperform the previous best reported results with continuous representations in these low-resource settings, while learning significantly more compressed representations.',\n",
       "  'Interestingly, we find that an amortized variant of Hard EM performs particularly well in the lowest-resource regimes.'],\n",
       " ['Various natural language processing tasks are structured prediction problems where outputs are constructed with multiple interdependent decisions.',\n",
       "  'Past work has shown that domain knowledge, framed as constraints over the output space, can help improve predictive accuracy.',\n",
       "  'However, designing good constraints often relies on domain expertise.',\n",
       "  'In this paper, we study the problem of learning such constraints.',\n",
       "  'We frame the problem as that of training a two-layer rectifier network to identify valid structures or substructures, and show a construction for converting a trained network into a system of linear constraints over the inference variables.',\n",
       "  'Our experiments on several NLP tasks show that the learned constraints can improve the prediction accuracy, especially when the number of training examples is small.'],\n",
       " ['Recent models for unsupervised representation learning of text have employed a number of techniques to improve contextual word representations but have put little focus on discourse-level representations.',\n",
       "  'We propose Conpono, an inter-sentence objective for pretraining language models that models discourse coherence and the distance between sentences.',\n",
       "  'Given an anchor sentence, our model is trained to predict the text k sentences away using a sampled-softmax objective where the candidates consist of neighboring sentences and sentences randomly sampled from the corpus.',\n",
       "  'On the discourse representation benchmark DiscoEval, our model improves over the previous state-of-the-art by up to 13% and on average 4% absolute across 7 tasks.',\n",
       "  'Our model is the same size as BERT-Base, but outperforms the much larger BERT-Large model and other more recent approaches that incorporate discourse.',\n",
       "  'We also show that Conpono yields gains of 2%-6% absolute even for tasks that do not explicitly evaluate discourse: textual entailment (RTE), common sense reasoning (COPA) and reading comprehension (ReCoRD).'],\n",
       " ['Many high-level procedural tasks can be decomposed into sequences of instructions that vary in their order and choice of tools.',\n",
       "  'In the cooking domain, the web offers many, partially-overlapping, text and video recipes (i.e.',\n",
       "  'procedures) that describe how to make the same dish (i.e.',\n",
       "  'high-level task).',\n",
       "  'Aligning instructions for the same dish across different sources can yield descriptive visual explanations that are far richer semantically than conventional textual instructions, providing commonsense insight into how real-world procedures are structured.',\n",
       "  'Learning to align these different instruction sets is challenging because: a) different recipes vary in their order of instructions and use of ingredients; and b) video instructions can be noisy and tend to contain far more information than text instructions.',\n",
       "  'To address these challenges, we use an unsupervised alignment algorithm that learns pairwise alignments between instructions of different recipes for the same dish.',\n",
       "  'We then use a graph algorithm to derive a joint alignment between multiple text and multiple video recipes for the same dish.',\n",
       "  'We release the Microsoft Research Multimodal Aligned Recipe Corpus containing ~150K pairwise alignments between recipes across 4262 dishes with rich commonsense information.'],\n",
       " ['We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure.',\n",
       "  'We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set.',\n",
       "  'Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses.',\n",
       "  'The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate.'],\n",
       " ['Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors.',\n",
       "  'Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models.',\n",
       "  'CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly.',\n",
       "  'We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models.',\n",
       "  'In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model.',\n",
       "  'In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.'],\n",
       " ['There is an increasing interest in studying natural language and computer code together, as large corpora of programming texts become readily available on the Internet.',\n",
       "  'For example, StackOverflow currently has over 15 million programming related questions written by 8.5 million users.',\n",
       "  'Meanwhile, there is still a lack of fundamental NLP techniques for identifying code tokens or software-related named entities that appear within natural language sentences.',\n",
       "  'In this paper, we introduce a new named entity recognition (NER) corpus for the computer programming domain, consisting of 15,372 sentences annotated with 20 fine-grained entity types.',\n",
       "  'We trained in-domain BERT representations (BERTOverflow) on 152 million sentences from StackOverflow, which lead to an absolute increase of +10 F1 score over off-the-shelf BERT.',\n",
       "  'We also present the SoftNER model which achieves an overall 79.10 F-1 score for code and named entity recognition on StackOverflow data.',\n",
       "  'Our SoftNER model incorporates a context-independent code token classifier with corpus-level features to improve the BERT-based tagging model.',\n",
       "  'Our code and data are available at: https://github.com/jeniyat/StackOverflowNER/'],\n",
       " ['We present the first human-annotated dialogue-based relation extraction (RE) dataset DialogRE, aiming to support the prediction of relation(s) between two arguments that appear in a dialogue.',\n",
       "  'We further offer DialogRE as a platform for studying cross-sentence RE as most facts span multiple sentences.',\n",
       "  'We argue that speaker-related information plays a critical role in the proposed task, based on an analysis of similarities and differences between dialogue-based and traditional RE tasks.',\n",
       "  'Considering the timeliness of communication in a dialogue, we design a new metric to evaluate the performance of RE methods in a conversational setting and investigate the performance of several representative RE methods on DialogRE.',\n",
       "  'Experimental results demonstrate that a speaker-aware extension on the best-performing model leads to gains in both the standard and conversational evaluation settings.',\n",
       "  'DialogRE is available at https://dataset.org/dialogre/.'],\n",
       " ['Commonly adopted metrics for extractive summarization focus on lexical overlap at the token level.',\n",
       "  'In this paper, we present a facet-aware evaluation setup for better assessment of the information coverage in extracted summaries.',\n",
       "  'Specifically, we treat each sentence in the reference summary as a facet, identify the sentences in the document that express the semantics of each facet as support sentences of the facet, and automatically evaluate extractive summarization methods by comparing the indices of extracted sentences and support sentences of all the facets in the reference summary.',\n",
       "  'To facilitate this new evaluation setup, we construct an extractive version of the CNN/Daily Mail dataset and perform a thorough quantitative investigation, through which we demonstrate that facet-aware evaluation manifests better correlation with human judgment than ROUGE, enables fine-grained evaluation as well as comparative analysis, and reveals valuable insights of state-of-the-art summarization methods.',\n",
       "  'Data can be found at https://github.com/morningmoni/FAR.'],\n",
       " ['Automated generation of conversational dialogue using modern neural architectures has made notable advances.',\n",
       "  'However, these models are known to have a drawback of often producing uninteresting, predictable responses; this is known as the diversity problem.',\n",
       "  'We introduce a new strategy to address this problem, called Diversity-Informed Data Collection.',\n",
       "  'Unlike prior approaches, which modify model architectures to solve the problem, this method uses dynamically computed corpus-level statistics to determine which conversational participants to collect data from.',\n",
       "  'Diversity-Informed Data Collection produces significantly more diverse data than baseline data collection methods, and better results on two downstream tasks: emotion classification and dialogue generation.',\n",
       "  'This method is generalizable and can be used with other corpus-level metrics.'],\n",
       " ['We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines.',\n",
       "  'The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers.',\n",
       "  'Full text is annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects.',\n",
       "  'In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academic text to date.',\n",
       "  'We hope this resource will facilitate research and development of tools and tasks for text mining over academic text.'],\n",
       " ['Automatic metrics are fundamental for the development and evaluation of machine translation systems.',\n",
       "  'Judging whether, and to what extent, automatic metrics concur with the gold standard of human evaluation is not a straightforward problem.',\n",
       "  'We show that current methods for judging metrics are highly sensitive to the translations used for assessment, particularly the presence of outliers, which often leads to falsely confident conclusions about a metric’s efficacy.',\n",
       "  'Finally, we turn to pairwise system ranking, developing a method for thresholding performance improvement under an automatic metric against human judgements, which allows quantification of type I versus type II errors incurred, i.e., insignificant human differences in system quality that are accepted, and significant human differences that are rejected.',\n",
       "  'Together, these findings suggest improvements to the protocols for metric evaluation and system performance evaluation in machine translation.'],\n",
       " ['Generating a readable summary that describes the functionality of a program is known as source code summarization.',\n",
       "  'In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial.',\n",
       "  'To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies.',\n",
       "  'In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin.',\n",
       "  'We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens’ position hinders, while relative encoding significantly improves the summarization performance.',\n",
       "  'We have made our code publicly available to facilitate future research.'],\n",
       " ['Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input.',\n",
       "  'Existing automatic evaluation metrics for summarization are largely insensitive to such errors.',\n",
       "  'We propose QAGS (pronounced “kags”), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary.',\n",
       "  'QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source.',\n",
       "  'To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets.',\n",
       "  'QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics.',\n",
       "  'Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why.',\n",
       "  'We believe QAGS is a promising tool in automatically generating usable and factually consistent text.',\n",
       "  'Code for QAGS will be available at https://github.com/W4ngatang/qags.'],\n",
       " ['Recently BERT has been adopted for document encoding in state-of-the-art text summarization models.',\n",
       "  'However, sentence-based extractive models often result in redundant or uninformative phrases in the extracted summaries.',\n",
       "  'Also, long-range dependencies throughout a document are not well captured by BERT, which is pre-trained on sentence pairs instead of documents.',\n",
       "  'To address these issues, we present a discourse-aware neural summarization model - DiscoBert.',\n",
       "  'DiscoBert extracts sub-sentential discourse units (instead of sentences) as candidates for extractive selection on a finer granularity.',\n",
       "  'To capture the long-range dependencies among discourse units, structural discourse graphs are constructed based on RST trees and coreference mentions, encoded with Graph Convolutional Networks.',\n",
       "  'Experiments show that the proposed model outperforms state-of-the-art methods by a significant margin on popular summarization benchmarks compared to other BERT-base models.'],\n",
       " ['Automatic sentence summarization produces a shorter version of a sentence, while preserving its most important information.',\n",
       "  'A good summary is characterized by language fluency and high information overlap with the source sentence.',\n",
       "  'We model these two aspects in an unsupervised objective function, consisting of language modeling and semantic similarity metrics.',\n",
       "  'We search for a high-scoring summary by discrete optimization.',\n",
       "  'Our proposed method achieves a new state-of-the art for unsupervised sentence summarization according to ROUGE scores.',\n",
       "  'Additionally, we demonstrate that the commonly reported ROUGE F1 metric is sensitive to summary length.',\n",
       "  'Since this is unwillingly exploited in recent work, we emphasize that future evaluation should explicitly group summarization systems by output length brackets.'],\n",
       " ['We present a new summarization task, generating summaries of novel chapters using summary/chapter pairs from online study guides.',\n",
       "  'This is a harder task than the news summarization task, given the chapter length as well as the extreme paraphrasing and generalization found in the summaries.',\n",
       "  'We focus on extractive summarization, which requires the creation of a gold-standard set of extractive summaries.',\n",
       "  'We present a new metric for aligning reference summary sentences with chapter sentences to create gold extracts and also experiment with different alignment methods.',\n",
       "  'Our experiments demonstrate significant improvement over prior alignment approaches for our task as shown through automatic metrics and a crowd-sourced pyramid analysis.'],\n",
       " ['Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e.',\n",
       "  'unfaithful.',\n",
       "  'Existing automatic metrics do not capture such mistakes effectively.',\n",
       "  'We tackle the problem of evaluating faithfulness of a generated summary given its source document.',\n",
       "  'We first collected human annotations of faithfulness for outputs from numerous models on two datasets.',\n",
       "  'We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful.',\n",
       "  'Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension.',\n",
       "  'Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary.',\n",
       "  'Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.'],\n",
       " ['Abstractive summarisation is notoriously hard to evaluate since standard word-overlap-based metrics are insufficient.',\n",
       "  'We introduce a new evaluation metric which is based on fact-level content weighting, i.e.',\n",
       "  'relating the facts of the document to the facts of the summary.',\n",
       "  'We fol- low the assumption that a good summary will reflect all relevant facts, i.e.',\n",
       "  'the ones present in the ground truth (human-generated refer- ence summary).',\n",
       "  'We confirm this hypothe- sis by showing that our weightings are highly correlated to human perception and compare favourably to the recent manual highlight- based metric of Hardy et al.',\n",
       "  '(2019).'],\n",
       " ['Current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles.',\n",
       "  'We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), thus attracting more readers.',\n",
       "  'With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates stylistic headlines by combining the summarization and reconstruction tasks into a multitasking framework.',\n",
       "  'We also introduced a novel parameter sharing scheme to further disentangle the style from text.',\n",
       "  'Through both automatic and human evaluation, we demonstrate that TitleStylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait.',\n",
       "  'The attraction score of our model generated headlines outperforms the state-of-the-art summarization model by 9.68%, even outperforming human-written references.'],\n",
       " ['Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive.',\n",
       "  'We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries.',\n",
       "  'In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD.',\n",
       "  'We propose the use of dual encoders—a sequential document encoder and a graph-structured encoder—to maintain the global context and local characteristics of entities, complementing each other.',\n",
       "  'We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions.',\n",
       "  'Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets.',\n",
       "  'We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models.',\n",
       "  'Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.'],\n",
       " ['Neural abstractive summarization models are able to generate summaries which have high overlap with human references.',\n",
       "  'However, existing models are not optimized for factual correctness, a critical metric in real-world applications.',\n",
       "  'In this work, we develop a general framework where we evaluate the factual correctness of a generated summary by fact-checking it automatically against its reference using an information extraction module.',\n",
       "  'We further propose a training strategy which optimizes a neural summarization model with a factual correctness reward via reinforcement learning.',\n",
       "  'We apply the proposed method to the summarization of radiology reports, where factual correctness is a key requirement.',\n",
       "  'On two separate datasets collected from hospitals, we show via both automatic and human evaluation that the proposed approach substantially improves the factual correctness and overall quality of outputs over a competitive neural summarization system, producing radiology summaries that approach the quality of human-authored ones.'],\n",
       " ['This paper describes the Critical Role Dungeons and Dragons Dataset (CRD3) and related analyses.',\n",
       "  'Critical Role is an unscripted, live-streamed show where a fixed group of people play Dungeons and Dragons, an open-ended role-playing game.',\n",
       "  'The dataset is collected from 159 Critical Role episodes transcribed to text dialogues, consisting of 398,682 turns.',\n",
       "  'It also includes corresponding abstractive summaries collected from the Fandom wiki.',\n",
       "  'The dataset is linguistically unique in that the narratives are generated entirely through player collaboration and spoken interaction.',\n",
       "  'For each dialogue, there are a large number of turns, multiple abstractive summaries with varying levels of detail, and semantic ties to the previous dialogues.',\n",
       "  'In addition, we provide a data augmentation method that produces 34,243 summary-dialogue chunk pairs to support current neural ML approaches, and we provide an abstractive summarization benchmark and evaluation.'],\n",
       " ['This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint.',\n",
       "  'It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary.',\n",
       "  'A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score summaries.',\n",
       "  'When tested on popular news summarization datasets, the method outperforms previous unsupervised methods by more than 2 R-1 points, and approaches results of competitive supervised methods.',\n",
       "  'Our model attains higher levels of abstraction with copied passages roughly two times shorter than prior work, and learns to compress and merge sentences without supervision.'],\n",
       " ['Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents, such as product reviews.',\n",
       "  'While the majority of previous work has focused on the extractive setting, i.e., selecting fragments from input reviews to produce a summary, we let the model generate novel sentences and hence produce abstractive summaries.',\n",
       "  'Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs.',\n",
       "  'Since such training data is expensive to acquire, we instead consider the unsupervised setting, in other words, we do not use any summaries in training.',\n",
       "  'We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product, we should be able to control the “amount of novelty” going into the new review or, equivalently, vary the extent to which it deviates from the input.',\n",
       "  'At test time, when generating summaries, we force the novelty to be minimal, and produce a text reflecting consensus opinions.',\n",
       "  'We capture this intuition by defining a hierarchical variational autoencoder model.',\n",
       "  'Both individual reviews and the products they correspond to are associated with stochastic latent codes, and the review generator (“decoder”) has direct access to the text of input reviews through the pointer-generator mechanism.',\n",
       "  'Experiments on Amazon and Yelp datasets, show that setting at test time the review’s latent code to its mean, allows the model to produce fluent and coherent summaries reflecting common opinions.'],\n",
       " ['Human speakers have an extensive toolkit of ways to express themselves.',\n",
       "  'In this paper, we engage with an idea largely absent from discussions of meaning in natural language understanding—namely, that the way something is expressed reflects different ways of conceptualizing or construing the information being conveyed.',\n",
       "  'We first define this phenomenon more precisely, drawing on considerable prior work in theoretical cognitive semantics and psycholinguistics.',\n",
       "  'We then survey some dimensions of construed meaning and show how insights from construal could inform theoretical and practical work in NLP.'],\n",
       " ['The success of the large neural language models on many NLP tasks is exciting.',\n",
       "  'However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”.',\n",
       "  'In this position paper, we argue that a system trained only on form has a priori no way to learn meaning.',\n",
       "  'In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.'],\n",
       " ['We extracted information from the ACL Anthology (AA) and Google Scholar (GS) to examine trends in citations of NLP papers.',\n",
       "  'We explore questions such as: how well cited are papers of different types (journal articles, conference papers, demo papers, etc.)? how well cited are papers from different areas of within NLP? etc.',\n",
       "  'Notably, we show that only about 56% of the papers in AA are cited ten or more times.',\n",
       "  'CL Journal has the most cited papers, but its citation dominance has lessened in recent years.',\n",
       "  'On average, long papers get almost three times as many citations as short papers; and papers on sentiment classification, anaphora resolution, and entity recognition have the highest median citations.',\n",
       "  'The analyses presented here, and the associated dataset of NLP papers mapped to citations, have a number of uses including: understanding how the field is growing and quantifying the impact of different types of papers.'],\n",
       " ['This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding.',\n",
       "  'This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set.',\n",
       "  'This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set.',\n",
       "  'This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way.',\n",
       "  'We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.'],\n",
       " ['Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain.',\n",
       "  'In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork.',\n",
       "  'Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods.',\n",
       "  'In this paper, we introduce the history, the current state, and the future directions of research in LegalAI.',\n",
       "  'We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI.',\n",
       "  'We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions.',\n",
       "  'You can find the implementation of our work from https://github.com/thunlp/CLAIM.'],\n",
       " ['While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task.',\n",
       "  'However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task.',\n",
       "  'To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations.',\n",
       "  'We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer.',\n",
       "  'We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best.',\n",
       "  'We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution.',\n",
       "  'However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks.',\n",
       "  'We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.'],\n",
       " ['An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models).',\n",
       "  'However, these works have been conducted individually, without a unifying framework to organize efforts within the field.',\n",
       "  'This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures.',\n",
       "  'In this paper, we propose a unifying predictive bias framework for NLP.',\n",
       "  'We summarize the NLP literature and suggest general mathematical definitions of predictive bias.',\n",
       "  'We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias.',\n",
       "  'Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.'],\n",
       " ['Pre-trained visually grounded language models such as ViLBERT, LXMERT, and UNITER have achieved significant performance improvement on vision-and-language tasks but what they learn during pre-training remains unclear.',\n",
       "  'In this work, we demonstrate that certain attention heads of a visually grounded language model actively ground elements of language to image regions.',\n",
       "  'Specifically, some heads can map entities to image regions, performing the task known as entity grounding.',\n",
       "  'Some heads can even detect the syntactic relations between non-entity words and image regions, tracking, for example, associations between verbs and regions corresponding to their arguments.',\n",
       "  'We denote this ability as syntactic grounding.',\n",
       "  'We verify grounding both quantitatively and qualitatively, using Flickr30K Entities as a testbed.'],\n",
       " ['Throughout a conversation, participants make choices that can orient the flow of the interaction.',\n",
       "  'Such choices are particularly salient in the consequential domain of crisis counseling, where a difficulty for counselors is balancing between two key objectives: advancing the conversation towards a resolution, and empathetically addressing the crisis situation.',\n",
       "  'In this work, we develop an unsupervised methodology to quantify how counselors manage this balance.',\n",
       "  'Our main intuition is that if an utterance can only receive a narrow range of appropriate replies, then its likely aim is to advance the conversation forwards, towards a target within that range.',\n",
       "  'Likewise, an utterance that can only appropriately follow a narrow range of possible utterances is likely aimed backwards at addressing a specific situation within that range.',\n",
       "  'By applying this intuition, we can map each utterance to a continuous orientation axis that captures the degree to which it is intended to direct the flow of the conversation forwards or backwards.',\n",
       "  'This unsupervised method allows us to characterize counselor behaviors in a large dataset of crisis counseling conversations, where we show that known counseling strategies intuitively align with this axis.',\n",
       "  'We also illustrate how our measure can be indicative of a conversation’s progress, as well as its effectiveness.'],\n",
       " ['Natural disasters (e.g., hurricanes) affect millions of people each year, causing widespread destruction in their wake.',\n",
       "  'People have recently taken to social media websites (e.g., Twitter) to share their sentiments and feelings with the larger community.',\n",
       "  'Consequently, these platforms have become instrumental in understanding and perceiving emotions at scale.',\n",
       "  'In this paper, we introduce HurricaneEmo, an emotion dataset of 15,000 English tweets spanning three hurricanes: Harvey, Irma, and Maria.',\n",
       "  'We present a comprehensive study of fine-grained emotions and propose classification tasks to discriminate between coarse-grained emotion groups.',\n",
       "  'Our best BERT model, even after task-guided pre-training which leverages unlabeled Twitter data, achieves only 68% accuracy (averaged across all groups).',\n",
       "  'HurricaneEmo serves not only as a challenging benchmark for models but also as a valuable resource for analyzing emotions in disaster-centric domains.'],\n",
       " ['Not all documents are equally important.',\n",
       "  'Language processing is increasingly finding use as a supplement for questionnaires to assess psychological attributes of consenting individuals, but most approaches neglect to consider whether all documents of an individual are equally informative.',\n",
       "  'In this paper, we present a novel model that uses message-level attention to learn the relative weight of users’ social media posts for assessing their five factor personality traits.',\n",
       "  'We demonstrate that models with message-level attention outperform those with word-level attention, and ultimately yield state-of-the-art accuracies for all five traits by using both word and message attention in combination with past approaches (an average increase in Pearson r of 2.5%).',\n",
       "  'In addition, examination of the high-signal posts identified by our model provides insight into the relationship between language and personality, helping to inform future work.'],\n",
       " ['People vary in their ability to make accurate predictions about the future.',\n",
       "  'Prior studies have shown that some individuals can predict the outcome of future events with consistently better accuracy.',\n",
       "  'This leads to a natural question: what makes some forecasters better than others? In this paper we explore connections between the language people use to describe their predictions and their forecasting skill.',\n",
       "  'Datasets from two different forecasting domains are explored: (1) geopolitical forecasts from Good Judgment Open, an online prediction forum and (2) a corpus of company earnings forecasts made by financial analysts.',\n",
       "  'We present a number of linguistic metrics which are computed over text associated with people’s predictions about the future including: uncertainty, readability, and emotion.',\n",
       "  'By studying linguistic factors associated with predictions, we are able to shed some light on the approach taken by skilled forecasters.',\n",
       "  'Furthermore, we demonstrate that it is possible to accurately predict forecasting skill using a model that is based solely on language.',\n",
       "  'This could potentially be useful for identifying accurate predictions or potentially skilled forecasters earlier.'],\n",
       " ['Many applications of computational social science aim to infer causal conclusions from non-experimental data.',\n",
       "  'Such observational data often contains confounders, variables that influence both potential causes and potential effects.',\n",
       "  'Unmeasured or latent confounders can bias causal estimates, and this has motivated interest in measuring potential confounders from observed text.',\n",
       "  'For example, an individual’s entire history of social media posts or the content of a news article could provide a rich measurement of multiple confounders.Yet, methods and applications for this problem are scattered across different communities and evaluation practices are inconsistent.This review is the first to gather and categorize these examples and provide a guide to data-processing and evaluation decisions.',\n",
       "  'Despite increased attention on adjusting for confounding using text, there are still many open problems, which we highlight in this paper.'],\n",
       " ['Ideal point models analyze lawmakers’ votes to quantify their political positions, or ideal points.',\n",
       "  'But votes are not the only way to express a political position.',\n",
       "  'Lawmakers also give speeches, release press statements, and post tweets.',\n",
       "  'In this paper, we introduce the text-based ideal point model (TBIP), an unsupervised probabilistic topic model that analyzes texts to quantify the political positions of its authors.',\n",
       "  'We demonstrate the TBIP with two types of politicized text data: U.S.',\n",
       "  'Senate speeches and senator tweets.',\n",
       "  'Though the model does not analyze their votes or political affiliations, the TBIP separates lawmakers by party, learns interpretable politicized topics, and infers ideal points close to the classical vote-based ideal points.',\n",
       "  'One benefit of analyzing texts, as opposed to votes, is that the TBIP can estimate ideal points of anyone who authors political texts, including non-voting actors.',\n",
       "  'To this end, we use it to study tweets from the 2020 Democratic presidential candidates.',\n",
       "  'Using only the texts of their tweets, it identifies them along an interpretable progressive-to-moderate spectrum.'],\n",
       " ['While national politics often receive the spotlight, the overwhelming majority of legislation proposed, discussed, and enacted is done at the state level.',\n",
       "  'Despite this fact, there is little awareness of the dynamics that lead to adopting these policies.',\n",
       "  'In this paper, we take the first step towards a better understanding of these processes and the underlying dynamics that shape them, using data-driven methods.',\n",
       "  'We build a new large-scale dataset, from multiple data sources, connecting state bills and legislator information, geographical information about their districts, and donations and donors’ information.',\n",
       "  'We suggest a novel task, predicting the legislative body’s vote breakdown for a given bill, according to different criteria of interest, such as gender, rural-urban and ideological splits.',\n",
       "  'Finally, we suggest a shared relational embedding model, representing the interactions between the text of the bill and the legislative context in which it is presented.',\n",
       "  'Our experiments show that providing this context helps improve the prediction over strong text-based models.'],\n",
       " ['Understanding human preferences, along with cultural and social nuances, lives at the heart of natural language understanding.',\n",
       "  'Concretely, we present a new task and corpus for learning alignments between machine and human preferences.',\n",
       "  'Our newly introduced problem is concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations.',\n",
       "  'Our problem is framed as a natural language inference task with crowd-sourced preference votes by human players, obtained from a gamified voting platform.',\n",
       "  'We benchmark several state-of-the-art neural models, along with BERT and friends on this task.',\n",
       "  'Our experimental results show that current state-of-the-art NLP models still leave much room for improvement.'],\n",
       " ['Understanding discourse structures of news articles is vital to effectively contextualize the occurrence of a news event.',\n",
       "  'To enable computational modeling of news structures, we apply an existing theory of functional discourse structure for news articles that revolves around the main event and create a human-annotated corpus of 802 documents spanning over four domains and three media sources.',\n",
       "  'Next, we propose several document-level neural-network models to automatically construct news content structures.',\n",
       "  'Finally, we demonstrate that incorporating system predicted news structures yields new state-of-the-art performance for event coreference resolution.',\n",
       "  'The news documents we annotated are openly available and the annotations are publicly released for future research.'],\n",
       " ['Pragmatic inferences often subtly depend on the presence or absence of linguistic features.',\n",
       "  'For example, the presence of a partitive construction (of the) increases the strength of a so-called scalar inference: listeners perceive the inference that Chris did not eat all of the cookies to be stronger after hearing “Chris ate some of the cookies” than after hearing the same utterance without a partitive, “Chris ate some cookies”.',\n",
       "  'In this work, we explore to what extent neural network sentence encoders can learn to predict the strength of scalar inferences.',\n",
       "  'We first show that an LSTM-based sentence encoder trained on an English dataset of human inference strength ratings is able to predict ratings with high accuracy (r = 0.78).',\n",
       "  'We then probe the model’s behavior using manually constructed minimal sentence pairs and corpus data.',\n",
       "  'We first that the model inferred previously established associations between linguistic features and inference strength, suggesting that the model learns to use linguistic features to predict pragmatic inferences.'],\n",
       " ['Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations.',\n",
       "  'However, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature.',\n",
       "  'In this work, we highlight these inconsistencies and propose an improved evaluation protocol.',\n",
       "  'Paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for PDTB 2.0.',\n",
       "  'Furthermore, this work is the first to explore fine-grained relation classification on PDTB 3.0.',\n",
       "  'We expect our work to serve as a point of comparison for future work, and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability.'],\n",
       " ['We propose PeTra, a memory-augmented neural network designed to track entities in its memory slots.',\n",
       "  'PeTra is trained using sparse annotation from the GAP pronoun resolution dataset and outperforms a prior memory model on the task while using a simpler architecture.',\n",
       "  'We empirically compare key modeling choices, finding that we can simplify several aspects of the design of the memory module while retaining strong performance.',\n",
       "  'To measure the people tracking capability of memory models, we (a) propose a new diagnostic evaluation based on counting the number of unique entities in text, and (b) conduct a small scale human evaluation to compare evidence of people tracking in the memory logs of PeTra relative to a previous approach.',\n",
       "  'PeTra is highly effective in both evaluations, demonstrating its ability to track people in its memory despite being trained with limited annotation.'],\n",
       " ['Zero pronoun recovery and resolution aim at recovering the dropped pronoun and pointing out its anaphoric mentions, respectively.',\n",
       "  'We propose to better explore their interaction by solving both tasks together, while the previous work treats them separately.',\n",
       "  'For zero pronoun resolution, we study this task in a more realistic setting, where no parsing trees or only automatic trees are available, while most previous work assumes gold trees.',\n",
       "  'Experiments on two benchmarks show that joint modeling significantly outperforms our baseline that already beats the previous state of the arts.'],\n",
       " ['Hate speech classifiers trained on imbalanced datasets struggle to determine if group identifiers like “gay” or “black” are used in offensive or prejudiced ways.',\n",
       "  'Such biases manifest in false positives when these identifiers are present, due to models’ inability to learn the contexts which constitute a hateful usage of identifiers.',\n",
       "  'We extract post-hoc explanations from fine-tuned BERT classifiers to detect bias towards identity terms.',\n",
       "  'Then, we propose a novel regularization technique based on these explanations that encourages models to learn from the context of group identifiers in addition to the identifiers themselves.',\n",
       "  'Our approach improved over baselines in limiting false positives on out-of-domain data while maintaining and in cases improving in-domain performance.'],\n",
       " ['Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models.',\n",
       "  'Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace.',\n",
       "  'We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms.',\n",
       "  'We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace.',\n",
       "  'Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.'],\n",
       " ['We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process.',\n",
       "  'We further find that these papers’ proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP.',\n",
       "  'Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems.',\n",
       "  'These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.'],\n",
       " ['Warning: this paper contains content that may be offensive or upsetting.',\n",
       "  'Language has the power to reinforce stereotypes and project social biases onto others.',\n",
       "  'At the core of the challenge is that it is rarely what is stated explicitly, but rather the implied meanings, that frame people’s judgments about others.',\n",
       "  'For example, given a statement that “we shouldn’t lower our standards to hire more women,” most listeners will infer the implicature intended by the speaker - that “women (candidates) are less qualified.” Most semantic formalisms, to date, do not capture such pragmatic implications in which people express social biases and power differentials in language.',\n",
       "  'We introduce Social Bias Frames, a new conceptual formalism that aims to model the pragmatic frames in which people project social biases and stereotypes onto others.',\n",
       "  'In addition, we introduce the Social Bias Inference Corpus to support large-scale modelling and evaluation with 150k structured annotations of social media posts, covering over 34k implications about a thousand demographic groups.',\n",
       "  'We then establish baseline approaches that learn to recover Social Bias Frames from unstructured text.',\n",
       "  'We find that while state-of-the-art neural models are effective at high-level categorization of whether a given statement projects unwanted social bias (80% F1), they are not effective at spelling out more detailed explanations in terms of Social Bias Frames.',\n",
       "  'Our study motivates future work that combines structured pragmatic inference with commonsense reasoning on social implications.'],\n",
       " ['Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models.',\n",
       "  'In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained.',\n",
       "  'In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis.',\n",
       "  'Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability.',\n",
       "  'We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.'],\n",
       " ['As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes.',\n",
       "  'Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs.',\n",
       "  'While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT.',\n",
       "  'In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases.',\n",
       "  'We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding.',\n",
       "  'We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.'],\n",
       " ['Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs.',\n",
       "  'A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing.',\n",
       "  'However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods.',\n",
       "  'In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem.',\n",
       "  'The proposed protocol is robust to handle bias in the model, which can substantially affect the final results.',\n",
       "  'We conduct extensive experiments and report performance of several existing methods using our protocol.',\n",
       "  'The reproducible code has been made publicly available.'],\n",
       " ['A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy.',\n",
       "  'However, these studies are based primarily on monolingual evidence from English.',\n",
       "  'To investigate how these models’ ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models.',\n",
       "  'CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop.',\n",
       "  'We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT.',\n",
       "  'Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses.',\n",
       "  'On other constructions, agreement accuracy was generally higher in languages with richer morphology.',\n",
       "  'Multilingual models generally underperformed monolingual models.',\n",
       "  'Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages.'],\n",
       " ['Algorithmic approaches to interpreting machine learning models have proliferated in recent years.',\n",
       "  'We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors.',\n",
       "  'A model is simulatable when a person can predict its behavior on new inputs.',\n",
       "  'Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method.',\n",
       "  'Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests.',\n",
       "  'We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are.',\n",
       "  'Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains.',\n",
       "  'We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.'],\n",
       " ['Modern deep learning models for NLP are notoriously opaque.',\n",
       "  'This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights.',\n",
       "  'Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text.',\n",
       "  'While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning.',\n",
       "  'In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers.',\n",
       "  'Influence functions explain the decisions of a model by identifying influential training examples.',\n",
       "  'Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work.',\n",
       "  'We conduct a comparison between influence functions and common word-saliency methods on representative tasks.',\n",
       "  'As suspected, we find that influence functions are particularly useful for natural language inference, a task in which ‘saliency maps’ may not have clear interpretation.',\n",
       "  'Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.'],\n",
       " ['Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually.',\n",
       "  'To better understand this overlap, we extend recent work on finding syntactic trees in neural networks’ internal representations to the multilingual setting.',\n",
       "  'We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages.',\n",
       "  'Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy.',\n",
       "  'This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.'],\n",
       " ['Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness.',\n",
       "  'In natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them.',\n",
       "  'It poses challenges for humans to interpret an explanation and connect it to model prediction.',\n",
       "  'In this work, we build hierarchical explanations by detecting feature interactions.',\n",
       "  'Such explanations visualize how words and phrases are combined at different levels of the hierarchy, which can help users understand the decision-making of black-box models.',\n",
       "  'The proposed method is evaluated with three neural text classifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic and human evaluations.',\n",
       "  'Experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans.'],\n",
       " ['Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture.',\n",
       "  'However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model’s reasoning; that is, that all modules perform their intended behaviour.',\n",
       "  'In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps.',\n",
       "  'We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour.',\n",
       "  'To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.'],\n",
       " ['Selecting input features of top relevance has become a popular method for building self-explaining models.',\n",
       "  'In this work, we extend this selective rationalization approach to text matching, where the goal is to jointly select and align text pieces, such as tokens or sentences, as a justification for the downstream prediction.',\n",
       "  'Our approach employs optimal transport (OT) to find a minimal cost alignment between the inputs.',\n",
       "  'However, directly applying OT often produces dense and therefore uninterpretable alignments.',\n",
       "  'To overcome this limitation, we introduce novel constrained variants of the OT problem that result in highly sparse alignments with controllable sparsity.',\n",
       "  'Our model is end-to-end differentiable using the Sinkhorn algorithm for OT and can be trained without any alignment annotations.',\n",
       "  'We evaluate our model on the StackExchange, MultiNews, e-SNLI, and MultiRC datasets.',\n",
       "  'Our model achieves very sparse rationale selections with high fidelity while preserving prediction accuracy compared to strong attention baseline models.'],\n",
       " ['Complex compositional reading comprehension datasets require performing latent sequential decisions that are learned via supervision from the final answer.',\n",
       "  'A large combinatorial space of possible decision paths that result in the same answer, compounded by the lack of intermediate supervision to help choose the right path, makes the learning particularly hard for this task.',\n",
       "  'In this work, we study the benefits of collecting intermediate reasoning supervision along with the answer during data collection.',\n",
       "  'We find that these intermediate annotations can provide two-fold benefits.',\n",
       "  'First, we observe that for any collection budget, spending a fraction of it on intermediate annotations results in improved model performance, for two complex compositional datasets: DROP and Quoref.',\n",
       "  'Second, these annotations encourage the model to learn the correct latent reasoning steps, helping combat some of the biases introduced during the data collection process.'],\n",
       " ['Answer retrieval is to find the most aligned answer from a large set of candidates given a question.',\n",
       "  'Learning vector representations of questions/answers is the key factor.',\n",
       "  'Question-answer alignment and question/answer semantics are two important signals for learning the representations.',\n",
       "  'Existing methods learned semantic representations with dual encoders or dual variational auto-encoders.',\n",
       "  'The semantic information was learned from language models or question-to-question (answer-to-answer) generative processes.',\n",
       "  'However, the alignment and semantics were too separate to capture the aligned semantics between question and answer.',\n",
       "  'In this work, we propose to cross variational auto-encoders by generating questions with aligned answers and generating answers with aligned questions.',\n",
       "  'Experiments show that our method outperforms the state-of-the-art answer retrieval method on SQuAD.'],\n",
       " ['Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events.',\n",
       "  'This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models.',\n",
       "  'Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model.',\n",
       "  'Improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (QA) tasks, including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension.',\n",
       "  'In particular, our method significantly improves the performance of RoBERTa-based models by 1-5% across datasets.',\n",
       "  'We advance state of the art by around 5-8% on WIQA and QuaRel and reduce consistency violations by 58% on HotpotQA.',\n",
       "  'We further demonstrate that our approach can learn effectively from limited data.'],\n",
       " ['Automatic question generation (QG) has shown promise as a source of synthetic training data for question answering (QA).',\n",
       "  'In this paper we ask: Is textual diversity in QG beneficial for downstream QA? Using top-p nucleus sampling to derive samples from a transformer-based question generator, we show that diversity-promoting QG indeed provides better QA training than likelihood maximization approaches such as beam search.',\n",
       "  'We also show that standard QG evaluation metrics such as BLEU, ROUGE and METEOR are inversely correlated with diversity, and propose a diversity-aware intrinsic measure of overall QG quality that correlates well with extrinsic evaluation on QA.'],\n",
       " ['We address the problem of extractive question answering using document-level distant super-vision, pairing questions and relevant documents with answer strings.',\n",
       "  'We compare previously used probability space and distant supervision assumptions (assumptions on the correspondence between the weak answer string labels and possible answer mention spans).',\n",
       "  'We show that these assumptions interact, and that different configurations provide complementary benefits.',\n",
       "  'We demonstrate that a multi-objective model can efficiently combine the advantages of multiple assumptions and outperform the best individual formulation.',\n",
       "  'Our approach outperforms previous state-of-the-art models by 4.3 points in F1 on TriviaQA-Wiki and 1.7 points in Rouge-L on NarrativeQA summaries.'],\n",
       " ['We introduce SCDE, a dataset to evaluate the performance of computational models through sentence prediction.',\n",
       "  'SCDE is a human created sentence cloze dataset, collected from public school English examinations.',\n",
       "  'Our task requires a model to fill up multiple blanks in a passage from a shared candidate set with distractors designed by English teachers.',\n",
       "  'Experimental results demonstrate that this task requires the use of non-local, discourse-level context beyond the immediate sentence neighborhood.',\n",
       "  'The blanks require joint solving and significantly impair each other’s context.',\n",
       "  'Furthermore, through ablations, we show that the distractors are of high quality and make the task more challenging.',\n",
       "  'Our experiments show that there is a significant performance gap between advanced models (72%) and humans (87%), encouraging future models to bridge this gap.'],\n",
       " ['To avoid giving wrong answers, question answering (QA) models need to know when to abstain from answering.',\n",
       "  'Moreover, users often ask questions that diverge from the model’s training data, making errors more likely and thus abstention more critical.',\n",
       "  'In this work, we propose the setting of selective question answering under domain shift, in which a QA model is tested on a mixture of in-domain and out-of-domain data, and must answer (i.e., not abstain on) as many questions as possible while maintaining high accuracy.',\n",
       "  'Abstention policies based solely on the model’s softmax probabilities fare poorly, since models are overconfident on out-of-domain inputs.',\n",
       "  'Instead, we train a calibrator to identify inputs on which the QA model errs, and abstain when it predicts an error is likely.',\n",
       "  'Crucially, the calibrator benefits from observing the model’s behavior on out-of-domain data, even if from a different domain than the test data.',\n",
       "  'We combine this method with a SQuAD-trained QA model and evaluate on mixtures of SQuAD and five other QA datasets.',\n",
       "  'Our method answers 56% of questions while maintaining 80% accuracy; in contrast, directly using the model’s probabilities only answers 48% at 80% accuracy.'],\n",
       " ['Large transformer-based language models have been shown to be very effective in many classification tasks.',\n",
       "  'However, their computational complexity prevents their use in applications requiring the classification of a large set of candidates.',\n",
       "  'While previous works have investigated approaches to reduce model size, relatively little attention has been paid to techniques to improve batch throughput during inference.',\n",
       "  'In this paper, we introduce the Cascade Transformer, a simple yet effective technique to adapt transformer-based models into a cascade of rankers.',\n",
       "  'Each ranker is used to prune a subset of candidates in a batch, thus dramatically increasing throughput at inference time.',\n",
       "  'Partial encodings from the transformer model are shared among rerankers, providing further speed-up.',\n",
       "  'When compared to a state-of-the-art transformer model, our approach reduces computation by 37% with almost no impact on accuracy, as measured on two English Question Answering datasets.'],\n",
       " ['We introduce a novel approach to transformers that learns hierarchical representations in multiparty dialogue.',\n",
       "  'First, three language modeling tasks are used to pre-train the transformers, token- and utterance-level language modeling and utterance order prediction, that learn both token and utterance embeddings for better understanding in dialogue contexts.',\n",
       "  'Then, multi-task learning between the utterance prediction and the token span prediction is applied to fine-tune for span-based question answering (QA).',\n",
       "  'Our approach is evaluated on the FriendsQA dataset and shows improvements of 3.8% and 1.4% over the two state-of-the-art transformer models, BERT and RoBERTa, respectively.'],\n",
       " ['Empirical research in Natural Language Processing (NLP) has adopted a narrow set of principles for assessing hypotheses, relying mainly on p-value computation, which suffers from several known issues.',\n",
       "  'While alternative proposals have been well-debated and adopted in other fields, they remain rarely discussed or used within the NLP community.',\n",
       "  'We address this gap by contrasting various hypothesis assessment techniques, especially those not commonly used in the field (such as evaluations based on Bayesian inference).',\n",
       "  'Since these statistical techniques differ in the hypotheses they can support, we argue that practitioners should first decide their target hypothesis before choosing an assessment method.',\n",
       "  'This is crucial because common fallacies, misconceptions, and misinterpretation surrounding hypothesis assessment methods often stem from a discrepancy between what one would like to claim versus what the method used actually assesses.',\n",
       "  'Our survey reveals that these issues are omnipresent in the NLP research community.',\n",
       "  'As a step forward, we provide best practices and guidelines tailored to NLP research, as well as an easy-to-use package for Bayesian assessment of hypotheses, complementing existing tools.'],\n",
       " ['We present STARC (Structured Annotations for Reading Comprehension), a new annotation framework for assessing reading comprehension with multiple choice questions.',\n",
       "  'Our framework introduces a principled structure for the answer choices and ties them to textual span annotations.',\n",
       "  'The framework is implemented in OneStopQA, a new high-quality dataset for evaluation and analysis of reading comprehension in English.',\n",
       "  'We use this dataset to demonstrate that STARC can be leveraged for a key new application for the development of SAT-like reading comprehension materials: automatic annotation quality probing via span ablation experiments.',\n",
       "  'We further show that it enables in-depth analyses and comparisons between machine and human reading comprehension behavior, including error distributions and guessing ability.',\n",
       "  'Our experiments also reveal that the standard multiple choice dataset in NLP, RACE, is limited in its ability to measure reading comprehension.',\n",
       "  '47% of its questions can be guessed by machines without accessing the passage, and 18% are unanimously judged by humans as not having a unique correct answer.',\n",
       "  'OneStopQA provides an alternative test set for reading comprehension which alleviates these shortcomings and has a substantially higher human ceiling performance.'],\n",
       " ['In this paper, we present the first comprehensive categorization of essential commonsense knowledge for answering the Winograd Schema Challenge (WSC).',\n",
       "  'For each of the questions, we invite annotators to first provide reasons for making correct decisions and then categorize them into six major knowledge categories.',\n",
       "  'By doing so, we better understand the limitation of existing methods (i.e., what kind of knowledge cannot be effectively represented or inferred with existing methods) and shed some light on the commonsense knowledge that we need to acquire in the future for better commonsense reasoning.',\n",
       "  'Moreover, to investigate whether current WSC models can understand the commonsense or they simply solve the WSC questions based on the statistical bias of the dataset, we leverage the collected reasons to develop a new task called WinoWhy, which requires models to distinguish plausible reasons from very similar but wrong reasons for all WSC questions.',\n",
       "  'Experimental results prove that even though pre-trained language representation models have achieved promising progress on the original WSC dataset, they are still struggling at WinoWhy.',\n",
       "  'Further experiments show that even though supervised models can achieve better performance, the performance of these models can be sensitive to the dataset distribution.',\n",
       "  'WinoWhy and all codes are available at: https://github.com/HKUST-KnowComp/WinoWhy.'],\n",
       " ['In online debates, users express different levels of agreement/disagreement with one another’s arguments and ideas.',\n",
       "  'Often levels of agreement/disagreement are implicit in the text, and must be predicted to analyze collective opinions.',\n",
       "  'Existing stance detection methods predict the polarity of a post’s stance toward a topic or post, but don’t consider the stance’s degree of intensity.',\n",
       "  'We introduce a new research problem, stance polarity and intensity prediction in response relationships between posts.',\n",
       "  'This problem is challenging because differences in stance intensity are often subtle and require nuanced language understanding.',\n",
       "  'Cyber argumentation research has shown that incorporating both stance polarity and intensity data in online debates leads to better discussion analysis.',\n",
       "  'We explore five different learning models: Ridge-M regression, Ridge-S regression, SVR-RF-R, pkudblab-PIP, and T-PAN-PIP for predicting stance polarity and intensity in argumentation.',\n",
       "  'These models are evaluated using a new dataset for stance polarity and intensity prediction collected using a cyber argumentation platform.',\n",
       "  'The SVR-RF-R model performs best for prediction of stance polarity with an accuracy of 70.43% and intensity with RMSE of 0.596.',\n",
       "  'This work is the first to train models for predicting a post’s stance polarity and intensity in one combined value in cyber argumentation with reasonably good accuracy.'],\n",
       " ['Recent neural network models have achieved impressive performance on sentiment classification in English as well as other languages.',\n",
       "  'Their success heavily depends on the availability of a large amount of labeled data or parallel corpus.',\n",
       "  'In this paper, we investigate an extreme scenario of cross-lingual sentiment classification, in which the low-resource language does not have any labels or parallel corpus.',\n",
       "  'We propose an unsupervised cross-lingual sentiment classification model named multi-view encoder-classifier (MVEC) that leverages an unsupervised machine translation (UMT) system and a language discriminator.',\n",
       "  'Unlike previous language model (LM) based fine-tuning approaches that adjust parameters solely based on the classification error on training data, we employ the encoder-decoder framework of a UMT as a regularization component on the shared network parameters.',\n",
       "  'In particular, the cross-lingual encoder of our model learns a shared representation, which is effective for both reconstructing input sentences of two languages and generating more representative views from the input for classification.',\n",
       "  'Extensive experiments on five language pairs verify that our model significantly outperforms other models for 8/11 sentiment classification tasks.'],\n",
       " ['We present an efficient annotation framework for argument quality, a feature difficult to be measured reliably as per previous work.',\n",
       "  'A stochastic transitivity model is combined with an effective sampling strategy to infer high-quality labels with low effort from crowdsourced pairwise judgments.',\n",
       "  'The model’s capabilities are showcased by compiling Webis-ArgQuality-20, an argument quality corpus that comprises scores for rhetorical, logical, dialectical, and overall quality inferred from a total of 41,859 pairwise judgments among 1,271 arguments.',\n",
       "  'With up to 93% cost savings, our approach significantly outperforms existing annotation procedures.',\n",
       "  'Furthermore, novel insight into argument quality is provided through statistical analysis, and a new aggregation method to infer overall quality from individual quality dimensions is proposed.'],\n",
       " ['This paper studies the task of comparative preference classification (CPC).',\n",
       "  'Given two entities in a sentence, our goal is to classify whether the first (or the second) entity is preferred over the other or no comparison is expressed at all between the two entities.',\n",
       "  'Existing works either do not learn entity-aware representations well and fail to deal with sentences involving multiple entity pairs or use sequential modeling approaches that are unable to capture long-range dependencies between the entities.',\n",
       "  'Some also use traditional machine learning approaches that do not generalize well.',\n",
       "  'This paper proposes a novel Entity-aware Dependency-based Deep Graph Attention Network (ED-GAT) that employs a multi-hop graph attention over a dependency graph sentence representation to leverage both the semantic information from word embeddings and the syntactic information from the dependency graph to solve the problem.',\n",
       "  'Empirical evaluation shows that the proposed model achieves the state-of-the-art performance in comparative preference classification.'],\n",
       " ['We present OpinionDigest, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training.',\n",
       "  'The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews, and trains a Transformer model to reconstruct the original reviews from these extractions.',\n",
       "  'At summarization time, we merge extractions from multiple reviews and select the most popular ones.',\n",
       "  'The selected opinions are used as input to the trained Transformer model, which verbalizes them into an opinion summary.',\n",
       "  'OpinionDigest can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and/or sentiment.',\n",
       "  'Automatic evaluation on Yelp data shows that our framework outperforms competitive baselines.',\n",
       "  'Human studies on two corpora verify that OpinionDigest produces informative summaries and shows promising customization capabilities.'],\n",
       " ['Affective tasks such as sentiment analysis, emotion classification, and sarcasm detection have been popular in recent years due to an abundance of user-generated data, accurate computational linguistic models, and a broad range of relevant applications in various domains.',\n",
       "  'At the same time, many studies have highlighted the importance of text preprocessing, as an integral step to any natural language processing prediction model and downstream task.',\n",
       "  'While preprocessing in affective systems is well-studied, preprocessing in word vector-based models applied to affective systems, is not.',\n",
       "  'To address this limitation, we conduct a comprehensive analysis of the role of preprocessing techniques in affective analysis based on word vector models.',\n",
       "  'Our analysis is the first of its kind and provides useful insights of the importance of each preprocessing technique when applied at the training phase, commonly ignored in pretrained word vector models, and/or at the downstream task phase.'],\n",
       " ['Generative dialogue systems tend to produce generic responses, which often leads to boring conversations.',\n",
       "  'For alleviating this issue, Recent studies proposed to retrieve and introduce knowledge facts from knowledge graphs.',\n",
       "  'While this paradigm works to a certain extent, it usually retrieves knowledge facts only based on the entity word itself, without considering the specific dialogue context.',\n",
       "  'Thus, the introduction of the context-irrelevant knowledge facts can impact the quality of generations.',\n",
       "  'To this end, this paper proposes a novel commonsense knowledge-aware dialogue generation model, ConKADI.',\n",
       "  'We design a Felicitous Fact mechanism to help the model focus on the knowledge facts that are highly relevant to the context; furthermore, two techniques, Context-Knowledge Fusion and Flexible Mode Fusion are proposed to facilitate the integration of the knowledge in the ConKADI.',\n",
       "  'We collect and build a large-scale Chinese dataset aligned with the commonsense knowledge for dialogue generation.',\n",
       "  'Extensive evaluations over both an open-released English dataset and our Chinese dataset demonstrate that our approach ConKADI outperforms the state-of-the-art approach CCM, in most experiments.'],\n",
       " ['Maintaining a consistent personality in conversations is quite natural for human beings, but is still a non-trivial task for machines.',\n",
       "  'The persona-based dialogue generation task is thus introduced to tackle the personality-inconsistent problem by incorporating explicit persona text into dialogue generation models.',\n",
       "  'Despite the success of existing persona-based models on generating human-like responses, their one-stage decoding framework can hardly avoid the generation of inconsistent persona words.',\n",
       "  'In this work, we introduce a three-stage framework that employs a generate-delete-rewrite mechanism to delete inconsistent words from a generated response prototype and further rewrite it to a personality-consistent one.',\n",
       "  'We carry out evaluations by both human and automatic metrics.',\n",
       "  'Experiments on the Persona-Chat dataset show that our approach achieves good performance.'],\n",
       " ['Training the generative models with minimal corpus is one of the critical challenges for building open-domain dialogue systems.',\n",
       "  'Existing methods tend to use the meta-learning framework which pre-trains the parameters on all non-target tasks then fine-tunes on the target task.',\n",
       "  'However, fine-tuning distinguishes tasks from the parameter perspective but ignores the model-structure perspective, resulting in similar dialogue models for different tasks.',\n",
       "  'In this paper, we propose an algorithm that can customize a unique dialogue model for each task in the few-shot setting.',\n",
       "  'In our approach, each dialogue model consists of a shared module, a gating module, and a private module.',\n",
       "  'The first two modules are shared among all the tasks, while the third one will differentiate into different network structures to better capture the characteristics of the corresponding task.',\n",
       "  'The extensive experiments on two datasets show that our method outperforms all the baselines in terms of task consistency, response quality, and diversity.'],\n",
       " ['Pre-trained language models have shown remarkable success in improving various downstream NLP tasks due to their ability to capture dependencies in textual data and generate natural responses.',\n",
       "  'In this paper, we leverage the power of pre-trained language models for improving video-grounded dialogue, which is very challenging and involves complex features of different dynamics: (1) Video features which can extend across both spatial and temporal dimensions; and (2) Dialogue features which involve semantic dependencies over multiple dialogue turns.',\n",
       "  'We propose a framework by extending GPT-2 models to tackle these challenges by formulating video-grounded dialogue tasks as a sequence-to-sequence task, combining both visual and textual representation into a structured sequence, and fine-tuning a large pre-trained GPT-2 network.',\n",
       "  'Our framework allows fine-tuning language models to capture dependencies across multiple modalities over different levels of information: spatio-temporal level in video and token-sentence level in dialogue context.',\n",
       "  'We achieve promising improvement on the Audio-Visual Scene-Aware Dialogues (AVSD) benchmark from DSTC7, which supports a potential direction in this line of research.'],\n",
       " ['The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not.Models are usually separately developed for the two tasks, since sequence labeling models, the most widely used backbone for flat NER, are only able to assign a single label to a particular token, which is unsuitable for nested NER where a token may be assigned several labels.',\n",
       "  'In this paper, we propose a unified framework that is capable of handling both flat and nested NER tasks.',\n",
       "  'Instead of treating the task of NER as a sequence labeling problem, we propose to formulate it as a machine reading comprehension (MRC) task.',\n",
       "  'For example, extracting entities with the per label is formalized as extracting answer spans to the question “which person is mentioned in the text\".This formulation naturally tackles the entity overlapping issue in nested NER: the extraction of two overlapping entities with different categories requires answering two independent questions.',\n",
       "  'Additionally, since the query encodes informative prior knowledge, this strategy facilitates the process of entity extraction, leading to better performances for not only nested NER, but flat NER.',\n",
       "  'We conduct experiments on both nested and flat NER datasets.Experiment results demonstrate the effectiveness of the proposed formulation.',\n",
       "  'We are able to achieve a vast amount of performance boost over current SOTA models on nested NER datasets, i.e., +1.28, +2.55, +5.44, +6.37,respectively on ACE04, ACE05, GENIA and KBP17, along with SOTA results on flat NER datasets, i.e., +0.24, +1.95, +0.21, +1.49 respectively on English CoNLL 2003, English OntoNotes 5.0, Chinese MSRA and Chinese OntoNotes 4.0.'],\n",
       " ['Unlike widely used Named Entity Recognition (NER) data sets in generic domains, biomedical NER data sets often contain mentions consisting of discontinuous spans.',\n",
       "  'Conventional sequence tagging techniques encode Markov assumptions that are efficient but preclude recovery of these mentions.',\n",
       "  'We propose a simple, effective transition-based model with generic neural encoding for discontinuous NER.',\n",
       "  'Through extensive experiments on three biomedical data sets, we show that our model can effectively recognize discontinuous mentions without sacrificing the accuracy on continuous mentions.'],\n",
       " ['While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task.',\n",
       "  'Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et.',\n",
       "  'al.',\n",
       "  '18).',\n",
       "  'Our analysis reveals that CopyAttention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information.',\n",
       "  'We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples.',\n",
       "  'This approach overcomes both shortcomings of CopyAttention, resulting in a variable number of diverse extractions per sentence.',\n",
       "  'We train IMoJIE on training data bootstrapped from extractions of several non-neural systems, which have been automatically filtered to reduce redundancy and noise.',\n",
       "  'IMoJIE outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task.'],\n",
       " ['Event Detection (ED) is a fundamental task in automatically structuring texts.',\n",
       "  'Due to the small scale of training data, previous methods perform poorly on unseen/sparsely labeled trigger words and are prone to overfitting densely labeled trigger words.',\n",
       "  'To address the issue, we propose a novel Enrichment Knowledge Distillation (EKD) model to leverage external open-domain trigger knowledge to reduce the in-built biases to frequent trigger words in annotations.',\n",
       "  'Experiments on benchmark ACE2005 show that our model outperforms nine strong baselines, is especially effective for unseen/sparsely labeled trigger words.',\n",
       "  'The source code is released on https://github.com/shuaiwa16/ekd.git.'],\n",
       " ['Exploiting sentence-level labels, which are easy to obtain, is one of the plausible methods to improve low-resource named entity recognition (NER), where token-level labels are costly to annotate.',\n",
       "  'Current models for jointly learning sentence and token labeling are limited to binary classification.',\n",
       "  'We present a joint model that supports multi-class classification and introduce a simple variant of self-attention that allows the model to learn scaling factors.',\n",
       "  'Our model produces 3.78%, 4.20%, 2.08% improvements in F1 over the BiLSTM-CRF baseline on e-commerce product titles in three different low-resource languages: Vietnamese, Thai, and Indonesian, respectively.'],\n",
       " ['Cross-domain NER is a challenging yet practical problem.',\n",
       "  'Entity mentions can be highly different across domains.',\n",
       "  'However, the correlations between entity types can be relatively more stable across domains.',\n",
       "  'We investigate a multi-cell compositional LSTM structure for multi-task learning, modeling each entity type using a separate cell state.',\n",
       "  'With the help of entity typed units, cross-domain knowledge transfer can be made in an entity type level.',\n",
       "  'Theoretically, the resulting distinct feature distributions for each entity type make it more powerful for cross-domain transfer.',\n",
       "  'Empirically, experiments on four few-shot and zero-shot datasets show our method significantly outperforms a series of multi-task learning methods and achieves the best results.'],\n",
       " ['This paper presents Pyramid, a novel layered model for Nested Named Entity Recognition (nested NER).',\n",
       "  'In our approach, token or text region embeddings are recursively inputted into L flat NER layers, from bottom to top, stacked in a pyramid shape.',\n",
       "  'Each time an embedding passes through a layer of the pyramid, its length is reduced by one.',\n",
       "  'Its hidden state at layer l represents an l-gram in the input text, which is labeled only if its corresponding text region represents a complete entity mention.',\n",
       "  'We also design an inverse pyramid to allow bidirectional interaction between layers.',\n",
       "  'The proposed method achieves state-of-the-art F1 scores in nested NER on ACE-2004, ACE-2005, GENIA, and NNE, which are 80.27, 79.42, 77.78, and 93.70 with conventional embeddings, and 87.74, 86.34, 79.31, and 94.68 with pre-trained contextualized embeddings.',\n",
       "  'In addition, our model can be used for the more general task of Overlapping Named Entity Recognition.',\n",
       "  'A preliminary experiment confirms the effectiveness of our method in overlapping NER.'],\n",
       " ['The goal of Knowledge graph embedding (KGE) is to learn how to represent the low dimensional vectors for entities and relations based on the observed triples.',\n",
       "  'The conventional shallow models are limited to their expressiveness.',\n",
       "  'ConvE (Dettmers et al., 2018) takes advantage of CNN and improves the expressive power with parameter efficient operators by increasing the interactions between head and relation embeddings.',\n",
       "  'However, there is no structural information in the embedding space of ConvE, and the performance is still limited by the number of interactions.',\n",
       "  'The recent KBGAT (Nathani et al., 2019) provides another way to learn embeddings by adaptively utilizing structural information.',\n",
       "  'In this paper, we take the benefits of ConvE and KBGAT together and propose a Relation-aware Inception network with joint local-global structural information for knowledge graph Embedding (ReInceptionE).',\n",
       "  'Specifically, we first explore the Inception network to learn query embedding, which aims to further increase the interactions between head and relation embeddings.',\n",
       "  'Then, we propose to use a relation-aware attention mechanism to enrich the query embedding with the local neighborhood and global entity information.',\n",
       "  'Experimental results on both WN18RR and FB15k-237 datasets demonstrate that ReInceptionE achieves competitive performance compared with state-of-the-art methods.'],\n",
       " ['Distant supervision based methods for entity and relation extraction have received increasing popularity due to the fact that these methods require light human annotation efforts.',\n",
       "  'In this paper, we consider the problem of shifted label distribution, which is caused by the inconsistency between the noisy-labeled training set subject to external knowledge graph and the human-annotated test set, and exacerbated by the pipelined entity-then-relation extraction manner with noise propagation.',\n",
       "  'We propose a joint extraction approach to address this problem by re-labeling noisy instances with a group of cooperative multiagents.',\n",
       "  'To handle noisy instances in a fine-grained manner, each agent in the cooperative group evaluates the instance by calculating a continuous confidence score from its own perspective; To leverage the correlations between these two extraction tasks, a confidence consensus module is designed to gather the wisdom of all agents and re-distribute the noisy training set with confidence-scored labels.',\n",
       "  'Further, the confidences are used to adjust the training losses of extractors.',\n",
       "  'Experimental results on two real-world datasets verify the benefits of re-labeling noisy instance, and show that the proposed model significantly outperforms the state-of-the-art entity and relation extraction methods.'],\n",
       " ['Recently, many works have tried to augment the performance of Chinese named entity recognition (NER) using word lexicons.',\n",
       "  'As a representative, Lattice-LSTM has achieved new benchmark results on several public Chinese NER datasets.',\n",
       "  'However, Lattice-LSTM has a complex model architecture.',\n",
       "  'This limits its application in many industrial areas where real-time NER responses are needed.',\n",
       "  'In this work, we propose a simple but effective method for incorporating the word lexicon into the character representations.',\n",
       "  'This method avoids designing a complicated sequence modeling architecture, and for any neural NER model, it requires only subtle adjustment of the character representation layer to introduce the lexicon information.',\n",
       "  'Experimental studies on four benchmark Chinese NER datasets show that our method achieves an inference speed up to 6.15 times faster than those of state-of-the-art methods, along with a better performance.',\n",
       "  'The experimental results also show that the proposed method can be easily incorporated with pre-trained models like BERT.'],\n",
       " ['In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT).',\n",
       "  'The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs.',\n",
       "  'We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning.',\n",
       "  'Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora.'],\n",
       " ['The advent of context-aware NMT has resulted in promising improvements in the overall translation quality and specifically in the translation of discourse phenomena such as pronouns.',\n",
       "  'Previous works have mainly focused on the use of past sentences as context with a focus on anaphora translation.',\n",
       "  'In this work, we investigate the effect of future sentences as context by comparing the performance of a contextual NMT model trained with the future context to the one trained with the past context.',\n",
       "  'Our experiments and evaluation, using generic and pronoun-focused automatic metrics, show that the use of future context not only achieves significant improvements over the context-agnostic Transformer, but also demonstrates comparable and in some cases improved performance over its counterpart trained on past context.',\n",
       "  'We also perform an evaluation on a targeted cataphora test suite and report significant gains over the context-agnostic Transformer in terms of BLEU.'],\n",
       " ['Although neural machine translation (NMT) has achieved significant progress in recent years, most previous NMT models only depend on the source text to generate translation.',\n",
       "  'Inspired by the success of template-based and syntax-based approaches in other fields, we propose to use extracted templates from tree structures as soft target templates to guide the translation procedure.',\n",
       "  'In order to learn the syntactic structure of the target sentences, we adopt constituency-based parse tree to generate candidate templates.',\n",
       "  'We incorporate the template information into the encoder-decoder framework to jointly utilize the templates and source text.',\n",
       "  'Experiments show that our model significantly outperforms the baseline models on four benchmarks and demonstrates the effectiveness of soft target templates.'],\n",
       " ['In this paper, we show that neural machine translation (NMT) systems trained on large back-translated data overfit some of the characteristics of machine-translated texts.',\n",
       "  'Such NMT systems better translate human-produced translations, i.e., translationese, but may largely worsen the translation quality of original texts.',\n",
       "  'Our analysis reveals that adding a simple tag to back-translations prevents this quality degradation and improves on average the overall translation quality by helping the NMT system to distinguish back-translated data from original parallel data during training.',\n",
       "  'We also show that, in contrast to high-resource configurations, NMT systems trained in low-resource settings are much less vulnerable to overfit back-translations.',\n",
       "  'We conclude that the back-translations in the training data should always be tagged especially when the origin of the text to be translated is unknown.'],\n",
       " ['Speech translation (ST) aims to learn transformations from speech in the source language to the text in the target language.',\n",
       "  'Previous works show that multitask learning improves the ST performance, in which the recognition decoder generates the text of the source language, and the translation decoder obtains the final translations based on the output of the recognition decoder.',\n",
       "  'Because whether the output of the recognition decoder has the correct semantics is more critical than its accuracy, we propose to improve the multitask ST model by utilizing word embedding as the intermediate.'],\n",
       " ['Measuring the scholarly impact of a document without citations is an important and challenging problem.',\n",
       "  'Existing approaches such as Document Influence Model (DIM) are based on dynamic topic models, which only consider the word frequency change.',\n",
       "  'In this paper, we use both frequency changes and word semantic shifts to measure document influence by developing a neural network framework.',\n",
       "  'Our model has three steps.',\n",
       "  'Firstly, we train the word embeddings for different time periods.',\n",
       "  'Subsequently, we propose an unsupervised method to align vectors for different time periods.',\n",
       "  'Finally, we compute the influence value of documents.',\n",
       "  'Our experimental results show that our model outperforms DIM.'],\n",
       " ['Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation.',\n",
       "  'Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity.',\n",
       "  'To address these problems, we propose a novel retrieval-based method for paraphrase generation.',\n",
       "  'Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index.',\n",
       "  'With its novel editor module, the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences.',\n",
       "  'In order to have fine-grained control over the editing process, our model uses the newly introduced concept of Micro Edit Vectors.',\n",
       "  'It both extracts and exploits these vectors using the attention mechanism in the Transformer architecture.',\n",
       "  'Experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics, and human evaluation of relevance, grammaticality, and diversity of generated paraphrases.'],\n",
       " ['We study the problem of multilingual masked language modeling, i.e.',\n",
       "  'the training of a single model on concatenated text from multiple languages, and present a detailed study of several factors that influence why these models are so effective for cross-lingual transfer.',\n",
       "  'We show, contrary to what was previously hypothesized, that transfer is possible even when there is no shared vocabulary across the monolingual corpora and also when the text comes from very different domains.',\n",
       "  'The only requirement is that there are some shared parameters in the top layers of the multi-lingual encoder.',\n",
       "  'To better understand this result, we also show that representations from monolingual BERT models in different languages can be aligned post-hoc quite effectively, strongly suggesting that, much like for non-contextual word embeddings, there are universal latent symmetries in the learned embedding spaces.',\n",
       "  'For multilingual masked language modeling, these symmetries are automatically discovered and aligned during the joint training process.'],\n",
       " ['Pre-trained language models like BERT have proven to be highly performant.',\n",
       "  'However, they are often computationally expensive in many practical scenarios, for such heavy models can hardly be readily implemented with limited resources.',\n",
       "  'To improve their efficiency with an assured model performance, we propose a novel speed-tunable FastBERT with adaptive inference time.',\n",
       "  'The speed at inference can be flexibly adjusted under varying demands, while redundant calculation of samples is avoided.',\n",
       "  'Moreover, this model adopts a unique self-distillation mechanism at fine-tuning, further enabling a greater computational efficacy with minimal loss in performance.',\n",
       "  'Our model achieves promising results in twelve English and Chinese datasets.',\n",
       "  'It is able to speed up by a wide range from 1 to 12 times than BERT if given different speedup thresholds to make a speed-performance tradeoff.'],\n",
       " ['Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents.',\n",
       "  'Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation.',\n",
       "  'Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa.',\n",
       "  'The code and resources are available at https://github.com/neulab/external-knowledge-codegen.'],\n",
       " ['Verifying the correctness of a textual statement requires not only semantic reasoning about the meaning of words, but also symbolic reasoning about logical operations like count, superlative, aggregation, etc.',\n",
       "  'In this work, we propose LogicalFactChecker, a neural network approach capable of leveraging logical operations for fact checking.',\n",
       "  'It achieves the state-of-the-art performance on TABFACT, a large-scale, benchmark dataset built for verifying a textual statement with semi-structured tables.',\n",
       "  'This is achieved by a graph module network built upon the Transformer-based architecture.',\n",
       "  'With a textual statement and a table as the input, LogicalFactChecker automatically derives a program (a.k.a.',\n",
       "  'logical form) of the statement in a semantic parsing manner.',\n",
       "  'A heterogeneous graph is then constructed to capture not only the structures of the table and the program, but also the connections between inputs with different modalities.',\n",
       "  'Such a graph reveals the related contexts of each word in the statement, the table and the program.',\n",
       "  'The graph is used to obtain graph-enhanced contextual representations of words in Transformer-based architecture.',\n",
       "  'After that, a program-driven module network is further introduced to exploit the hierarchical structure of the program, where semantic compositionality is dynamically modeled along the program structure with a set of function-specific modules.',\n",
       "  'Ablation experiments suggest that both the heterogeneous graph and the module network are important to obtain strong results.'],\n",
       " ['Adversarial attacks are carried out to reveal the vulnerability of deep neural networks.',\n",
       "  'Textual adversarial attacking is challenging because text is discrete and a small perturbation can bring significant change to the original input.',\n",
       "  'Word-level attacking, which can be regarded as a combinatorial optimization problem, is a well-studied class of textual attack methods.',\n",
       "  'However, existing word-level attack models are far from perfect, largely because unsuitable search space reduction methods and inefficient optimization algorithms are employed.',\n",
       "  'In this paper, we propose a novel attack model, which incorporates the sememe-based word substitution method and particle swarm optimization-based search algorithm to solve the two problems separately.',\n",
       "  'We conduct exhaustive experiments to evaluate our attack model by attacking BiLSTM and BERT on three benchmark datasets.',\n",
       "  'Experimental results demonstrate that our model consistently achieves much higher attack success rates and crafts more high-quality adversarial examples as compared to baseline methods.',\n",
       "  'Also, further experiments show our model has higher transferability and can bring more robustness enhancement to victim models by adversarial training.',\n",
       "  'All the code and data of this paper can be obtained on https://github.com/thunlp/SememePSO-Attack.'],\n",
       " ['Existing datasets for regular expression (regex) generation from natural language are limited in complexity; compared to regex tasks that users post on StackOverflow, the regexes in these datasets are simple, and the language used to describe them is not diverse.',\n",
       "  'We introduce StructuredRegex, a new regex synthesis dataset differing from prior ones in three aspects.',\n",
       "  'First, to obtain structurally complex and realistic regexes, we generate the regexes using a probabilistic grammar with pre-defined macros observed from real-world StackOverflow posts.',\n",
       "  'Second, to obtain linguistically diverse natural language descriptions, we show crowdworkers abstract depictions of the underlying regex and ask them to describe the pattern they see, rather than having them paraphrase synthetic language.',\n",
       "  'Third, we augment each regex example with a collection of strings that are and are not matched by the ground truth regex, similar to how real users give examples.',\n",
       "  'Our quantitative and qualitative analysis demonstrates the advantages of StructuredRegex over prior datasets.',\n",
       "  'Further experimental results using various multimodal synthesis techniques highlight the challenge presented by our dataset, including non-local constraints and multi-modal inputs.'],\n",
       " ['With the great success of pre-trained language models, the pretrain-finetune paradigm now becomes the undoubtedly dominant solution for natural language understanding (NLU) tasks.',\n",
       "  'At the fine-tune stage, target task data is usually introduced in a completely random order and treated equally.',\n",
       "  'However, examples in NLU tasks can vary greatly in difficulty, and similar to human learning procedure, language models can benefit from an easy-to-difficult curriculum.',\n",
       "  'Based on this idea, we propose our Curriculum Learning approach.',\n",
       "  'By reviewing the trainset in a crossed way, we are able to distinguish easy examples from difficult ones, and arrange a curriculum for language models.',\n",
       "  'Without any manual model architecture design or use of external data, our Curriculum Learning approach obtains significant and universal performance improvements on a wide range of NLU tasks.'],\n",
       " ['Despite the success of language models using neural networks, it remains unclear to what extent neural models have the generalization ability to perform inferences.',\n",
       "  'In this paper, we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition.',\n",
       "  'We consider four aspects of monotonicity inferences and test whether the models can systematically interpret lexical and logical phenomena on different training/test splits.',\n",
       "  'A series of experiments show that three neural models systematically draw inferences on unseen combinations of lexical and logical phenomena when the syntactic structures of the sentences are similar between the training and test sets.',\n",
       "  'However, the performance of the models significantly decreases when the structures are slightly changed in the test set while retaining all vocabularies and constituents already appearing in the training set.',\n",
       "  'This indicates that the generalization ability of neural models is limited to cases where the syntactic structures are nearly the same as those in the training set.'],\n",
       " ['Generating inferential texts about an event in different perspectives requires reasoning over different contexts that the event occurs.',\n",
       "  'Existing works usually ignore the context that is not explicitly provided, resulting in a context-independent semantic representation that struggles to support the generation.',\n",
       "  'To address this, we propose an approach that automatically finds evidence for an event from a large text corpus, and leverages the evidence to guide the generation of inferential texts.',\n",
       "  'Our approach works in an encoderdecoder manner and is equipped with Vector Quantised-Variational Autoencoder, where the encoder outputs representations from a distribution over discrete variables.',\n",
       "  'Such discrete representations enable automatically selecting relevant evidence, which not only facilitates evidence-aware generation, but also provides a natural way to uncover rationales behind the generation.',\n",
       "  'Our approach provides state-of-the-art performance on both Event2mind and Atomic datasets.',\n",
       "  'More importantly, we find that with discrete representations, our model selectively uses evidence to generate different inferential texts.'],\n",
       " ['Given a sentence and its relevant answer, how to ask good questions is a challenging task, which has many real applications.',\n",
       "  'Inspired by human’s paraphrasing capability to ask questions of the same meaning but with diverse expressions, we propose to incorporate paraphrase knowledge into question generation(QG) to generate human-like questions.',\n",
       "  'Specifically, we present a two-hand hybrid model leveraging a self-built paraphrase resource, which is automatically conducted by a simple back-translation method.',\n",
       "  'On the one hand, we conduct multi-task learning with sentence-level paraphrase generation (PG) as an auxiliary task to supplement paraphrase knowledge to the task-share encoder.',\n",
       "  'On the other hand, we adopt a new loss function for diversity training to introduce more question patterns to QG.',\n",
       "  'Extensive experimental results show that our proposed model obtains obvious performance gain over several strong baselines, and further human evaluation validates that our model can ask questions of high quality by leveraging paraphrase knowledge.'],\n",
       " ['Knowledge inference on knowledge graph has attracted extensive attention, which aims to find out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications.',\n",
       "  'However, researchers have mainly poured attention to knowledge inference on binary facts.',\n",
       "  'The studies on n-ary facts are relatively scarcer, although they are also ubiquitous in the real world.',\n",
       "  'Therefore, this paper addresses knowledge inference on n-ary facts.',\n",
       "  'We represent each n-ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute-value pair(s).',\n",
       "  'We further propose a neural network model, NeuInfer, for knowledge inference on n-ary facts.',\n",
       "  'Besides handling the common task to infer an unknown element in a whole fact, NeuInfer can cope with a new type of task, flexible knowledge inference.',\n",
       "  'It aims to infer an unknown element in a partial fact consisting of the primary triple coupled with any number of its auxiliary description(s).',\n",
       "  'Experimental results demonstrate the remarkable superiority of NeuInfer.'],\n",
       " ['Chinese short text matching usually employs word sequences rather than character sequences to get better performance.',\n",
       "  'However, Chinese word segmentation can be erroneous, ambiguous or inconsistent, which consequently hurts the final matching performance.',\n",
       "  'To address this problem, we propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information.',\n",
       "  'Instead of a character sequence or a single word sequence, paired word lattices formed from multiple word segmentation hypotheses are used as input and the model learns a graph representation according to an attentive graph matching mechanism.',\n",
       "  'Experiments on two Chinese datasets show that our models outperform the state-of-the-art short text matching models.'],\n",
       " ['Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics.',\n",
       "  'However, the existing parameter inference method like Monte Carlo sampling is quite time-consuming.',\n",
       "  'In this paper, we propose two efficient neural mixed counting models, i.e., the Negative Binomial-Neural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery.',\n",
       "  'Neural variational inference algorithms are developed to infer model parameters by using the reparameterization of Gamma distribution and the Gaussian approximation of Poisson distribution.',\n",
       "  'Experiments on real-world datasets indicate that our models outperform state-of-the-art baseline models in terms of perplexity and topic coherence.',\n",
       "  'The results also validate that both NB-NTM and GNB-NTM can produce explainable intermediate variables by generating dispersed proportions of document topics.'],\n",
       " ['Fact checking is a challenging task because verifying the truthfulness of a claim requires reasoning about multiple retrievable evidence.',\n",
       "  'In this work, we present a method suitable for reasoning about the semantic-level structure of evidence.',\n",
       "  'Unlike most previous works, which typically represent evidence sentences with either string concatenation or fusing the features of isolated evidence sentences, our approach operates on rich semantic structures of evidence obtained by semantic role labeling.',\n",
       "  'We propose two mechanisms to exploit the structure of evidence while leveraging the advances of pre-trained models like BERT, GPT or XLNet.',\n",
       "  'Specifically, using XLNet as the backbone, we first utilize the graph structure to re-define the relative distances of words, with the intuition that semantically related words should have short distances.',\n",
       "  'Then, we adopt graph convolutional network and graph attention network to propagate and aggregate information from neighboring nodes on the graph.',\n",
       "  'We evaluate our system on FEVER, a benchmark dataset for fact checking, and find that rich structural information is helpful and both our graph-based mechanisms improve the accuracy.',\n",
       "  'Our model is the state-of-the-art system in terms of both official evaluation metrics, namely claim verification accuracy and FEVER score.'],\n",
       " ['In this paper, we study the challenging problem of automatic generation of citation texts in scholarly papers.',\n",
       "  'Given the context of a citing paper A and a cited paper B, the task aims to generate a short text to describe B in the given context of A.',\n",
       "  'One big challenge for addressing this task is the lack of training data.',\n",
       "  'Usually, explicit citation texts are easy to extract, but it is not easy to extract implicit citation texts from scholarly papers.',\n",
       "  'We thus first train an implicit citation extraction model based on BERT and leverage the model to construct a large training dataset for the citation text generation task.',\n",
       "  'Then we propose and train a multi-source pointer-generator network with cross attention mechanism for citation text generation.',\n",
       "  'Empirical evaluation results on a manually labeled test dataset verify the efficacy of our model.',\n",
       "  'This pilot study confirms the feasibility of automatically generating citation texts in scholarly papers and the technique has the great potential to help researchers prepare their scientific papers.'],\n",
       " ['In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization.',\n",
       "  'To well handle the problem of composing EDUs into an informative and fluent summary, we propose a novel summarization method that first designs an EDU selection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence.',\n",
       "  'We also design the reinforcement learning mechanism to use EDU fusion results to reward the EDU selection action, boosting the final summarization performance.',\n",
       "  'Experiments on CNN/Daily Mail have demonstrated the effectiveness of our model.'],\n",
       " ['This paper creates a paradigm shift with regard to the way we build neural extractive summarization systems.',\n",
       "  'Instead of following the commonly used framework of extracting sentences individually and modeling the relationship between sentences, we formulate the extractive summarization task as a semantic text matching problem, in which a source document and candidate summaries will be (extracted from the original text) matched in a semantic space.',\n",
       "  'Notably, this paradigm shift to semantic matching framework is well-grounded in our comprehensive analysis of the inherent gap between sentence-level and summary-level extractors based on the property of the dataset.',\n",
       "  'Besides, even instantiating the framework with a simple form of a matching model, we have driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1).',\n",
       "  'Experiments on the other five datasets also show the effectiveness of the matching framework.',\n",
       "  'We believe the power of this matching-based summarization framework has not been fully exploited.',\n",
       "  'To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github.com/maszhongming/MatchSum.'],\n",
       " ['As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches.',\n",
       "  'An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships.',\n",
       "  'In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences.',\n",
       "  'These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations.',\n",
       "  'Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes.',\n",
       "  'To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits.',\n",
       "  'The code will be released on Github.'],\n",
       " ['Cross-lingual summarization is the task of generating a summary in one language given a text in a different language.',\n",
       "  'Previous works on cross-lingual summarization mainly focus on using pipeline methods or training an end-to-end model using the translated parallel data.',\n",
       "  'However, it is a big challenge for the model to directly learn cross-lingual summarization as it requires learning to understand different languages and learning how to summarize at the same time.',\n",
       "  'In this paper, we propose to ease the cross-lingual summarization training by jointly learning to align and summarize.',\n",
       "  'We design relevant loss functions to train this framework and propose several methods to enhance the isomorphism and cross-lingual transfer between languages.',\n",
       "  'Experimental results show that our model can outperform competitive models in most cases.',\n",
       "  'In addition, we show that our model even has the ability to generate cross-lingual summaries without access to any cross-lingual corpus.'],\n",
       " ['Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries.',\n",
       "  'In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries.',\n",
       "  'Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents.',\n",
       "  'Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries.',\n",
       "  'Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly.',\n",
       "  'Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines.'],\n",
       " ['In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents.',\n",
       "  'The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary.',\n",
       "  'We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization.',\n",
       "  'Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset.'],\n",
       " ['We present a constituency parsing algorithm that, like a supertagger, works by assigning labels to each word in a sentence.',\n",
       "  'In order to maximally leverage current neural architectures, the model scores each word’s tags in parallel, with minimal task-specific structure.',\n",
       "  'After scoring, a left-to-right reconciliation phase extracts a tree in (empirically) linear time.',\n",
       "  'Our parser achieves 95.4 F1 on the WSJ test set while also achieving substantial speedups compared to current state-of-the-art parsers with comparable accuracies.'],\n",
       " ['Recent advances in pre-trained multilingual language models lead to state-of-the-art results on the task of quality estimation (QE) for machine translation.',\n",
       "  'A carefully engineered ensemble of such models won the QE shared task at WMT19.',\n",
       "  'Our in-depth analysis, however, shows that the success of using pre-trained language models for QE is over-estimated due to three issues we observed in current QE datasets: (i) The distributions of quality scores are imbalanced and skewed towards good quality scores; (iii) QE models can perform well on these datasets while looking at only source or translated sentences; (iii) They contain statistical artifacts that correlate well with human-annotated QE labels.',\n",
       "  'Our findings suggest that although QE models might capture fluency of translated sentences and complexity of source sentences, they cannot model adequacy of translations effectively.'],\n",
       " ['While natural language understanding (NLU) is advancing rapidly, today’s technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization.',\n",
       "  'This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL).',\n",
       "  'According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction.',\n",
       "  'This position paper argues that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems, and proposes a system architecture along with a roadmap towards realizing this vision.'],\n",
       " ['Language technologies contribute to promoting multilingualism and linguistic diversity around the world.',\n",
       "  'However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications.',\n",
       "  'In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time.',\n",
       "  'Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the “language agnostic” status of current models and systems.',\n",
       "  'Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.'],\n",
       " ['In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures.',\n",
       "  'We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model.',\n",
       "  'This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding.'],\n",
       " ['Corpus query systems exist to address the multifarious information needs of any person interested in the content of annotated corpora.',\n",
       "  'In this role they play an important part in making those resources usable for a wider audience.',\n",
       "  'Over the past decades, several such query systems and languages have emerged, varying greatly in their expressiveness and technical details.',\n",
       "  'This paper offers a broad overview of the history of corpora and corpus query tools.',\n",
       "  'It focusses strongly on the query side and hints at exciting directions for future development.'],\n",
       " ['Recent studies in dialogue state tracking (DST) leverage historical information to determine states which are generally represented as slot-value pairs.',\n",
       "  'However, most of them have limitations to efficiently exploit relevant context due to the lack of a powerful mechanism for modeling interactions between the slot and the dialogue history.',\n",
       "  'Besides, existing methods usually ignore the slot imbalance problem and treat all slots indiscriminately, which limits the learning of hard slots and eventually hurts overall performance.',\n",
       "  'In this paper, we propose to enhance the DST through employing a contextual hierarchical attention network to not only discern relevant information at both word level and turn level but also learn contextual representations.',\n",
       "  'We further propose an adaptive objective to alleviate the slot imbalance problem by dynamically adjust weights of different slots during training.',\n",
       "  'Experimental results show that our approach reaches 52.68% and 58.55% joint accuracy on MultiWOZ 2.0 and MultiWOZ 2.1 datasets respectively and achieves new state-of-the-art performance with considerable improvements (+1.24% and +5.98%).'],\n",
       " ['Current state-of-the-art neural dialogue models learn from human conversations following the data-driven paradigm.',\n",
       "  'As such, a reliable training corpus is the crux of building a robust and well-behaved dialogue model.',\n",
       "  'However, due to the open-ended nature of human conversations, the quality of user-generated training data varies greatly, and effective training samples are typically insufficient while noisy samples frequently appear.',\n",
       "  'This impedes the learning of those data-driven neural dialogue models.',\n",
       "  'Therefore, effective dialogue learning requires not only more reliable learning samples, but also fewer noisy samples.',\n",
       "  'In this paper, we propose a data manipulation framework to proactively reshape the data distribution towards reliable samples by augmenting and highlighting effective learning samples as well as reducing the effect of inefficient samples simultaneously.',\n",
       "  'In particular, the data manipulation model selectively augments the training samples and assigns an importance weight to each instance to reform the training data.',\n",
       "  'Note that, the proposed data manipulation framework is fully data-driven and learnable.',\n",
       "  'It not only manipulates training samples to optimize the dialogue generation model, but also learns to increase its manipulation skills through gradient descent with validation samples.',\n",
       "  'Extensive experiments show that our framework can improve the dialogue generation performance with respect to various automatic evaluation metrics and human judgments.'],\n",
       " ['Recent studies have shown remarkable success in end-to-end task-oriented dialog system.',\n",
       "  'However, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling.',\n",
       "  'This makes it difficult to scalable for a new domain with limited labeled data.',\n",
       "  'However, there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains.',\n",
       "  'To this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge.',\n",
       "  'In addition, we propose a novel Dynamic Fusion Network (DF-Net) which automatically exploit the relevance between the target domain and each domain.',\n",
       "  'Results show that our models outperforms existing methods on multi-domain dialogue, giving the state-of-the-art in the literature.',\n",
       "  'Besides, with little training data, we show its transferability by outperforming prior best model by 13.9% on average.'],\n",
       " ['Training a task-oriented dialogue agent with reinforcement learning is prohibitively expensive since it requires a large volume of interactions with users.',\n",
       "  'Human demonstrations can be used to accelerate learning progress.',\n",
       "  'However, how to effectively leverage demonstrations to learn dialogue policy remains less explored.',\n",
       "  'In this paper, we present Sˆ2Agent that efficiently learns dialogue policy from demonstrations through policy shaping and reward shaping.',\n",
       "  'We use an imitation model to distill knowledge from demonstrations, based on which policy shaping estimates feedback on how the agent should act in policy space.',\n",
       "  'Reward shaping is then incorporated to bonus state-actions similar to demonstrations explicitly in value space encouraging better exploration.',\n",
       "  'The effectiveness of the proposed Sˆ2Agentt is demonstrated in three dialogue domains and a challenging domain adaptation task with both user simulator evaluation and human evaluation.'],\n",
       " ['Dialogue state tracker is responsible for inferring user intentions through dialogue history.',\n",
       "  'Previous methods have difficulties in handling dialogues with long interaction context, due to the excessive information.',\n",
       "  'We propose a Dialogue State Tracker with Slot Attention and Slot Information Sharing (SAS) to reduce redundant information’s interference and improve long dialogue context tracking.',\n",
       "  'Specially, we first apply a Slot Attention to learn a set of slot-specific features from the original dialogue and then integrate them using a slot information sharing module.',\n",
       "  'Our model yields a significantly improved performance compared to previous state-of the-art models on the MultiWOZ dataset.'],\n",
       " ['Automatic evaluation of open-domain dialogue response generation is very challenging because there are many appropriate responses for a given context.',\n",
       "  'Existing evaluation models merely compare the generated response with the ground truth response and rate many of the appropriate responses as inappropriate if they deviate from the ground truth.',\n",
       "  'One approach to resolve this problem is to consider the similarity of the generated response with the conversational context.',\n",
       "  'In this paper, we propose an automatic evaluation model based on that idea and learn the model parameters from an unlabeled conversation corpus.',\n",
       "  'Our approach considers the speakers in defining the different levels of similar context.',\n",
       "  'We use a Twitter conversation corpus that contains many speakers and conversations to test our evaluation model.',\n",
       "  'Experiments show that our model outperforms the other existing evaluation metrics in terms of high correlation with human annotation scores.',\n",
       "  'We also show that our model trained on Twitter can be applied to movie dialogues without any additional training.',\n",
       "  'We provide our code and the learned parameters so that they can be used for automatic evaluation of dialogue response generation models.'],\n",
       " ['Due to its great importance in deep natural language understanding and various down-stream applications, text-level parsing of discourse rhetorical structure (DRS) has been drawing more and more attention in recent years.',\n",
       "  'However, all the previous studies on text-level discourse parsing adopt bottom-up approaches, which much limit the DRS determination on local information and fail to well benefit from global information of the overall discourse.',\n",
       "  'In this paper, we justify from both computational and perceptive points-of-view that the top-down architecture is more suitable for text-level DRS parsing.',\n",
       "  'On the basis, we propose a top-down neural architecture toward text-level DRS parsing.',\n",
       "  'In particular, we cast discourse parsing as a recursive split point ranking task, where a split point is classified to different levels according to its rank and the elementary discourse units (EDUs) associated with it are arranged accordingly.',\n",
       "  'In this way, we can determine the complete DRS as a hierarchical tree structure via an encoder-decoder with an internal stack.',\n",
       "  'Experimentation on both the English RST-DT corpus and the Chinese CDTB corpus shows the great effectiveness of our proposed top-down approach towards text-level DRS parsing.'],\n",
       " ['An in-depth exploration of protein-protein interactions (PPI) is essential to understand the metabolism in addition to the regulations of biological entities like proteins, carbohydrates, and many more.',\n",
       "  'Most of the recent PPI tasks in BioNLP domain have been carried out solely using textual data.',\n",
       "  'In this paper, we argue that incorporating multimodal cues can improve the automatic identification of PPI.',\n",
       "  'As a first step towards enabling the development of multimodal approaches for PPI identification, we have developed two multi-modal datasets which are extensions and multi-modal versions of two popular benchmark PPI corpora (BioInfer and HRPD50).',\n",
       "  'Besides, existing textual modalities, two new modalities, 3D protein structure and underlying genomic sequence, are also added to each instance.',\n",
       "  'Further, a novel deep multi-modal architecture is also implemented to efficiently predict the protein interactions from the developed datasets.',\n",
       "  'A detailed experimental analysis reveals the superiority of the multi-modal approach in comparison to the strong baselines including unimodal approaches and state-of the-art methods over both the generated multi-modal datasets.',\n",
       "  'The developed multi-modal datasets are available for use at https://github.com/sduttap16/MM_PPI_NLP.'],\n",
       " ['In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for nested named entity recognition (NER), which contains two subgraph modules: a flat NER module for outermost entities and a graph module for all the entities located in inner layers.',\n",
       "  'Bidirectional LSTM (BiLSTM) and graph convolutional network (GCN) are adopted to jointly learn flat entities and their inner dependencies.',\n",
       "  'Different from previous models, which only consider the unidirectional delivery of information from innermost layers to outer ones (or outside-to-inside), our model effectively captures the bidirectional interaction between them.',\n",
       "  'We first use the entities recognized by the flat NER module to construct an entity graph, which is fed to the next graph module.',\n",
       "  'The richer representation learned from graph module carries the dependencies of inner entities and can be exploited to improve outermost entity predictions.',\n",
       "  'Experimental results on three standard nested NER datasets demonstrate that our BiFlaG outperforms previous state-of-the-art models.'],\n",
       " ['Knowledge graph (KG) entity typing aims at inferring possible missing entity type instances in KG, which is a very significant but still under-explored subtask of knowledge graph completion.',\n",
       "  'In this paper, we propose a novel approach for KG entity typing which is trained by jointly utilizing local typing knowledge from existing entity type assertions and global triple knowledge in KGs.',\n",
       "  'Specifically, we present two distinct knowledge-driven effective mechanisms of entity type inference.',\n",
       "  'Accordingly, we build two novel embedding models to realize the mechanisms.',\n",
       "  'Afterward, a joint model via connecting them is used to infer missing entity type instances, which favors inferences that agree with both entity type instances and triple knowledge in KGs.',\n",
       "  'Experimental results on two real-world datasets (Freebase and YAGO) demonstrate the effectiveness of our proposed mechanisms and models for improving KG entity typing.',\n",
       "  'The source code and data of this paper can be obtained from: https://github.com/Adam1679/ConnectE .'],\n",
       " ['Continual relation learning aims to continually train a model on new data to learn incessantly emerging novel relations while avoiding catastrophically forgetting old relations.',\n",
       "  'Some pioneering work has proved that storing a handful of historical relation examples in episodic memory and replaying them in subsequent training is an effective solution for such a challenging problem.',\n",
       "  'However, these memory-based methods usually suffer from overfitting the few memorized examples of old relations, which may gradually cause inevitable confusion among existing relations.',\n",
       "  'Inspired by the mechanism in human long-term memory formation, we introduce episodic memory activation and reconsolidation (EMAR) to continual relation learning.',\n",
       "  'Every time neural models are activated to learn both new and memorized data, EMAR utilizes relation prototypes for memory reconsolidation exercise to keep a stable understanding of old relations.',\n",
       "  'The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models.'],\n",
       " ['One great challenge in neural sequence labeling is the data sparsity problem for rare entity words and phrases.',\n",
       "  'Most of test set entities appear only few times and are even unseen in training corpus, yielding large number of out-of-vocabulary (OOV) and low-frequency (LF) entities during evaluation.',\n",
       "  'In this work, we propose approaches to address this problem.',\n",
       "  'For OOV entities, we introduce local context reconstruction to implicitly incorporate contextual information into their representations.',\n",
       "  'For LF entities, we present delexicalized entity identification to explicitly extract their frequency-agnostic and entity-type-specific representations.',\n",
       "  'Extensive experiments on multiple benchmark datasets show that our model has significantly outperformed all previous methods and achieved new start-of-the-art results.',\n",
       "  'Notably, our methods surpass the model fine-tuned on pre-trained language models without external resource.'],\n",
       " ['Interpretable rationales for model predictions play a critical role in practical applications.',\n",
       "  'In this study, we develop models possessing interpretable inference process for structured prediction.',\n",
       "  'Specifically, we present a method of instance-based learning that learns similarities between spans.',\n",
       "  'At inference time, each span is assigned a class label based on its similar spans in the training set, where it is easy to understand how much each training instance contributes to the predictions.',\n",
       "  'Through empirical analysis on named entity recognition, we demonstrate that our method enables to build models that have high interpretability without sacrificing performance.'],\n",
       " ['Electronic Medical Records (EMRs) have become key components of modern medical care systems.',\n",
       "  'Despite the merits of EMRs, many doctors suffer from writing them, which is time-consuming and tedious.',\n",
       "  'We believe that automatically converting medical dialogues to EMRs can greatly reduce the burdens of doctors, and extracting information from medical dialogues is an essential step.',\n",
       "  'To this end, we annotate online medical consultation dialogues in a window-sliding style, which is much easier than the sequential labeling annotation.',\n",
       "  'We then propose a Medical Information Extractor (MIE) towards medical dialogues.',\n",
       "  'MIE is able to extract mentioned symptoms, surgeries, tests, other information and their corresponding status.',\n",
       "  'To tackle the particular challenges of the task, MIE uses a deep matching architecture, taking dialogue turn-interaction into account.',\n",
       "  'The experimental results demonstrate MIE is a promising solution to extract medical information from doctor-patient dialogues.'],\n",
       " ['Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities.',\n",
       "  'NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009).',\n",
       "  'In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017).',\n",
       "  'The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately.',\n",
       "  'We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points.'],\n",
       " ['Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment.',\n",
       "  'This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling the structural heterogeneity challenge.',\n",
       "  'NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference.',\n",
       "  'It provides two innovative components for better learning representations for entity alignment.',\n",
       "  'It first uses a novel graph sampling method to distill a discriminative neighborhood for each entity.',\n",
       "  'It then adopts a cross-graph neighborhood matching module to jointly encode the neighborhood difference for a given entity pair.',\n",
       "  'Such strategies allow NMN to effectively construct matching-oriented entity representations while ignoring noisy neighbors that have a negative impact on the alignment task.',\n",
       "  'Extensive experiments performed on three entity alignment datasets show that NMN can well estimate the neighborhood similarity in more tough cases and significantly outperforms 12 previous state-of-the-art methods.'],\n",
       " ['Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences.',\n",
       "  'Efforts thus far have focused on improving extraction accuracy but little is known about their explanability.',\n",
       "  'In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models.',\n",
       "  'We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation.',\n",
       "  'We also propose to automatically generate “distractor” sentences to augment the bags and train the model to ignore the distractors.',\n",
       "  'Evaluations on the widely used FB-NYT dataset show that our methods achieve new state-of-the-art accuracy while improving model explanability.'],\n",
       " ['We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images.',\n",
       "  'We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document.',\n",
       "  'These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases.'],\n",
       " ['To better tackle the named entity recognition (NER) problem on languages with little/no labeled data, cross-lingual NER must effectively leverage knowledge learned from source languages with rich labeled data.',\n",
       "  'Previous works on cross-lingual NER are mostly based on label projection with pairwise texts or direct model transfer.',\n",
       "  'However, such methods either are not applicable if the labeled data in the source languages is unavailable, or do not leverage information contained in unlabeled data in the target language.',\n",
       "  'In this paper, we propose a teacher-student learning method to address such limitations, where NER models in the source languages are used as teachers to train a student model on unlabeled data in the target language.',\n",
       "  'The proposed method works for both single-source and multi-source cross-lingual NER.',\n",
       "  'For the latter, we further propose a similarity measuring method to better weight the supervision from different teacher models.',\n",
       "  'Extensive experiments for 3 target languages on benchmark datasets well demonstrate that our method outperforms existing state-of-the-art methods for both single-source and multi-source cross-lingual NER.'],\n",
       " ['Opinion entity extraction is a fundamental task in fine-grained opinion mining.',\n",
       "  'Related studies generally extract aspects and/or opinion expressions without recognizing the relations between them.',\n",
       "  'However, the relations are crucial for downstream tasks, including sentiment classification, opinion summarization, etc.',\n",
       "  'In this paper, we explore Aspect-Opinion Pair Extraction (AOPE) task, which aims at extracting aspects and opinion expressions in pairs.',\n",
       "  'To deal with this task, we propose Synchronous Double-channel Recurrent Network (SDRN) mainly consisting of an opinion entity extraction unit, a relation detection unit, and a synchronization unit.',\n",
       "  'The opinion entity extraction unit and the relation detection unit are developed as two channels to extract opinion entities and relations simultaneously.',\n",
       "  'Furthermore, within the synchronization unit, we design Entity Synchronization Mechanism (ESM) and Relation Synchronization Mechanism (RSM) to enhance the mutual benefit on the above two channels.',\n",
       "  'To verify the performance of SDRN, we manually build three datasets based on SemEval 2014 and 2015 benchmarks.',\n",
       "  'Extensive experiments demonstrate that SDRN achieves state-of-the-art performances.'],\n",
       " ['We use coherence relations inspired by computational models of discourse to study the information needs and goals of image captioning.',\n",
       "  'Using an annotation protocol specifically devised for capturing image–caption coherence relations, we annotate 10,000 instances from publicly-available image–caption pairs.',\n",
       "  'We introduce a new task for learning inferences in imagery and text, coherence relation prediction, and show that these coherence annotations can be exploited to learn relation classifiers as an intermediary step, and also train coherence-aware, controllable image captioning models.',\n",
       "  'The results show a dramatic improvement in the consistency and quality of the generated captions with respect to information needs specified via coherence relations.'],\n",
       " ['In human cognition, world knowledge supports the perception of object colours: knowing that trees are typically green helps to perceive their colour in certain contexts.',\n",
       "  'We go beyond previous studies on colour terms using isolated colour swatches and study visual grounding of colour terms in realistic objects.',\n",
       "  'Our models integrate processing of visual information and object-specific knowledge via hard-coded (late) or learned (early) fusion.',\n",
       "  'We find that both models consistently outperform a bottom-up baseline that predicts colour terms solely from visual inputs, but show interesting differences when predicting atypical colours of so-called colour diagnostic objects.',\n",
       "  'Our models also achieve promising results when tested on new object categories not seen during training.'],\n",
       " ['Given an untrimmed video and a text query, natural language video localization (NLVL) is to locate a matching span from the video that semantically corresponds to the query.',\n",
       "  'Existing solutions formulate NLVL either as a ranking task and apply multimodal matching architecture, or as a regression task to directly regress the target video span.',\n",
       "  'In this work, we address NLVL task with a span-based QA approach by treating the input video as text passage.',\n",
       "  'We propose a video span localizing network (VSLNet), on top of the standard span-based QA framework, to address NLVL.',\n",
       "  'The proposed VSLNet tackles the differences between NLVL and span-based QA through a simple and yet effective query-guided highlighting (QGH) strategy.',\n",
       "  'The QGH guides VSLNet to search for matching video span within a highlighted region.',\n",
       "  'Through extensive experiments on three benchmark datasets, we show that the proposed VSLNet outperforms the state-of-the-art methods; and adopting span-based QA framework is a promising direction to solve NLVL.'],\n",
       " ['Visual referring expression recognition is a challenging task that requires natural language understanding in the context of an image.',\n",
       "  'We critically examine RefCOCOg, a standard benchmark for this task, using a human study and show that 83.7% of test instances do not require reasoning on linguistic structure, i.e., words are enough to identify the target object, the word order doesn’t matter.',\n",
       "  'To measure the true progress of existing models, we split the test set into two sets, one which requires reasoning on linguistic structure and the other which doesn’t.',\n",
       "  'Additionally, we create an out-of-distribution dataset Ref-Adv by asking crowdworkers to perturb in-domain examples such that the target object changes.',\n",
       "  'Using these datasets, we empirically show that existing methods fail to exploit linguistic structure and are 12% to 23% lower in performance than the established progress for this task.',\n",
       "  'We also propose two methods, one based on contrastive learning and the other based on multi-task learning, to increase the robustness of ViLBERT, the current state-of-the-art model for this task.',\n",
       "  'Our datasets are publicly available at https://github.com/aws/aws-refcocog-adv.'],\n",
       " ['Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks.',\n",
       "  'Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss.',\n",
       "  'In this work, we instead “reallocate” them—the model learns to activate different heads on different inputs.',\n",
       "  'Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE).',\n",
       "  'MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters.',\n",
       "  'Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks.',\n",
       "  'Particularly, on the WMT14 English to German translation dataset, MAE improves over “transformer-base” by 0.8 BLEU, with a comparable number of parameters.',\n",
       "  'Our analysis shows that our model learns to specialize different experts to different inputs.'],\n",
       " ['Aspect-based sentiment classification is a popular task aimed at identifying the corresponding emotion of a specific aspect.',\n",
       "  'One sentence may contain various sentiments for different aspects.',\n",
       "  'Many sophisticated methods such as attention mechanism and Convolutional Neural Networks (CNN) have been widely employed for handling this challenge.',\n",
       "  'Recently, semantic dependency tree implemented by Graph Convolutional Networks (GCN) is introduced to describe the inner connection between aspects and the associated emotion words.',\n",
       "  'But the improvement is limited due to the noise and instability of dependency trees.',\n",
       "  'To this end, we propose a dependency graph enhanced dual-transformer network (named DGEDT) by jointly considering the flat representations learnt from Transformer and graph-based representations learnt from the corresponding dependency graph in an iterative interaction manner.',\n",
       "  'Specifically, a dual-transformer structure is devised in DGEDT to support mutual reinforcement between the flat representation learning and graph-based representation learning.',\n",
       "  'The idea is to allow the dependency graph to guide the representation learning of the transformer encoder and vice versa.',\n",
       "  'The results on five datasets demonstrate that the proposed DGEDT outperforms all state-of-the-art alternatives with a large margin.'],\n",
       " ['We propose Differentiable Window, a new neural module and general purpose component for dynamic window selection.',\n",
       "  'While universally applicable, we demonstrate a compelling use case of utilizing Differentiable Window to improve standard attention modules by enabling more focused attentions over the input regions.',\n",
       "  'We propose two variants of Differentiable Window, and integrate them within the Transformer architecture in two novel ways.',\n",
       "  'We evaluate our proposed approach on a myriad of NLP tasks, including machine translation, sentiment analysis, subject-verb agreement and language modeling.',\n",
       "  'Our experimental results demonstrate consistent and sizable improvements across all tasks.'],\n",
       " ['Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples.',\n",
       "  'Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension.',\n",
       "  'In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings.',\n",
       "  'Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data.'],\n",
       " ['It is commonly believed that knowledge of syntactic structure should improve language modeling.',\n",
       "  'However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic.',\n",
       "  'In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called “syntactic distances”, where information between these two separate objectives shares the same intermediate representation.',\n",
       "  'Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.'],\n",
       " ['Neural architecture search (NAS) has advanced significantly in recent years but most NAS systems restrict search to learning architectures of a recurrent or convolutional cell.',\n",
       "  'In this paper, we extend the search space of NAS.',\n",
       "  'In particular, we present a general approach to learn both intra-cell and inter-cell architectures (call it ESS).',\n",
       "  'For a better search result, we design a joint learning method to perform intra-cell and inter-cell NAS simultaneously.',\n",
       "  'We implement our model in a differentiable architecture search system.',\n",
       "  'For recurrent neural language modeling, it outperforms a strong baseline significantly on the PTB and WikiText data, with a new state-of-the-art on PTB.',\n",
       "  'Moreover, the learned architectures show good transferability to other systems.',\n",
       "  'E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity recognition (NER) tasks and CoNLL chunking task, indicating a promising line of research on large-scale pre-learned architectures.'],\n",
       " ['As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs.',\n",
       "  'To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) “exit” from neural network calculations for simple instances, and late (and accurate) exit for hard instances.',\n",
       "  'To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions.',\n",
       "  'We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks.',\n",
       "  'Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy.',\n",
       "  'Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model.',\n",
       "  'Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time.',\n",
       "  'We publicly release our code.'],\n",
       " ['Polysynthetic languages have exceptionally large and sparse vocabularies, thanks to the number of morpheme slots and combinations in a word.',\n",
       "  'This complexity, together with a general scarcity of written data, poses a challenge to the development of natural language technologies.',\n",
       "  'To address this challenge, we offer linguistically-informed approaches for bootstrapping a neural morphological analyzer, and demonstrate its application to Kunwinjku, a polysynthetic Australian language.',\n",
       "  'We generate data from a finite state transducer to train an encoder-decoder model.',\n",
       "  'We improve the model by “hallucinating” missing linguistic structure into the training data, and by resampling from a Zipf distribution to simulate a more natural distribution of morphemes.',\n",
       "  'The best model accounts for all instances of reduplication in the test set and achieves an accuracy of 94.7% overall, a 10 percentage point improvement over the FST baseline.',\n",
       "  'This process demonstrates the feasibility of bootstrapping a neural morph analyzer from minimal resources.'],\n",
       " ['Fully supervised neural approaches have achieved significant progress in the task of Chinese word segmentation (CWS).',\n",
       "  'Nevertheless, the performance of supervised models always drops gravely if the domain shifts due to the distribution gap across domains and the out of vocabulary (OOV) problem.',\n",
       "  'In order to simultaneously alleviate the issues, this paper intuitively couples distant annotation and adversarial training for cross-domain CWS.',\n",
       "  '1) We rethink the essence of “Chinese words” and design an automatic distant annotation mechanism, which does not need any supervision or pre-defined dictionaries on the target domain.',\n",
       "  'The method could effectively explore domain-specific words and distantly annotate the raw texts for the target domain.',\n",
       "  '2) We further develop a sentence-level adversarial training procedure to perform noise reduction and maximum utilization of the source domain information.',\n",
       "  'Experiments on multiple real-world datasets across various domains show the superiority and robustness of our model, significantly outperforming previous state-of-the-arts cross-domain CWS methods.'],\n",
       " ['This paper describes a language-independent model for fully unsupervised morphological analysis that exploits a universal framework leveraging morphological typology.',\n",
       "  'By modeling morphological processes including suffixation, prefixation, infixation, and full and partial reduplication with constrained stem change rules, our system effectively constrains the search space and offers a wide coverage in terms of morphological typology.',\n",
       "  'The system is tested on nine typologically and genetically diverse languages, and shows superior performance over leading systems.',\n",
       "  'We also investigate the effect of an oracle that provides only a handful of bits per language to signal morphological type.'],\n",
       " ['The noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties.',\n",
       "  'Class membership is far from deterministic, but the phonological form of a noun and/or its meaning can often provide imperfect clues.',\n",
       "  'Here, we investigate the strength of those clues.',\n",
       "  'More specifically, we operationalize this by measuring how much information, in bits, we can glean about declension class from knowing the form and/or meaning of nouns.',\n",
       "  'We know that form and meaning are often also indicative of grammatical gender—which, as we quantitatively verify, can itself share information with declension class—so we also control for gender.',\n",
       "  'We find for two Indo-European languages (Czech and German) that form and meaning respectively share significant amounts of information with class (and contribute additional information above and beyond gender).',\n",
       "  'The three-way interaction between class, form, and meaning (given gender) is also significant.',\n",
       "  'Our study is important for two reasons: First, we introduce a new method that provides additional quantitative support for a classic linguistic finding that form and meaning are relevant for the classification of nouns into declensions.',\n",
       "  'Secondly, we show not only that individual declensions classes vary in the strength of their clues within a language, but also that these variations themselves vary across languages.'],\n",
       " ['We propose the task of unsupervised morphological paradigm completion.',\n",
       "  'Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas.',\n",
       "  'From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators.',\n",
       "  'From a cognitive science perspective, this can shed light on how children acquire morphological knowledge.',\n",
       "  'We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation.',\n",
       "  'We perform an evaluation on 14 typologically diverse languages.',\n",
       "  'Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.'],\n",
       " ['Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer).',\n",
       "  'Despite the effectiveness of existing methods on this benchmark, they treat these two sub-tasks individually during training while ignoring their dependencies.',\n",
       "  'To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and tokens.',\n",
       "  'We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously.',\n",
       "  'The long and short answers can be extracted from paragraph-level representation and token-level representation, respectively.',\n",
       "  'In this way, we can model the dependencies between the two-grained answers to provide evidence for each other.',\n",
       "  'We jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria.'],\n",
       " ['Question Answering (QA) has shown great success thanks to the availability of large-scale datasets and the effectiveness of neural models.',\n",
       "  'Recent research works have attempted to extend these successes to the settings with few or no labeled data available.',\n",
       "  'In this work, we introduce two approaches to improve unsupervised QA.',\n",
       "  'First, we harvest lexically and syntactically divergent questions from Wikipedia to automatically construct a corpus of question-answer pairs (named as RefQA).',\n",
       "  'Second, we take advantage of the QA model to extract more appropriate answers, which iteratively refines data over RefQA.',\n",
       "  'We conduct experiments on SQuAD 1.1, and NewsQA by fine-tuning BERT without access to manually annotated data.',\n",
       "  'Our approach outperforms previous unsupervised approaches by a large margin, and is competitive with early supervised models.',\n",
       "  'We also show the effectiveness of our approach in the few-shot learning setting.'],\n",
       " ['This paper focuses on generating multi-hop reasoning questions from the raw text in a low resource circumstance.',\n",
       "  'Such questions have to be syntactically valid and need to logically correlate with the answers by deducing over multiple relations on several sentences in the text.',\n",
       "  'Specifically, we first build a multi-hop generation model and guide it to satisfy the logical rationality by the reasoning chain extracted from a given text.',\n",
       "  'Since the labeled data is limited and insufficient for training, we propose to learn the model with the help of a large scale of unlabeled data that is much easier to obtain.',\n",
       "  'Such data contains rich expressive forms of the questions with structural patterns on syntax and semantics.',\n",
       "  'These patterns can be estimated by the neural hidden semi-Markov model using latent variables.',\n",
       "  'With latent patterns as a prior, we can regularize the generation model and produce the optimal results.',\n",
       "  'Experimental results on the HotpotQA data set demonstrate the effectiveness of our model.',\n",
       "  'Moreover, we apply the generated results to the task of machine reading comprehension and achieve significant performance improvements.'],\n",
       " ['Recent studies have revealed that reading comprehension (RC) systems learn to exploit annotation artifacts and other biases in current datasets.',\n",
       "  'This prevents the community from reliably measuring the progress of RC systems.',\n",
       "  'To address this issue, we introduce R4C, a new task for evaluating RC systems’ internal reasoning.',\n",
       "  'R4C requires giving not only answers but also derivations: explanations that justify predicted answers.',\n",
       "  'We present a reliable, crowdsourced framework for scalably annotating RC datasets with derivations.',\n",
       "  'We create and publicly release the R4C dataset, the first, quality-assured dataset consisting of 4.6k questions, each of which is annotated with 3 reference derivations (i.e.',\n",
       "  '13.8k derivations).',\n",
       "  'Experiments show that our automatic evaluation metrics using multiple reference derivations are reliable, and that R4C assesses different skills from an existing benchmark.'],\n",
       " ['In this paper, we study machine reading comprehension (MRC) on long texts: where a model takes as inputs a lengthy document and a query, extracts a text span from the document as an answer.',\n",
       "  'State-of-the-art models (e.g., BERT) tend to use a stack of transformer layers that are pre-trained from a large number of unlabeled language corpora to encode the joint contextual information of query and document.',\n",
       "  'However, these transformer models can only take as input a fixed-length (e.g., 512) text.',\n",
       "  'To deal with even longer text inputs, previous approaches usually chunk them into equally-spaced segments and predict answers based on each segment independently without considering the information from other segments.',\n",
       "  'As a result, they may form segments that fail to cover complete answers or retain insufficient contexts around the correct answer required for question answering.',\n",
       "  'Moreover, they are less capable of answering questions that need cross-segment information.',\n",
       "  'We propose to let a model learn to chunk in a more flexible way via reinforcement learning: a model can decide the next segment that it wants to process in either direction.',\n",
       "  'We also apply recurrent mechanisms to enable information to flow across segments.',\n",
       "  'Experiments on three MRC tasks – CoQA, QuAC, and TriviaQA – demonstrate the effectiveness of our proposed recurrent chunking mechanisms: we can obtain segments that are more likely to contain complete answers and at the same time provide sufficient contexts around the ground truth answers for better predictions.'],\n",
       " ['Reading long documents to answer open-domain questions remains challenging in natural language understanding.',\n",
       "  'In this paper, we introduce a new model, called RikiNet, which reads Wikipedia pages for natural question answering.',\n",
       "  'RikiNet contains a dynamic paragraph dual-attention reader and a multi-level cascaded answer predictor.',\n",
       "  'The reader dynamically represents the document and question by utilizing a set of complementary attention mechanisms.',\n",
       "  'The representations are then fed into the predictor to obtain the span of the short answer, the paragraph of the long answer, and the answer type in a cascaded manner.',\n",
       "  'On the Natural Questions (NQ) dataset, a single RikiNet achieves 74.3 F1 and 57.9 F1 on long-answer and short-answer tasks.',\n",
       "  'To our best knowledge, it is the first single model that outperforms the single human performance.',\n",
       "  'Furthermore, an ensemble RikiNet obtains 76.1 F1 and 61.3 F1 on long-answer and short-answer tasks, achieving the best performance on the official NQ leaderboard.'],\n",
       " ['We propose variable-in-situ logico-semantic graphs to bridge the gap between semantic graph and logical form parsing.',\n",
       "  'The new type of graph-based meaning representation allows us to include analysis for scope-related phenomena, such as quantification, negation and modality, in a way that is consistent with the state-of-the-art underspecification approach.',\n",
       "  'Moreover, the well-formedness of such a graph is clear, since model-theoretic interpretation is available.',\n",
       "  'We demonstrate the effectiveness of this new perspective by developing a new state-of-the-art semantic parser for English Resource Semantics.',\n",
       "  'At the core of this parser is a novel neural graph rewriting system which combines the strengths of Hyperedge Replacement Grammar, a knowledge-intensive model, and Graph Neural Networks, a data-intensive model.',\n",
       "  'Our parser achieves an accuracy of 92.39% in terms of elementary dependency match, which is a 2.88 point improvement over the best data-driven model in the literature.',\n",
       "  'The output of our parser is highly coherent: at least 91% graphs are valid, in that they allow at least one sound scope-resolved logical form.'],\n",
       " ['This paper is concerned with semantic parsing for English as a second language (ESL).',\n",
       "  'Motivated by the theoretical emphasis on the learning challenges that occur at the syntax-semantics interface during second language acquisition, we formulate the task based on the divergence between literal and intended meanings.',\n",
       "  'We combine the complementary strengths of English Resource Grammar, a linguistically-precise hand-crafted deep grammar, and TLE, an existing manually annotated ESL UD-TreeBank with a novel reranking model.',\n",
       "  'Experiments demonstrate that in comparison to human annotations, our method can obtain a very promising SemBanking quality.',\n",
       "  'By means of the newly created corpus, we evaluate state-of-the-art semantic parsing as well as grammatical error correction models.',\n",
       "  'The evaluation profiles the performance of neural NLP techniques for handling ESL data and suggests some research directions.'],\n",
       " ['Semantic dependency parsing, which aims to find rich bi-lexical relationships, allows words to have multiple dependency heads, resulting in graph-structured representations.',\n",
       "  'We propose an approach to semi-supervised learning of semantic dependency parsers based on the CRF autoencoder framework.',\n",
       "  'Our encoder is a discriminative neural semantic dependency parser that predicts the latent parse graph of the input sentence.',\n",
       "  'Our decoder is a generative neural model that reconstructs the input sentence conditioned on the latent parse graph.',\n",
       "  'Our model is arc-factored and therefore parsing and learning are both tractable.',\n",
       "  'Experiments show our model achieves significant and consistent improvement over the supervised baseline.'],\n",
       " ['One daunting problem for semantic parsing is the scarcity of annotation.',\n",
       "  'Aiming to reduce nontrivial human labor, we propose a two-stage semantic parsing framework, where the first stage utilizes an unsupervised paraphrase model to convert an unlabeled natural language utterance into the canonical utterance.',\n",
       "  'The downstream naive semantic parser accepts the intermediate output and returns the target logical form.',\n",
       "  'Furthermore, the entire training process is split into two phases: pre-training and cycle learning.',\n",
       "  'Three tailored self-supervised tasks are introduced throughout training to activate the unsupervised paraphrase model.',\n",
       "  'Experimental results on benchmarks Overnight and GeoGranno demonstrate that our framework is effective and compatible with supervised training.'],\n",
       " ['Discourse representation tree structure (DRTS) parsing is a novel semantic parsing task which has been concerned most recently.',\n",
       "  'State-of-the-art performance can be achieved by a neural sequence-to-sequence model, treating the tree construction as an incremental sequence generation problem.',\n",
       "  'Structural information such as input syntax and the intermediate skeleton of the partial output has been ignored in the model, which could be potentially useful for the DRTS parsing.',\n",
       "  'In this work, we propose a structural-aware model at both the encoder and decoder phase to integrate the structural information, where graph attention network (GAT) is exploited for effectively modeling.',\n",
       "  'Experimental results on a benchmark dataset show that our proposed model is effective and can obtain the best performance in the literature.'],\n",
       " ['We tackle the task of Term Set Expansion (TSE): given a small seed set of example terms from a semantic class, finding more members of that class.',\n",
       "  'The task is of great practical utility, and also of theoretical utility as it requires generalization from few examples.',\n",
       "  'Previous approaches to the TSE task can be characterized as either distributional or pattern-based.',\n",
       "  'We harness the power of neural masked language models (MLM) and propose a novel TSE algorithm, which combines the pattern-based and distributional approaches.',\n",
       "  'Due to the small size of the seed set, fine-tuning methods are not effective, calling for more creative use of the MLM.',\n",
       "  'The gist of the idea is to use the MLM to first mine for informative patterns with respect to the seed set, and then to obtain more members of the seed class by generalizing these patterns.',\n",
       "  'Our method outperforms state-of-the-art TSE algorithms.',\n",
       "  'Implementation is available at: https://github.com/ guykush/TermSetExpansion-MPB/'],\n",
       " ['Recently, the character-word lattice structure has been proved to be effective for Chinese named entity recognition (NER) by incorporating the word information.',\n",
       "  'However, since the lattice structure is complex and dynamic, the lattice-based models are hard to fully utilize the parallel computation of GPUs and usually have a low inference speed.',\n",
       "  'In this paper, we propose FLAT: Flat-LAttice Transformer for Chinese NER, which converts the lattice structure into a flat structure consisting of spans.',\n",
       "  'Each span corresponds to a character or latent word and its position in the original lattice.',\n",
       "  'With the power of Transformer and well-designed position encoding, FLAT can fully leverage the lattice information and has an excellent parallel ability.',\n",
       "  'Experiments on four datasets show FLAT outperforms other lexicon-based models in performance and efficiency.'],\n",
       " ['Entity embeddings, which represent different aspects of each entity with a single vector like word embeddings, are a key component of neural entity linking models.',\n",
       "  'Existing entity embeddings are learned from canonical Wikipedia articles and local contexts surrounding target entities.',\n",
       "  'Such entity embeddings are effective, but too distinctive for linking models to learn contextual commonality.',\n",
       "  'We propose a simple yet effective method, FGS2EE, to inject fine-grained semantic information into entity embeddings to reduce the distinctiveness and facilitate the learning of contextual commonality.',\n",
       "  'FGS2EE first uses the embeddings of semantic type words to generate semantic embeddings, and then combines them with existing entity embeddings through linear aggregation.',\n",
       "  'Extensive experiments show the effectiveness of such embeddings.',\n",
       "  'Based on our entity embeddings, we achieved new sate-of-the-art performance on entity linking.'],\n",
       " ['We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT).',\n",
       "  'Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013–2015 containing English documents and queries in several European languages.',\n",
       "  'We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach).',\n",
       "  'The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages.',\n",
       "  'NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT.'],\n",
       " ['Showing items that do not match search query intent degrades customer experience in e-commerce.',\n",
       "  'These mismatches result from counterfactual biases of the ranking algorithms toward noisy behavioral signals such as clicks and purchases in the search logs.',\n",
       "  'Mitigating the problem requires a large labeled dataset, which is expensive and time-consuming to obtain.',\n",
       "  'In this paper, we develop a deep, end-to-end model that learns to effectively classify mismatches and to generate hard mismatched examples to improve the classifier.',\n",
       "  'We train the model end-to-end by introducing a latent variable into the cross-entropy loss that alternates between using the real and generated samples.',\n",
       "  'This not only makes the classifier more robust but also boosts the overall ranking performance.',\n",
       "  'Our model achieves a relative gain compared to baselines by over 26% in F-score, and over 17% in Area Under PR curve.',\n",
       "  'On live search traffic, our model gains significant improvement in multiple countries.'],\n",
       " ['Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e.',\n",
       "  'over-confident) predictions, a common sign of overfitting.',\n",
       "  'This class of techniques, of which label smoothing is one, has a connection to entropy regularization.',\n",
       "  'Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored.',\n",
       "  'We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks.',\n",
       "  'We also find that variance in model performance can be explained largely by the resulting entropy of the model.',\n",
       "  'Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place.'],\n",
       " ['Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations.',\n",
       "  'Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations.',\n",
       "  'The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms.',\n",
       "  'We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.'],\n",
       " ['Knowledge graph (KG) embeddings learn low- dimensional representations of entities and relations to predict missing facts.',\n",
       "  'KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space.',\n",
       "  'For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations.',\n",
       "  'However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs.',\n",
       "  'In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns.',\n",
       "  'Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns.',\n",
       "  'Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions.',\n",
       "  'Furthermore, we observe that different geometric transformations capture different types of relations while attention- based transformations generalize to multiple relations.',\n",
       "  'In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10.'],\n",
       " ['Effective projection-based cross-lingual word embedding (CLWE) induction critically relies on the iterative self-learning procedure.',\n",
       "  'It gradually expands the initial small seed dictionary to learn improved cross-lingual mappings.',\n",
       "  'In this work, we present ClassyMap, a classification-based approach to self-learning, yielding a more robust and a more effective induction of projection-based CLWEs.',\n",
       "  'Unlike prior self-learning methods, our approach allows for integration of diverse features into the iterative process.',\n",
       "  'We show the benefits of ClassyMap for bilingual lexicon induction: we report consistent improvements in a weakly supervised setup (500 seed translation pairs) on a benchmark with 28 language pairs.'],\n",
       " ['Translating from languages without productive grammatical gender like English into gender-marked languages is a well-known difficulty for machines.',\n",
       "  'This difficulty is also due to the fact that the training data on which models are built typically reflect the asymmetries of natural languages, gender bias included.',\n",
       "  'Exclusively fed with textual data, machine translation is intrinsically constrained by the fact that the input sentence does not always contain clues about the gender identity of the referred human entities.',\n",
       "  'But what happens with speech translation, where the input is an audio signal? Can audio provide additional information to reduce gender bias? We present the first thorough investigation of gender bias in speech translation, contributing with: i) the release of a benchmark useful for future studies, and ii) the comparison of different technologies (cascade and end-to-end) on two language directions (English-Italian/French).'],\n",
       " ['Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages.',\n",
       "  'The keys lie in the assessment of data difficulty and model competence.',\n",
       "  'We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage.',\n",
       "  'Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty.',\n",
       "  'Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed.',\n",
       "  'Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.'],\n",
       " ['Exploiting natural language processing in the clinical domain requires de-identification, i.e., anonymization of personal information in texts.',\n",
       "  'However, current research considers de-identification and downstream tasks, such as concept extraction, only in isolation and does not study the effects of de-identification on other tasks.',\n",
       "  'In this paper, we close this gap by reporting concept extraction performance on automatically anonymized data and investigating joint models for de-identification and concept extraction.',\n",
       "  'In particular, we propose a stacked model with restricted access to privacy sensitive information and a multitask model.',\n",
       "  'We set the new state of the art on benchmark datasets in English (96.1% F1 for de-identification and 88.9% F1 for concept extraction) and Spanish (91.4% F1 for concept extraction).'],\n",
       " ['In this paper, we present CorefQA, an accurate and extensible approach for the coreference resolution task.',\n",
       "  'We formulate the problem as a span prediction task, like in question answering: A query is generated for each candidate mention using its surrounding context, and a span prediction module is employed to extract the text spans of the coreferences within the document using the generated query.',\n",
       "  'This formulation comes with the following key advantages: (1) The span prediction strategy provides the flexibility of retrieving mentions left out at the mention proposal stage; (2) In the question answering framework, encoding the mention and its context explicitly in a query makes it possible to have a deep and thorough examination of cues embedded in the context of coreferent mentions; and (3) A plethora of existing question answering datasets can be used for data augmentation to improve the model’s generalization capability.',\n",
       "  'Experiments demonstrate significant performance boost over previous models, with 83.1 (+3.5) F1 score on the CoNLL-2012 benchmark and 87.5 (+2.5) F1 score on the GAP benchmark.'],\n",
       " ['The inability to correctly resolve rumours circulating online can have harmful real-world consequences.',\n",
       "  'We present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification.',\n",
       "  'We show that these estimates can be used to filter out model predictions likely to be erroneous so that these difficult instances can be prioritised by a human fact-checker.',\n",
       "  'We propose two methods for uncertainty-based instance rejection, supervised and unsupervised.',\n",
       "  'We also show how uncertainty estimates can be used to interpret model performance as a rumour unfolds.'],\n",
       " ['Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB).',\n",
       "  'It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge.',\n",
       "  'The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs.',\n",
       "  'Existing approaches are mostly inappropriate for this, as they depend on training data.',\n",
       "  'However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch.',\n",
       "  'We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users.',\n",
       "  'We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy.',\n",
       "  'In a user study, the annotation speed improves by 35% compared to annotating without interactive support; users report that they strongly prefer our system.',\n",
       "  'An open-source and ready-to-use implementation based on the text annotation platform INCEpTION (https://inception-project.github.io) is made available.'],\n",
       " ['Transfer learning using ImageNet pre-trained models has been the de facto approach in a wide range of computer vision tasks.',\n",
       "  'However, fine-tuning still requires task-specific training data.',\n",
       "  'In this paper, we propose N3 (Neural Networks from Natural Language) - a new paradigm of synthesizing task-specific neural networks from language descriptions and a generic pre-trained model.',\n",
       "  'N3 leverages language descriptions to generate parameter adaptations as well as a new task-specific classification layer for a pre-trained neural network, effectively “fine-tuning” the network for a new task using only language descriptions as input.',\n",
       "  'To the best of our knowledge, N3 is the first method to synthesize entire neural networks from natural language.',\n",
       "  'Experimental results show that N3 can out-perform previous natural-language based zero-shot learning methods across 4 different zero-shot image classification benchmarks.',\n",
       "  'We also demonstrate a simple method to help identify keywords in language descriptions leveraged by N3 when synthesizing model parameters.'],\n",
       " ['Question-answer driven Semantic Role Labeling (QA-SRL) was proposed as an attractive open and natural flavour of SRL, potentially attainable from laymen.',\n",
       "  'Recently, a large-scale crowdsourced QA-SRL corpus and a trained parser were released.',\n",
       "  'Trying to replicate the QA-SRL annotation for new texts, we found that the resulting annotations were lacking in quality, particularly in coverage, making them insufficient for further research and evaluation.',\n",
       "  'In this paper, we present an improved crowdsourcing protocol for complex semantic annotation, involving worker selection and training, and a data consolidation phase.',\n",
       "  'Applying this protocol to QA-SRL yielded high-quality annotation with drastically higher coverage, producing a new gold evaluation dataset.',\n",
       "  'We believe that our annotation protocol and gold standard will facilitate future replicable research of natural semantic annotations.'],\n",
       " ['Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding.',\n",
       "  'Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English.',\n",
       "  'While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances.',\n",
       "  'Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection.',\n",
       "  'In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations.',\n",
       "  'Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly.'],\n",
       " ['We address the task of unsupervised Semantic Textual Similarity (STS) by ensembling diverse pre-trained sentence encoders into sentence meta-embeddings.',\n",
       "  'We apply, extend and evaluate different meta-embedding methods from the word embedding literature at the sentence level, including dimensionality reduction (Yin and Schütze, 2016), generalized Canonical Correlation Analysis (Rastogi et al., 2015) and cross-view auto-encoders (Bollegala and Bao, 2018).',\n",
       "  'Our sentence meta-embeddings set a new unsupervised State of The Art (SoTA) on the STS Benchmark and on the STS12-STS16 datasets, with gains of between 3.7% and 6.4% Pearson’s r over single-source systems.'],\n",
       " ['Transition-based parsers implemented with Pointer Networks have become the new state of the art in dependency parsing, excelling in producing labelled syntactic trees and outperforming graph-based models in this task.',\n",
       "  'In order to further test the capabilities of these powerful neural networks on a harder NLP problem, we propose a transition system that, thanks to Pointer Networks, can straightforwardly produce labelled directed acyclic graphs and perform semantic dependency parsing.',\n",
       "  'In addition, we enhance our approach with deep contextualized word embeddings extracted from BERT.',\n",
       "  'The resulting system not only outperforms all existing transition-based models, but also matches the best fully-supervised accuracy to date on the SemEval 2015 Task 18 datasets among previous state-of-the-art graph-based parsers.'],\n",
       " ['Semantic similarity detection is a fundamental task in natural language understanding.',\n",
       "  'Adding topic information has been useful for previous feature-engineered semantic similarity models as well as neural models for other tasks.',\n",
       "  'There is currently no standard way of combining topics with pretrained contextual representations such as BERT.',\n",
       "  'We propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets.',\n",
       "  'We find that the addition of topics to BERT helps particularly with resolving domain-specific cases.'],\n",
       " ['Aspect term extraction aims to extract aspect terms from review texts as opinion targets for sentiment analysis.',\n",
       "  'One of the big challenges with this task is the lack of sufficient annotated data.',\n",
       "  'While data augmentation is potentially an effective technique to address the above issue, it is uncontrollable as it may change aspect words and aspect labels unexpectedly.',\n",
       "  'In this paper, we formulate the data augmentation as a conditional generation task: generating a new sentence while preserving the original opinion targets and labels.',\n",
       "  'We propose a masked sequence-to-sequence method for conditional augmentation of aspect term extraction.',\n",
       "  'Unlike existing augmentation approaches, ours is controllable and allows to generate more diversified sentences.',\n",
       "  'Experimental results confirm that our method alleviates the data scarcity problem significantly.',\n",
       "  'It also effectively boosts the performances of several current models for aspect term extraction.'],\n",
       " ['Predicting the persuasiveness of arguments has applications as diverse as writing assistance, essay scoring, and advertising.',\n",
       "  'While clearly relevant to the task, the personal characteristics of an argument’s source and audience have not yet been fully exploited toward automated persuasiveness prediction.',\n",
       "  'In this paper, we model debaters’ prior beliefs, interests, and personality traits based on their previous activity, without dependence on explicit user profiles or questionnaires.',\n",
       "  'Using a dataset of over 60,000 argumentative discussions, comprising more than three million individual posts collected from the subreddit r/ChangeMyView, we demonstrate that our modeling of debater’s characteristics enhances the prediction of argument persuasiveness as well as of debaters’ resistance to persuasion.'],\n",
       " ['An educated and informed consumption of media content has become a challenge in modern times.',\n",
       "  'With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in “echo chambers” and may fall prey to fake news and disinformation, lacking easy access to dissenting views.',\n",
       "  'We suggest a novel task aiming to alleviate some of these concerns – that of detecting articles that most effectively counter the arguments – and not just the stance – made in a given text.',\n",
       "  'We study this problem in the context of debate speeches.',\n",
       "  'Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it.',\n",
       "  'We provide a large dataset of 3,685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community.',\n",
       "  'We explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research.',\n",
       "  'All data collected during this work is freely available for research.'],\n",
       " ['Neural network-based sequence-to-sequence (seq2seq) models strongly suffer from the low-diversity problem when it comes to open-domain dialogue generation.',\n",
       "  'As bland and generic utterances usually dominate the frequency distribution in our daily chitchat, avoiding them to generate more interesting responses requires complex data filtering, sampling techniques or modifying the training objective.',\n",
       "  'In this paper, we propose a new perspective to diversify dialogue generation by leveraging non-conversational text.',\n",
       "  'Compared with bilateral conversations, non-conversational text are easier to obtain, more diverse and cover a much broader range of topics.',\n",
       "  'We collect a large-scale non-conversational corpus from multi sources including forum comments, idioms and book snippets.',\n",
       "  'We further present a training paradigm to effectively incorporate these text via iterative back translation.',\n",
       "  'The resulting model is tested on two conversational datasets from different domains and is shown to produce significantly more diverse responses without sacrificing the relevance with context.'],\n",
       " ['The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations.',\n",
       "  'In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs.',\n",
       "  'Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0.',\n",
       "  'These conversations contain in-depth discussions on related topics and natural transition between multiple topics.',\n",
       "  'To facilitate the following research on this corpus, we provide several benchmark models.',\n",
       "  'Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research.',\n",
       "  'Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation.',\n",
       "  'The corpus and benchmark models are publicly available.'],\n",
       " ['A Dialogue State Tracker (DST) is a core component of a modular task-oriented dialogue system.',\n",
       "  'Tremendous progress has been made in recent years.',\n",
       "  'However, the major challenges remain.',\n",
       "  'The state-of-the-art accuracy for DST is below 50% for a multi-domain dialogue task.',\n",
       "  'A learnable DST for any new domain requires a large amount of labeled in-domain data and training from scratch.',\n",
       "  'In this paper, we propose a Meta-Reinforced Multi-Domain State Generator (MERET).',\n",
       "  'Our first contribution is to improve the DST accuracy.',\n",
       "  'We enhance a neural model based DST generator with a reward manager, which is built on policy gradient reinforcement learning (RL) to fine-tune the generator.',\n",
       "  'With this change, we are able to improve the joint accuracy of DST from 48.79% to 50.91% on the MultiWOZ corpus.',\n",
       "  'Second, we explore to train a DST meta-learning model with a few domains as source domains and a new domain as target domain.',\n",
       "  'We apply the model-agnostic meta-learning algorithm (MAML) to DST and the obtained meta-learning model is used for new domain adaptation.',\n",
       "  'Our experimental results show this solution is able to outperform the traditional training approach with extremely less training data in target domain.'],\n",
       " ['Based on the recently proposed transferable dialogue state generator (TRADE) that predicts dialogue states from utterance-concatenated dialogue context, we propose a multi-task learning model with a simple yet effective utterance tagging technique and a bidirectional language model as an auxiliary task for task-oriented dialogue state generation.',\n",
       "  'By enabling the model to learn a better representation of the long dialogue context, our approaches attempt to solve the problem that the performance of the baseline significantly drops when the input dialogue context sequence is long.',\n",
       "  'In our experiments, our proposed model achieves a 7.03% relative improvement over the baseline, establishing a new state-of-the-art joint goal accuracy of 52.04% on the MultiWOZ 2.0 dataset.'],\n",
       " ['Generating fluent and informative responses is of critical importance for task-oriented dialogue systems.',\n",
       "  'Existing pipeline approaches generally predict multiple dialogue acts first and use them to assist response generation.',\n",
       "  'There are at least two shortcomings with such approaches.',\n",
       "  'First, the inherent structures of multi-domain dialogue acts are neglected.',\n",
       "  'Second, the semantic associations between acts and responses are not taken into account for response generation.',\n",
       "  'To address these issues, we propose a neural co-generation model that generates dialogue acts and responses concurrently.',\n",
       "  'Unlike those pipeline approaches, our act generation module preserves the semantic structures of multi-domain dialogue acts and our response generation module dynamically attends to different acts as needed.',\n",
       "  'We train the two modules jointly using an uncertainty loss to adjust their task weights adaptively.',\n",
       "  'Extensive experiments are conducted on the large-scale MultiWOZ dataset and the results show that our model achieves very favorable improvement over several state-of-the-art models in both automatic and human evaluations.'],\n",
       " ['Unsupervised style transfer aims to change the style of an input sentence while preserving its original content without using parallel training data.',\n",
       "  'In current dominant approaches, owing to the lack of fine-grained control on the influence from the target style, they are unable to yield desirable output sentences.',\n",
       "  'In this paper, we propose a novel attentional sequence-to-sequence (Seq2seq) model that dynamically exploits the relevance of each output word to the target style for unsupervised style transfer.',\n",
       "  'Specifically, we first pretrain a style classifier, where the relevance of each input word to the original style can be quantified via layer-wise relevance propagation.',\n",
       "  'In a denoising auto-encoding manner, we train an attentional Seq2seq model to reconstruct input sentences and repredict word-level previously-quantified style relevance simultaneously.',\n",
       "  'In this way, this model is endowed with the ability to automatically predict the style relevance of each output word.',\n",
       "  'Then, we equip the decoder of this model with a neural style component to exploit the predicted wordlevel style relevance for better style transfer.',\n",
       "  'Particularly, we fine-tune this model using a carefully-designed objective function involving style transfer, style relevance consistency, content preservation and fluency modeling loss terms.',\n",
       "  'Experimental results show that our proposed model achieves state-of-the-art performance in terms of both transfer accuracy and content preservation.'],\n",
       " ['The graph-to-sequence (Graph2Seq) learning aims to transduce graph-structured representations to word sequences for text generation.',\n",
       "  'Recent studies propose various models to encode graph structure.',\n",
       "  'However, most previous works ignore the indirect relations between distance nodes, or treat indirect relations and direct relations in the same way.',\n",
       "  'In this paper, we propose the Heterogeneous Graph Transformer to independently model the different relations in the individual subgraphs of the original graph, including direct relations, indirect relations and multiple possible relations between nodes.',\n",
       "  'Experimental results show that our model strongly outperforms the state of the art on all four standard benchmarks of AMR-to-text generation and syntax-based neural machine translation.'],\n",
       " ['The neural attention model has achieved great success in data-to-text generation tasks.',\n",
       "  'Though usually excelling at producing fluent text, it suffers from the problem of information missing, repetition and “hallucination”.',\n",
       "  'Due to the black-box nature of the neural attention architecture, avoiding these problems in a systematic way is non-trivial.',\n",
       "  'To address this concern, we propose to explicitly segment target text into fragment units and align them with their data correspondences.',\n",
       "  'The segmentation and correspondence are jointly learned as latent variables without any human annotations.',\n",
       "  'We further impose a soft statistical constraint to regularize the segmental granularity.',\n",
       "  'The resulting architecture maintains the same expressive power as neural attention models, while being able to generate fully interpretable outputs with several times less computational cost.',\n",
       "  'On both E2E and WebNLG benchmarks, we show the proposed model consistently outperforms its neural attention counterparts.'],\n",
       " ['Visual question answering aims to answer the natural language question about a given image.',\n",
       "  'Existing graph-based methods only focus on the relations between objects in an image and neglect the importance of the syntactic dependency relations between words in a question.',\n",
       "  'To simultaneously capture the relations between objects in an image and the syntactic dependency relations between words in a question, we propose a novel dual channel graph convolutional network (DC-GCN) for better combining visual and textual advantages.',\n",
       "  'The DC-GCN model consists of three parts: an I-GCN module to capture the relations between objects in an image, a Q-GCN module to capture the syntactic dependency relations between words in a question, and an attention alignment module to align image representations and question representations.',\n",
       "  'Experimental results show that our model achieves comparable performance with the state-of-the-art approaches.'],\n",
       " ['We introduce a new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering.',\n",
       "  'The MN-GMN uses graph structure with different region features as node attributes and applies a recently proposed powerful graph neural network model, Graph Network (GN), to reason about objects and their interactions in an image.',\n",
       "  'The input module of the MN-GMN generates a set of visual features plus a set of encoded region-grounded captions (RGCs) for the image.',\n",
       "  'The RGCs capture object attributes and their relationships.',\n",
       "  'Two GNs are constructed from the input module using the visual features and encoded RGCs.',\n",
       "  'Each node of the GNs iteratively computes a question-guided contextualized representation of the visual/textual information assigned to it.',\n",
       "  'Then, to combine the information from both GNs, the nodes write the updated representations to an external spatial memory.',\n",
       "  'The final states of the memory cells are fed into an answer module to predict an answer.',\n",
       "  'Experiments show MN-GMN rivals the state-of-the-art models on Visual7W, VQA-v2.0, and CLEVR datasets.'],\n",
       " ['We propose a novel large-scale referring expression recognition dataset, Refer360°, consisting of 17,137 instruction sequences and ground-truth actions for completing these instructions in 360° scenes.',\n",
       "  'Refer360° differs from existing related datasets in three ways.',\n",
       "  'First, we propose a more realistic scenario where instructors and the followers have partial, yet dynamic, views of the scene – followers continuously modify their field-of-view (FoV) while interpreting instructions that specify a final target location.',\n",
       "  'Second, instructions to find the target location consist of multiple steps for followers who will start at random FoVs.',\n",
       "  'As a result, intermediate instructions are strongly grounded in object references, and followers must identify intermediate FoVs to find the final target location correctly.',\n",
       "  'Third, the target locations are neither restricted to predefined objects nor chosen by annotators; instead, they are distributed randomly across scenes.',\n",
       "  'This “point anywhere” approach leads to more linguistically complex instructions, as shown in our analyses.',\n",
       "  'Our examination of the dataset shows that Refer360° manifests linguistically rich phenomena in a language grounding task that poses novel challenges for computational modeling of language, vision, and navigation.'],\n",
       " ['Pretrained language models are now ubiquitous in Natural Language Processing.',\n",
       "  'Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages.',\n",
       "  'This makes practical use of such models –in all languages except English– very limited.',\n",
       "  'In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks.',\n",
       "  'We show that the use of web crawled data is preferable to the use of Wikipedia data.',\n",
       "  'More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB).',\n",
       "  'Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.'],\n",
       " ['Advances in variational inference enable parameterisation of probabilistic models by deep neural networks.',\n",
       "  'This combines the statistical transparency of the probabilistic modelling framework with the representational power of deep learning.',\n",
       "  'Yet, due to a problem known as posterior collapse, it is difficult to estimate such models in the context of language modelling effectively.',\n",
       "  'We concentrate on one such model, the variational auto-encoder, which we argue is an important building block in hierarchical probabilistic models of language.',\n",
       "  'This paper contributes a sober view of the problem, a survey of techniques to address it, novel techniques, and extensions to the model.',\n",
       "  'To establish a ranking of techniques, we perform a systematic comparison using Bayesian optimisation and find that many techniques perform reasonably similar, given enough resources.',\n",
       "  'Still, a favourite can be named based on convenience.',\n",
       "  'We also make several empirical observations and recommendations of best practices that should help researchers interested in this exciting field.'],\n",
       " ['The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models.',\n",
       "  'We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations.',\n",
       "  'Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space.',\n",
       "  'By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it.',\n",
       "  'While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.'],\n",
       " ['Simplified Chinese to Traditional Chinese character conversion is a common preprocessing step in Chinese NLP.',\n",
       "  'Despite this, current approaches have insufficient performance because they do not take into account that a simplified Chinese character can correspond to multiple traditional characters.',\n",
       "  'Here, we propose a model that can disambiguate between mappings and convert between the two scripts.',\n",
       "  'The model is based on subword segmentation, two language models, as well as a method for mapping between subword sequences.',\n",
       "  'We further construct benchmark datasets for topic classification and script conversion.',\n",
       "  'Our proposed method outperforms previous Chinese Character conversion approaches by 6 points in accuracy.',\n",
       "  'These results are further confirmed in a downstream application, where 2kenize is used to convert pretraining dataset for topic classification.',\n",
       "  'An error analysis reveals that our method’s particular strengths are in dealing with code mixing and named entities.'],\n",
       " ['We present the first study that examines the evolution of morphological families, i.e., sets of morphologically related words such as “trump”, “antitrumpism”, and “detrumpify”, in social media.',\n",
       "  'We introduce the novel task of Morphological Family Expansion Prediction (MFEP) as predicting the increase in the size of a morphological family.',\n",
       "  'We create a ten-year Reddit corpus as a benchmark for MFEP and evaluate a number of baselines on this benchmark.',\n",
       "  'Our experiments demonstrate very good performance on MFEP.'],\n",
       " ['Historical text normalization, the task of mapping historical word forms to their modern counterparts, has recently attracted a lot of interest (Bollmann, 2019; Tang et al., 2018; Lusetti et al., 2018; Bollmann et al., 2018;Robertson and Goldwater, 2018; Bollmannet al., 2017; Korchagina, 2017).',\n",
       "  'Yet, virtually all approaches suffer from the two limitations: 1) They consider a fully supervised setup, often with impractically large manually normalized datasets; 2) Normalization happens on words in isolation.',\n",
       "  'By utilizing a simple generative normalization model and obtaining powerful contextualization from the target-side language model, we train accurate models with unlabeled historical data.',\n",
       "  'In realistic training scenarios, our approach often leads to reduction in manually normalized data at the same accuracy levels.'],\n",
       " ['Question answering and conversational systems are often baffled and need help clarifying certain ambiguities.',\n",
       "  'However, limitations of existing datasets hinder the development of large-scale models capable of generating and utilising clarification questions.',\n",
       "  'In order to overcome these limitations, we devise a novel bootstrapping framework (based on self-supervision) that assists in the creation of a diverse, large-scale dataset of clarification questions based on post-comment tuples extracted from stackexchange.',\n",
       "  'The framework utilises a neural network based architecture for classifying clarification questions.',\n",
       "  'It is a two-step method where the first aims to increase the precision of the classifier and second aims to increase its recall.',\n",
       "  'We quantitatively demonstrate the utility of the newly created dataset by applying it to the downstream task of question-answering.',\n",
       "  'The final dataset, ClarQ, consists of ~2M examples distributed across 173 domains of stackexchange.',\n",
       "  'We release this dataset in order to foster research into the field of clarification question generation with the larger goal of enhancing dialog and question answering systems.'],\n",
       " ['The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of domain-specific information available in FAQ sites.',\n",
       "  'We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs.',\n",
       "  'The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing.',\n",
       "  'Compared to previous work, DoQA comprises well-defined information needs, leading to more coherent and natural conversations with less factoid questions and is multi-domain.',\n",
       "  'In addition, we introduce a more realistic information retrieval (IR) scenario where the system needs to find the answer in any of the FAQ documents.',\n",
       "  'The results of an existing, strong, system show that, thanks to transfer learning from a Wikipedia QA dataset and fine tuning on a single FAQ domain, it is possible to build high quality conversational QA systems for FAQs without in-domain training data.',\n",
       "  'The good results carry over into the more challenging IR scenario.',\n",
       "  'In both cases, there is still ample room for improvement, as indicated by the higher human upperbound.'],\n",
       " ['Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets.',\n",
       "  'Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging.',\n",
       "  'In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress.',\n",
       "  'We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area.',\n",
       "  'MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese.',\n",
       "  'MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average.',\n",
       "  'We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA.',\n",
       "  'In all cases, transfer results are shown to be significantly behind training-language performance.'],\n",
       " ['Multiple-choice question answering (MCQA) is one of the most challenging tasks in machine reading comprehension since it requires more advanced reading comprehension skills such as logical reasoning, summarization, and arithmetic operations.',\n",
       "  'Unfortunately, most existing MCQA datasets are small in size, which increases the difficulty of model learning and generalization.',\n",
       "  'To address this challenge, we propose a multi-source meta transfer (MMT) for low-resource MCQA.',\n",
       "  'In this framework, we first extend meta learning by incorporating multiple training sources to learn a generalized feature representation across domains.',\n",
       "  'To bridge the distribution gap between training sources and the target, we further introduce the meta transfer that can be integrated into the multi-source meta training.',\n",
       "  'More importantly, the proposed MMT is independent of backbone language models.',\n",
       "  'Extensive experiments demonstrate the superiority of MMT over state-of-the-arts, and continuous improvements can be achieved on different backbone networks on both supervised and unsupervised domain adaptation settings.'],\n",
       " ['Fact Verification requires fine-grained natural language inference capability that finds subtle clues to identify the syntactical and semantically correct but not well-supported claims.',\n",
       "  'This paper presents Kernel Graph Attention Network (KGAT), which conducts more fine-grained fact verification with kernel-based attentions.',\n",
       "  'Given a claim and a set of potential evidence sentences that form an evidence graph, KGAT introduces node kernels, which better measure the importance of the evidence node, and edge kernels, which conduct fine-grained evidence propagation in the graph, into Graph Attention Networks for more accurate fact verification.',\n",
       "  'KGAT achieves a 70.38% FEVER score and significantly outperforms existing fact verification models on FEVER, a large-scale benchmark for fact verification.',\n",
       "  'Our analyses illustrate that, compared to dot-product attentions, the kernel-based attention concentrates more on relevant evidence sentences and meaningful clues in the evidence graph, which is the main source of KGAT’s effectiveness.',\n",
       "  'All source codes of this work are available at https://github.com/thunlp/KernelGAT.'],\n",
       " ['Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims.',\n",
       "  'A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process – generating justifications for verdicts on claims.',\n",
       "  'This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction.',\n",
       "  'Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system.',\n",
       "  'The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.'],\n",
       " ['The discovery of supporting evidence for addressing complex mathematical problems is a semantically challenging task, which is still unexplored in the field of natural language processing for mathematical text.',\n",
       "  'The natural language premise selection task consists in using conjectures written in both natural language and mathematical formulae to recommend premises that most likely will be useful to prove a particular statement.',\n",
       "  'We propose an approach to solve this task as a link prediction problem, using Deep Convolutional Graph Neural Networks.',\n",
       "  'This paper also analyses how different baselines perform in this task and shows that a graph structure can provide higher F1-score, especially when considering multi-hop premise selection.'],\n",
       " ['We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them.',\n",
       "  'An existing rationale for such research is based on the lack of parallel data for many of the world’s languages.',\n",
       "  'However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice.',\n",
       "  'We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting.',\n",
       "  'We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices.',\n",
       "  'Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.'],\n",
       " ['Measuring what linguistic information is encoded in neural models of language has become popular in NLP.',\n",
       "  'Researchers approach this enterprise by training “probes”—supervised models designed to extract linguistic structure from another model’s output.',\n",
       "  'One such probe is the structural probe (Hewitt and Manning, 2019), designed to quantify the extent to which syntactic information is encoded in contextualised word representations.',\n",
       "  'The structural probe has a novel design, unattested in the parsing literature, the precise benefit of which is not immediately obvious.',\n",
       "  'To explore whether syntactic probes would do better to make use of existing techniques, we compare the structural probe to a more traditional parser with an identical lightweight parameterisation.',\n",
       "  'The parser outperforms structural probe on UUAS in seven of nine analysed languages, often by a substantial amount (e.g.',\n",
       "  'by 11.1 points in English).',\n",
       "  'Under a second less common metric, however, there is the opposite trend—the structural probe outperforms the parser.',\n",
       "  'This begs the question: which metric should we prefer?'],\n",
       " ['It has been exactly a decade since the first establishment of SPMRL, a research initiative unifying multiple research efforts to address the peculiar challenges of Statistical Parsing for Morphologically-Rich Languages (MRLs).',\n",
       "  'Here we reflect on parsing MRLs in that decade, highlight the solutions and lessons learned for the architectural, modeling and lexical challenges in the pre-neural era, and argue that similar challenges re-emerge in neural architectures for MRLs.',\n",
       "  'We then aim to offer a climax, suggesting that incorporating symbolic ideas proposed in SPMRL terms into nowadays neural architectures has the potential to push NLP for MRLs to a new level.',\n",
       "  'We sketch a strategies for designing Neural Models for MRLs (NMRL), and showcase preliminary support for these strategies via investigating the task of multi-tagging in Hebrew, a morphologically-rich, high-fusion, language.'],\n",
       " ['Over its three decade history, speech translation has experienced several shifts in its primary research themes; moving from loosely coupled cascades of speech recognition and machine translation, to exploring questions of tight coupling, and finally to end-to-end models that have recently attracted much attention.',\n",
       "  'This paper provides a brief survey of these developments, along with a discussion of the main challenges of traditional approaches which stem from committing to intermediate representations from the speech recognizer, and from training cascaded models separately towards different objectives.',\n",
       "  'Recent end-to-end modeling techniques promise a principled way of overcoming these issues by allowing joint training of all model components and removing the need for explicit intermediate representations.',\n",
       "  'However, a closer look reveals that many end-to-end models fall short of solving these issues, due to compromises made to address data scarcity.',\n",
       "  'This paper provides a unifying categorization and nomenclature that covers both traditional and recent approaches and that may help researchers by highlighting both trade-offs and open research questions.'],\n",
       " ['In addition to the traditional task of machines answering questions, question answering (QA) research creates interesting, challenging questions that help systems how to answer questions and reveal the best systems.',\n",
       "  'We argue that creating a QA dataset—and the ubiquitous leaderboard that goes with it—closely resembles running a trivia tournament: you write questions, have agents (either humans or machines) answer the questions, and declare a winner.',\n",
       "  'However, the research community has ignored the hard-learned lessons from decades of the trivia community creating vibrant, fair, and effective question answering competitions.',\n",
       "  'After detailing problems with existing QA datasets, we outline the key lessons—removing ambiguity, discriminating skill, and adjudicating disputes—that can transfer to QA research and how they might be implemented.'],\n",
       " ['Distributional semantic models have become a mainstay in NLP, providing useful features for downstream tasks.',\n",
       "  'However, assessing long-term progress requires explicit long-term goals.',\n",
       "  'In this paper, I take a broad linguistic perspective, looking at how well current models can deal with various semantic challenges.',\n",
       "  'Given stark differences between models proposed in different subfields, a broad perspective is needed to see how we could integrate them.',\n",
       "  'I conclude that, while linguistic insights can guide the design of model architectures, future progress will require balancing the often conflicting demands of linguistic expressiveness and computational tractability.'],\n",
       " ['Image captioning is a multimodal problem that has drawn extensive attention in both the natural language processing and computer vision community.',\n",
       "  'In this paper, we present a novel image captioning architecture to better explore semantics available in captions and leverage that to enhance both image representation and caption generation.',\n",
       "  'Our models first construct caption-guided visual relationship graphs that introduce beneficial inductive bias using weakly supervised multi-instance learning.',\n",
       "  'The representation is then enhanced with neighbouring and contextual nodes with their textual and visual features.',\n",
       "  'During generation, the model further incorporates visual relationships using multi-task learning for jointly predicting word and object/predicate tag sequences.',\n",
       "  'We perform extensive experiments on the MSCOCO dataset, showing that the proposed framework significantly outperforms the baselines, resulting in the state-of-the-art performance under a wide range of evaluation metrics.',\n",
       "  'The code of our paper has been made publicly available.'],\n",
       " ['The Surface Realization Shared Tasks of 2018 and 2019 were Natural Language Generation shared tasks with the goal of exploring approaches to surface realization from Universal-Dependency-like trees to surface strings for several languages.',\n",
       "  'In the 2018 shared task there was very little difference in the absolute performance of systems trained with and without additional, synthetically created data, and a new rule prohibiting the use of synthetic data was introduced for the 2019 shared task.',\n",
       "  'Contrary to the findings of the 2018 shared task, we show, in experiments on the English 2018 dataset, that the use of synthetic data can have a substantial positive effect – an improvement of almost 8 BLEU points for a previously state-of-the-art system.',\n",
       "  'We analyse the effects of synthetic data, and we argue that its use should be encouraged rather than prohibited so that future research efforts continue to explore systems that can take advantage of such data.'],\n",
       " ['We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives.',\n",
       "  'Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence.',\n",
       "  'Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings.',\n",
       "  'Notably this allows us to consider a large number of candidates for the next sentence during training.',\n",
       "  'We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.'],\n",
       " ['In this work, we explore the implicit event argument detection task, which studies event arguments beyond sentence boundaries.',\n",
       "  'The addition of cross-sentence argument candidates imposes great challenges for modeling.',\n",
       "  'To reduce the number of candidates, we adopt a two-step approach, decomposing the problem into two sub-problems: argument head-word detection and head-to-span expansion.',\n",
       "  'Evaluated on the recent RAMS dataset (Ebner et al., 2020), our model achieves overall better performance than a strong sequence labeling baseline.',\n",
       "  'We further provide detailed error analysis, presenting where the model mainly makes errors and indicating directions for future improvements.',\n",
       "  'It remains a challenge to detect implicit arguments, calling for more future work of document-level modeling for this task.'],\n",
       " ['Machine reading is an ambitious goal in NLP that subsumes a wide range of text understanding capabilities.',\n",
       "  'Within this broad framework, we address the task of machine reading the time of historical events, compile datasets for the task, and develop a model for tackling it.',\n",
       "  'Given a brief textual description of an event, we show that good performance can be achieved by extracting relevant sentences from Wikipedia, and applying a combination of task-specific and general-purpose feature embeddings for the classification.',\n",
       "  'Furthermore, we establish a link between the historical event ordering task and the event focus time task from the information retrieval literature, showing they also provide a challenging test case for machine reading algorithms.'],\n",
       " ['Unsupervised relation extraction (URE) extracts relations between named entities from raw text without manually-labelled data and existing knowledge bases (KBs).',\n",
       "  'URE methods can be categorised into generative and discriminative approaches, which rely either on hand-crafted features or surface form.',\n",
       "  'However, we demonstrate that by using only named entities to induce relation types, we can outperform existing methods on two popular datasets.',\n",
       "  'We conduct a comparison and evaluation of our findings with other URE techniques, to ascertain the important features in URE.',\n",
       "  'We conclude that entity types provide a strong inductive bias for URE.'],\n",
       " ['Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph.',\n",
       "  'It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections.',\n",
       "  'In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles.',\n",
       "  'We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources.',\n",
       "  'We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE.',\n",
       "  'Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models.',\n",
       "  'Our data and code are publicly available at https://github.com/allenai/SciREX .'],\n",
       " ['We propose a self-supervised method to solve Pronoun Disambiguation and Winograd Schema Challenge problems.',\n",
       "  'Our approach exploits the characteristic structure of training corpora related to so-called “trigger” words, which are responsible for flipping the answer in pronoun disambiguation.',\n",
       "  'We achieve such commonsense reasoning by constructing pair-wise contrastive auxiliary predictions.',\n",
       "  'To this end, we leverage a mutual exclusive loss regularized by a contrastive margin.',\n",
       "  'Our architecture is based on the recently introduced transformer networks, BERT, that exhibits strong performance on many NLP benchmarks.',\n",
       "  'Empirical results show that our method alleviates the limitation of current supervised approaches for commonsense reasoning.',\n",
       "  'This study opens up avenues for exploiting inexpensive self-supervision to achieve performance gain in commonsense reasoning tasks.'],\n",
       " ['Deep attention models have advanced the modelling of sequential data across many domains.',\n",
       "  'For language modelling in particular, the Transformer-XL — a Transformer augmented with a long-range memory of past activations — has been shown to be state-of-the-art across a variety of well-studied benchmarks.',\n",
       "  'The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors.',\n",
       "  'However it is unclear whether this is necessary.',\n",
       "  'We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.'],\n",
       " ['Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc.',\n",
       "  'Similar problems have been studied extensively for other forms of data, such as images and videos.',\n",
       "  'However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved).',\n",
       "  'Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics.',\n",
       "  'A new mutual information upper bound is derived and leveraged to measure dependence between style and content.',\n",
       "  'By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces.',\n",
       "  'Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.'],\n",
       " ['We consider a task based on CVPR 2018 challenge dataset on advertisement (Ad) understanding.',\n",
       "  'The task involves detecting the viewer’s interpretation of an Ad image captured as text.',\n",
       "  'Recent results have shown that the embedded scene-text in the image holds a vital cue for this task.',\n",
       "  'Motivated by this, we fine-tune the base BERT model for a sentence-pair classification task.',\n",
       "  'Despite utilizing the scene-text as the only source of visual information, we could achieve a hit-or-miss accuracy of 84.95% on the challenge test data.',\n",
       "  'To enable BERT to process other visual information, we append image captions to the scene-text.',\n",
       "  'This achieves an accuracy of 89.69%, which is an improvement of 4.7%.',\n",
       "  'This is the best reported result for this task.'],\n",
       " ['We present InstaMap, an instance-based method for learning projection-based cross-lingual word embeddings.',\n",
       "  'Unlike prior work, it deviates from learning a single global linear projection.',\n",
       "  'InstaMap is a non-parametric model that learns a non-linear projection by iteratively: (1) finding a globally optimal rotation of the source embedding space relying on the Kabsch algorithm, and then (2) moving each point along an instance-specific translation vector estimated from the translation vectors of the point’s nearest neighbours in the training dictionary.',\n",
       "  'We report performance gains with InstaMap over four representative state-of-the-art projection-based models on bilingual lexicon induction across a set of 28 diverse language pairs.',\n",
       "  'We note prominent improvements, especially for more distant language pairs (i.e., languages with non-isomorphic monolingual spaces).'],\n",
       " ['We propose a simple data augmentation protocol aimed at providing a compositional inductive bias in conditional and unconditional sequence models.',\n",
       "  'Under this protocol, synthetic training examples are constructed by taking real training examples and replacing (possibly discontinuous) fragments with other fragments that appear in at least one similar environment.',\n",
       "  'The protocol is model-agnostic and useful for a variety of tasks.',\n",
       "  'Applied to neural sequence-to-sequence models, it reduces error rate by as much as 87% on diagnostic tasks from the SCAN dataset and 16% on a semantic parsing task.',\n",
       "  'Applied to n-gram language models, it reduces perplexity by roughly 1% on small corpora in several languages.'],\n",
       " ['When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas.',\n",
       "  'The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query.',\n",
       "  'We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder.',\n",
       "  'On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement.',\n",
       "  'Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6% on the Spider leaderboard.',\n",
       "  'In addition, we observe qualitative improvements in the model’s understanding of schema linking and alignment.',\n",
       "  'Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql.'],\n",
       " ['Temporal common sense (e.g., duration and frequency of events) is crucial for understanding natural language.',\n",
       "  'However, its acquisition is challenging, partly because such information is often not expressed explicitly in text, and human annotation on such concepts is costly.',\n",
       "  'This work proposes a novel sequence modeling approach that exploits explicit and implicit mentions of temporal common sense, extracted from a large corpus, to build TacoLM, a temporal common sense language model.',\n",
       "  'Our method is shown to give quality predictions of various dimensions of temporal common sense (on UDST and a newly collected dataset from RealNews).',\n",
       "  'It also produces representations of events for relevant tasks such as duration comparison, parent-child relations, event coreference and temporal QA (on TimeBank, HiEVE and MCTACO) that are better than using the standard BERT.',\n",
       "  'Thus, it will be an important component of temporal NLP.'],\n",
       " ['Large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability.',\n",
       "  'We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding.',\n",
       "  'Our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones.'],\n",
       " ['Natural language processing models often have to make predictions on text data that evolves over time as a result of changes in language use or the information described in the text.',\n",
       "  'However, evaluation results on existing data sets are seldom reported by taking the timestamp of the document into account.',\n",
       "  'We analyze and propose methods that make better use of temporally-diverse training data, with a focus on the task of named entity recognition.',\n",
       "  'To support these experiments, we introduce a novel data set of English tweets annotated with named entities.',\n",
       "  'We empirically demonstrate the effect of temporal drift on performance, and how the temporal information of documents can be used to obtain better models compared to those that disregard temporal information.',\n",
       "  'Our analysis gives insights into why this information is useful, in the hope of informing potential avenues of improvement for named entity recognition as well as other NLP tasks under similar experimental setups.'],\n",
       " ['We tackle the task of building supervised event trigger identification models which can generalize better across domains.',\n",
       "  'Our work leverages the adversarial domain adaptation (ADA) framework to introduce domain-invariance.',\n",
       "  'ADA uses adversarial training to construct representations that are predictive for trigger identification, but not predictive of the example’s domain.',\n",
       "  'It requires no labeled data from the target domain, making it completely unsupervised.',\n",
       "  'Experiments with two domains (English literature and news) show that ADA leads to an average F1 score improvement of 3.9 on out-of-domain data.',\n",
       "  'Our best performing model (BERT-A) reaches 44-49 F1 across both domains, using no labeled target data.',\n",
       "  'Preliminary experiments reveal that finetuning on 1% labeled data, followed by self-training leads to substantial improvement, reaching 51.5 and 67.2 F1 on literature and news respectively.'],\n",
       " ['Approaches to Grounded Language Learning are commonly focused on a single task-based final performance measure which may not depend on desirable properties of the learned hidden representations, such as their ability to predict object attributes or generalize to unseen situations.',\n",
       "  'To remedy this, we present GroLLA, an evaluation framework for Grounded Language Learning with Attributes based on three sub-tasks: 1) Goal-oriented evaluation; 2) Object attribute prediction evaluation; and 3) Zero-shot evaluation.',\n",
       "  'We also propose a new dataset CompGuessWhat?! as an instance of this framework for evaluating the quality of learned neural representations, in particular with respect to attribute grounding.',\n",
       "  'To this end, we extend the original GuessWhat?! dataset by including a semantic layer on top of the perceptual one.',\n",
       "  'Specifically, we enrich the VisualGenome scene graphs associated with the GuessWhat?! images with several attributes from resources such as VISA and ImSitu.',\n",
       "  'We then compare several hidden state representations from current state-of-the-art approaches to Grounded Language Learning.',\n",
       "  'By using diagnostic classifiers, we show that current models’ learned representations are not expressive enough to encode object attributes (average F1 of 44.27).',\n",
       "  'In addition, they do not learn strategies nor representations that are robust enough to perform well when novel scenes or objects are involved in gameplay (zero-shot best accuracy 50.06%).'],\n",
       " ['This work deals with the challenge of learning and reasoning over language and vision data for the related downstream tasks such as visual question answering (VQA) and natural language for visual reasoning (NLVR).',\n",
       "  'We design a novel cross-modality relevance module that is used in an end-to-end framework to learn the relevance representation between components of various input modalities under the supervision of a target task, which is more generalizable to unobserved data compared to merely reshaping the original representation space.',\n",
       "  'In addition to modeling the relevance between the textual entities and visual entities, we model the higher-order relevance between entity relations in the text and object relations in the image.',\n",
       "  'Our proposed approach shows competitive performance on two different language and vision tasks using public benchmarks and improves the state-of-the-art published results.',\n",
       "  'The learned alignments of input spaces and their relevance representations by NLVR task boost the training efficiency of VQA task.'],\n",
       " ['We explore learning web-based tasks from a human teacher through natural language explanations and a single demonstration.',\n",
       "  'Our approach investigates a new direction for semantic parsing that models explaining a demonstration in a context, rather than mapping explanations to demonstrations.',\n",
       "  'By leveraging the idea of inverse semantics from program synthesis to reason backwards from observed demonstrations, we ensure that all considered interpretations are consistent with executable actions in any context, thus simplifying the problem of search over logical forms.',\n",
       "  'We present a dataset of explanations paired with demonstrations for web-based tasks.',\n",
       "  'Our methods show better task completion rates than a supervised semantic parsing baseline (40% relative improvement on average), and are competitive with simple exploration-and-demonstration based methods, while requiring no exploration of the environment.',\n",
       "  'In learning to align explanations with demonstrations, basic properties of natural language syntax emerge as learned behavior.',\n",
       "  'This is an interesting example of pragmatic language acquisition without any linguistic annotation.'],\n",
       " ['We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language.',\n",
       "  'Our starting point is a language model that has been trained on generic, not task-specific language data.',\n",
       "  'We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model.',\n",
       "  'We introduce a new way for combining the two types of learning based on the idea of reranking language model samples, and show that this method outperforms others in communicating with humans in a visual referential communication task.',\n",
       "  'Finally, we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them.'],\n",
       " ['Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation.',\n",
       "  'To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search.',\n",
       "  'We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers.',\n",
       "  'Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing.',\n",
       "  'Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware.',\n",
       "  'Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device).',\n",
       "  'When running WMT’14 translation task on Raspberry Pi-4, HAT can achieve 3× speedup, 3.7× smaller size over baseline Transformer; 2.7× speedup, 3.6× smaller size over Evolved Transformer with 12,041× less search cost and no performance loss.',\n",
       "  'HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.'],\n",
       " ['Recent work has questioned the importance of the Transformer’s multi-headed attention for achieving high translation quality.',\n",
       "  'We push further in this direction by developing a “hard-coded” attention variant without any learned parameters.',\n",
       "  'Surprisingly, replacing all learned self-attention heads in the encoder and decoder with fixed, input-agnostic Gaussian distributions minimally impacts BLEU scores across four different language pairs.',\n",
       "  'However, additionally, hard-coding cross attention (which connects the decoder to the encoder) significantly lowers BLEU, suggesting that it is more important than self-attention.',\n",
       "  'Much of this BLEU drop can be recovered by adding just a single learned cross attention head to an otherwise hard-coded Transformer.',\n",
       "  'Taken as a whole, our results offer insight into which components of the Transformer are actually important, which we hope will guide future work into the development of simpler and more efficient attention-based models.'],\n",
       " ['Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers.',\n",
       "  'We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning.',\n",
       "  'Word embeddings play an important role in transfer learning, particularly if they are properly aligned.',\n",
       "  'Although transfer learning can be performed without embeddings, results are sub-optimal.',\n",
       "  'In contrast, transferring only the embeddings but nothing else yields catastrophic results.',\n",
       "  'We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains.',\n",
       "  'Finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.'],\n",
       " ['Most data selection research in machine translation focuses on improving a single domain.',\n",
       "  'We perform data selection for multiple domains at once.',\n",
       "  'This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches.',\n",
       "  'Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain.',\n",
       "  'In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.'],\n",
       " ['Training data for NLP tasks often exhibits gender bias in that fewer sentences refer to women than to men.',\n",
       "  'In Neural Machine Translation (NMT) gender bias has been shown to reduce translation quality, particularly when the target language has grammatical gender.',\n",
       "  'The recent WinoMT challenge set allows us to measure this effect directly (Stanovsky et al, 2019) Ideally we would reduce system bias by simply debiasing all data prior to training, but achieving this effectively is itself a challenge.',\n",
       "  'Rather than attempt to create a ‘balanced’ dataset, we use transfer learning on a small set of trusted, gender-balanced examples.',\n",
       "  'This approach gives strong and consistent improvements in gender debiasing with much less computational cost than training from scratch.',\n",
       "  'A known pitfall of transfer learning on new domains is ‘catastrophic forgetting’, which we address at adaptation and inference time.',\n",
       "  'During adaptation we show that Elastic Weight Consolidation allows a performance trade-off between general translation quality and bias reduction.',\n",
       "  'At inference time we propose a lattice-rescoring scheme which outperforms all systems evaluated in Stanovsky et al, 2019 on WinoMT with no degradation of general test set BLEU.',\n",
       "  'We demonstrate our approach translating from English into three languages with varied linguistic properties and data availability.'],\n",
       " ['Machine translation has an undesirable propensity to produce “translationese” artifacts, which can lead to higher BLEU scores while being liked less by human raters.',\n",
       "  'Motivated by this, we model translationese and original (i.e.',\n",
       "  'natural) text as separate languages in a multilingual model, and pose the question: can we perform zero-shot translation between original source text and original target text? There is no data with original source and original target, so we train a sentence-level classifier to distinguish translationese from original target text, and use this classifier to tag the training data for an NMT model.',\n",
       "  'Using this technique we bias the model to produce more natural outputs at test time, yielding gains in human evaluation scores on both accuracy and fluency.',\n",
       "  'Additionally, we demonstrate that it is possible to bias the model to produce translationese and game the BLEU score, increasing it while decreasing human-rated quality.',\n",
       "  'We analyze these outputs using metrics measuring the degree of translationese, and present an analysis of the volatility of heuristic-based train-data tagging.'],\n",
       " ['The notion of “in-domain data” in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality.',\n",
       "  'In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems.',\n",
       "  'We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision – suggesting a simple data-driven definition of domains in textual data.',\n",
       "  'We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data.',\n",
       "  'We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.'],\n",
       " ['We present Neural Machine Translation (NMT) training using document-level metrics with batch-level documents.',\n",
       "  'Previous sequence-objective approaches to NMT training focus exclusively on sentence-level metrics like sentence BLEU which do not correspond to the desired evaluation metric, typically document BLEU.',\n",
       "  'Meanwhile research into document-level NMT training focuses on data or model architecture rather than training procedure.',\n",
       "  'We find that each of these lines of research has a clear space in it for the other, and propose merging them with a scheme that allows a document-level evaluation metric to be used in the NMT training objective.',\n",
       "  'We first sample pseudo-documents from sentence samples.',\n",
       "  'We then approximate the expected document BLEU gradient with Monte Carlo sampling for use as a cost function in Minimum Risk Training (MRT).',\n",
       "  'This two-level sampling procedure gives NMT performance gains over sequence MRT and maximum-likelihood training.',\n",
       "  'We demonstrate that training is more robust for document-level metrics than with sequence metrics.',\n",
       "  'We further demonstrate improvements on NMT with TER and Grammatical Error Correction (GEC) using GLEU, both metrics used at the document level for evaluations.'],\n",
       " ['Variational Neural Machine Translation (VNMT) is an attractive framework for modeling the generation of target translations, conditioned not only on the source sentence but also on some latent random variables.',\n",
       "  'The latent variable modeling may introduce useful statistical dependencies that can improve translation accuracy.',\n",
       "  'Unfortunately, learning informative latent variables is non-trivial, as the latent space can be prohibitively large, and the latent codes are prone to be ignored by many translation models at training time.',\n",
       "  'Previous works impose strong assumptions on the distribution of the latent code and limit the choice of the NMT architecture.',\n",
       "  'In this paper, we propose to apply the VNMT framework to the state-of-the-art Transformer and introduce a more flexible approximate posterior based on normalizing flows.',\n",
       "  'We demonstrate the efficacy of our proposal under both in-domain and out-of-domain conditions, significantly outperforming strong baselines.'],\n",
       " ['This work treats the paradigm discovery problem (PDP), the task of learning an inflectional morphological system from unannotated sentences.',\n",
       "  'We formalize the PDP and develop evaluation metrics for judging systems.',\n",
       "  'Using currently available resources, we construct datasets for the task.',\n",
       "  'We also devise a heuristic benchmark for the PDP and report empirical results on five diverse languages.',\n",
       "  'Our benchmark system first makes use of word embeddings and string similarity to cluster forms by cell and by paradigm.',\n",
       "  'Then, we bootstrap a neural transducer on top of the clustered data to predict words to realize the empty paradigm slots.',\n",
       "  'An error analysis of our system suggests clustering by cell across different inflection classes is the most pressing challenge for future work.'],\n",
       " ['Hindi grapheme-to-phoneme (G2P) conversion is mostly trivial, with one exception: whether a schwa represented in the orthography is pronounced or unpronounced (deleted).',\n",
       "  'Previous work has attempted to predict schwa deletion in a rule-based fashion using prosodic or phonetic analysis.',\n",
       "  'We present the first statistical schwa deletion classifier for Hindi, which relies solely on the orthography as the input and outperforms previous approaches.',\n",
       "  'We trained our model on a newly-compiled pronunciation lexicon extracted from various online dictionaries.',\n",
       "  'Our best Hindi model achieves state of the art performance, and also achieves good performance on a closely related language, Punjabi, without modification.'],\n",
       " ['In this theme paper, we focus on Automated Writing Evaluation (AWE), using Ellis Page’s seminal 1966 paper to frame the presentation.',\n",
       "  'We discuss some of the current frontiers in the field and offer some thoughts on the emergent uses of this technology.'],\n",
       " ['Building on Petroni et al.',\n",
       "  '2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs).',\n",
       "  '(1) Negation.',\n",
       "  'We find that PLMs do not distinguish between negated (‘‘Birds cannot [MASK]”) and non-negated (‘‘Birds can [MASK]”) cloze questions.',\n",
       "  '(2) Mispriming.',\n",
       "  'Inspired by priming methods in human psychology, we add “misprimes” to cloze questions (‘‘Talk? Birds can [MASK]”).',\n",
       "  'We find that PLMs are easily distracted by misprimes.',\n",
       "  'These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.'],\n",
       " ['The field of natural language processing is experiencing a period of unprecedented growth, and with it a surge of published papers.',\n",
       "  'This represents an opportunity for us to take stock of how we cite the work of other researchers, and whether this growth comes at the expense of “forgetting” about older literature.',\n",
       "  'In this paper, we address this question through bibliographic analysis.',\n",
       "  'By looking at the age of outgoing citations in papers published at selected ACL venues between 2010 and 2019, we find that there is indeed a tendency for recent papers to cite more recent work, but the rate at which papers older than 15 years are cited has remained relatively stable.'],\n",
       " ['Most NLP models today treat language as universal, even though socio- and psycholingustic research shows that the communicated message is influenced by the characteristics of the speaker as well as the target audience.',\n",
       "  'This paper surveys the landscape of personalization in natural language processing and related fields, and offers a path forward to mitigate the decades of deviation of the NLP tools from sociolingustic findings, allowing to flexibly process the “natural” language of each user rather than enforcing a uniform NLP treatment.',\n",
       "  'It outlines a possible direction to incorporate these aspects into neural NLP models by means of socially contextual personalization, and proposes to shift the focus of our evaluation strategies accordingly.'],\n",
       " ['Many tasks aim to measure machine reading comprehension (MRC), often focusing on question types presumed to be difficult.',\n",
       "  'Rarely, however, do task designers start by considering what systems should in fact comprehend.',\n",
       "  'In this paper we make two key contributions.',\n",
       "  'First, we argue that existing approaches do not adequately define comprehension; they are too unsystematic about what content is tested.',\n",
       "  'Second, we present a detailed definition of comprehension—a “Template of Understanding”—for a widely useful class of texts, namely short narratives.',\n",
       "  'We then conduct an experiment that strongly suggests existing systems are not up to the task of narrative understanding as we define it.'],\n",
       " ['Disparities in authorship and citations across genders can have substantial adverse consequences not just on the disadvantaged gender, but also on the field of study as a whole.',\n",
       "  'In this work, we examine female first author percentages and the citations to their papers in Natural Language Processing.',\n",
       "  'We find that only about 29% of first authors are female and only about 25% of last authors are female.',\n",
       "  'Notably, this percentage has not improved since the mid 2000s.',\n",
       "  'We also show that, on average, female first authors are cited less than male first authors, even when controlling for experience and area of research.',\n",
       "  'We hope that recording citation and participation gaps across demographic groups will improve awareness of gender gaps and encourage more inclusiveness and fairness in research.'],\n",
       " ['We present BART, a denoising autoencoder for pretraining sequence-to-sequence models.',\n",
       "  'BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.',\n",
       "  'It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes.',\n",
       "  'We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token.',\n",
       "  'BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks.',\n",
       "  'It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE.',\n",
       "  'BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.',\n",
       "  'We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.'],\n",
       " ['Text generation has made significant advances in the last few years.',\n",
       "  'Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment.',\n",
       "  'We propose BLEURT, a learned evaluation metric for English based on BERT.',\n",
       "  'BLEURT can model human judgment with a few thousand possibly biased training examples.',\n",
       "  'A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize.',\n",
       "  'BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set.',\n",
       "  'In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.'],\n",
       " ['Large-scale pre-trained language model such as BERT has achieved great success in language understanding tasks.',\n",
       "  'However, it remains an open question how to utilize BERT for language generation.',\n",
       "  'In this paper, we present a novel approach, Conditional Masked Language Modeling (C-MLM), to enable the finetuning of BERT on target generation tasks.',\n",
       "  'The finetuned BERT (teacher) is exploited as extra supervision to improve conventional Seq2Seq models (student) for better text generation performance.',\n",
       "  'By leveraging BERT’s idiosyncratic bidirectional nature, distilling knowledge learned in BERT can encourage auto-regressive Seq2Seq models to plan ahead, imposing global sequence-level supervision for coherent text generation.',\n",
       "  'Experiments show that the proposed approach significantly outperforms strong Transformer baselines on multiple language generation tasks such as machine translation and text summarization.',\n",
       "  'Our proposed model also achieves new state of the art on IWSLT German-English and English-Vietnamese MT datasets.'],\n",
       " ['Neural networks lack the ability to reason about qualitative physics and so cannot generalize to scenarios and tasks unseen during training.',\n",
       "  'We propose ESPRIT, a framework for commonsense reasoning about qualitative physics in natural language that generates interpretable descriptions of physical events.',\n",
       "  'We use a two-step approach of first identifying the pivotal physical events in an environment and then generating natural language descriptions of those events using a data-to-text approach.',\n",
       "  'Our framework learns to generate explanations of how the physical simulation will causally evolve so that an agent or a human can easily reason about a solution using those interpretable descriptions.',\n",
       "  'Human evaluations indicate that ESPRIT produces crucial fine-grained details and has high coverage of physical concepts compared to even human annotations.',\n",
       "  'Dataset, code and documentation are available at https://github.com/salesforce/esprit.'],\n",
       " ['We present a novel iterative, edit-based approach to unsupervised sentence simplification.',\n",
       "  'Our model is guided by a scoring function involving fluency, simplicity, and meaning preservation.',\n",
       "  'Then, we iteratively perform word and phrase-level edits on the complex sentence.',\n",
       "  'Compared with previous approaches, our model does not require a parallel training set, but is more controllable and interpretable.',\n",
       "  'Experiments on Newsela and WikiLarge datasets show that our approach is nearly as effective as state-of-the-art supervised approaches.'],\n",
       " ['Neural natural language generation (NLG) models have recently shown remarkable progress in fluency and coherence.',\n",
       "  'However, existing studies on neural NLG are primarily focused on surface-level realizations with limited emphasis on logical inference, an important aspect of human thinking and language.',\n",
       "  'In this paper, we suggest a new NLG task where a model is tasked with generating natural language statements that can be logically entailed by the facts in an open-domain semi-structured table.',\n",
       "  'To facilitate the study of the proposed logical NLG problem, we use the existing TabFact dataset~(CITATION) featured with a wide range of logical/symbolic inferences as our testbed, and propose new automatic metrics to evaluate the fidelity of generation models w.r.t.',\n",
       "  'logical inference.',\n",
       "  'The new task poses challenges to the existing monotonic generation frameworks due to the mismatch between sequence order and logical order.',\n",
       "  'In our experiments, we comprehensively survey different generation architectures (LSTM, Transformer, Pre-Trained LM) trained with different algorithms (RL, Adversarial Training, Coarse-to-Fine) on the dataset and made following observations: 1) Pre-Trained LM can significantly boost both the fluency and logical fidelity metrics, 2) RL and Adversarial Training are trading fluency for fidelity, 3) Coarse-to-Fine generation can help partially alleviate the fidelity issue while maintaining high language fluency.',\n",
       "  'The code and data are available at https://github.com/wenhuchen/LogicNLG.'],\n",
       " ['The success of a text simplification system heavily depends on the quality and quantity of complex-simple sentence pairs in the training corpus, which are extracted by aligning sentences between parallel articles.',\n",
       "  'To evaluate and improve sentence alignment quality, we create two manually annotated sentence-aligned datasets from two commonly used text simplification corpora, Newsela and Wikipedia.',\n",
       "  'We propose a novel neural CRF alignment model which not only leverages the sequential nature of sentences in parallel documents but also utilizes a neural sentence pair model to capture semantic similarity.',\n",
       "  'Experiments demonstrate that our proposed approach outperforms all the previous work on monolingual sentence alignment task by more than 5 points in F1.',\n",
       "  'We apply our CRF aligner to construct two new text simplification datasets, Newsela-Auto and Wiki-Auto, which are much larger and of better quality compared to the existing datasets.',\n",
       "  'A Transformer-based seq2seq model trained on our datasets establishes a new state-of-the-art for text simplification in both automatic and human evaluation.'],\n",
       " ['Different texts shall by nature correspond to different number of keyphrases.',\n",
       "  'This desideratum is largely missing from existing neural keyphrase generation models.',\n",
       "  'In this study, we address this problem from both modeling and evaluation perspectives.',\n",
       "  'We first propose a recurrent generative model that generates multiple keyphrases as delimiter-separated sequences.',\n",
       "  'Generation diversity is further enhanced with two novel techniques by manipulating decoder hidden states.',\n",
       "  'In contrast to previous approaches, our model is capable of generating diverse keyphrases and controlling number of outputs.',\n",
       "  'We further propose two evaluation metrics tailored towards the variable-number generation.',\n",
       "  'We also introduce a new dataset StackEx that expands beyond the only existing genre (i.e., academic writing) in keyphrase generation tasks.',\n",
       "  'With both previous and new evaluation metrics, our model outperforms strong baselines on all datasets.'],\n",
       " ['We propose an unsupervised approach for sarcasm generation based on a non-sarcastic input sentence.',\n",
       "  'Our method employs a retrieve-and-edit framework to instantiate two major characteristics of sarcasm: reversal of valence and semantic incongruity with the context, which could include shared commonsense or world knowledge between the speaker and the listener.',\n",
       "  'While prior works on sarcasm generation predominantly focus on context incongruity, we show that combining valence reversal and semantic incongruity based on the commonsense knowledge generates sarcasm of higher quality.',\n",
       "  'Human evaluation shows that our system generates sarcasm better than humans 34% of the time, and better than a reinforced hybrid baseline 90% of the time.'],\n",
       " ['The task of graph-to-text generation aims at producing sentences that preserve the meaning of input graphs.',\n",
       "  'As a crucial defect, the current state-of-the-art models may mess up or even drop the core structural information of input graphs when generating outputs.',\n",
       "  'We propose to tackle this problem by leveraging richer training signals that can guide our model for preserving input information.',\n",
       "  'In particular, we introduce two types of autoencoding losses, each individually focusing on different aspects (a.k.a.',\n",
       "  'views) of input graphs.',\n",
       "  'The losses are then back-propagated to better calibrate our model via multi-task training.',\n",
       "  'Experiments on two benchmarks for graph-to-text generation show the effectiveness of our approach over a state-of-the-art baseline.'],\n",
       " ['Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions.',\n",
       "  'For example, a victim of a die event is likely to be a victim of an attack event in the same sentence.',\n",
       "  'In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence.',\n",
       "  'OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder.',\n",
       "  'At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions.',\n",
       "  'Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks.',\n",
       "  'In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.'],\n",
       " ['Few works in the literature of event extraction have gone beyond individual sentences to make extraction decisions.',\n",
       "  'This is problematic when the information needed to recognize an event argument is spread across multiple sentences.',\n",
       "  'We argue that document-level event extraction is a difficult task since it requires a view of a larger context to determine which spans of text correspond to event role fillers.',\n",
       "  'We first investigate how end-to-end neural sequence models (with pre-trained language model representations) perform on document-level role filler extraction, as well as how the length of context captured affects the models’ performance.',\n",
       "  'To dynamically aggregate information captured by neural representations learned at different levels of granularity (e.g., the sentence- and paragraph-level), we propose a novel multi-granularity reader.',\n",
       "  'We evaluate our models on the MUC-4 event extraction dataset, and show that our best system performs substantially better than prior work.',\n",
       "  'We also report findings on the relationship between context length and neural model performance on the task.'],\n",
       " ['This paper studies the task of Relation Extraction (RE) that aims to identify the semantic relations between two entity mentions in text.',\n",
       "  'In the deep learning models for RE, it has been beneficial to incorporate the syntactic structures from the dependency trees of the input sentences.',\n",
       "  'In such models, the dependency trees are often used to directly structure the network architectures or to obtain the dependency relations between the word pairs to inject the syntactic information into the models via multi-task learning.',\n",
       "  'The major problem with these approaches is the lack of generalization beyond the syntactic structures in the training data or the failure to capture the syntactic importance of the words for RE.',\n",
       "  'In order to overcome these issues, we propose a novel deep learning model for RE that uses the dependency trees to extract the syntax-based importance scores for the words, serving as a tree representation to introduce syntactic information into the models with greater generalization.',\n",
       "  'In particular, we leverage Ordered-Neuron Long-Short Term Memory Networks (ON-LSTM) to infer the model-based importance scores for RE for every word in the sentences that are then regulated to be consistent with the syntax-based scores to enable syntactic information injection.',\n",
       "  'We perform extensive experiments to demonstrate the effectiveness of the proposed method, leading to the state-of-the-art performance on three RE benchmark datasets.'],\n",
       " ['Linguistic Code-switching (CS) is still an understudied phenomenon in natural language processing.',\n",
       "  'The NLP community has mostly focused on monolingual and multi-lingual scenarios, but little attention has been given to CS in particular.',\n",
       "  'This is partly because of the lack of resources and annotated data, despite its increasing occurrence in social media platforms.',\n",
       "  'In this paper, we aim at adapting monolingual models to code-switched text in various tasks.',\n",
       "  'Specifically, we transfer English knowledge from a pre-trained ELMo model to different code-switched language pairs (i.e., Nepali-English, Spanish-English, and Hindi-English) using the task of language identification.',\n",
       "  'Our method, CS-ELMo, is an extension of ELMo with a simple yet effective position-aware attention mechanism inside its character convolutions.',\n",
       "  'We show the effectiveness of this transfer learning step by outperforming multilingual BERT and homologous CS-unaware ELMo models and establishing a new state of the art in CS tasks, such as NER and POS tagging.',\n",
       "  'Our technique can be expanded to more English-paired code-switched languages, providing more resources to the CS community.'],\n",
       " ['Concept graphs are created as universal taxonomies for text understanding in the open-domain knowledge.',\n",
       "  'The nodes in concept graphs include both entities and concepts.',\n",
       "  'The edges are from entities to concepts, showing that an entity is an instance of a concept.',\n",
       "  'In this paper, we propose the task of learning interpretable relationships from open-domain facts to enrich and refine concept graphs.',\n",
       "  'The Bayesian network structures are learned from open-domain facts as the interpretable relationships between relations of facts and concepts of entities.',\n",
       "  'We conduct extensive experiments on public English and Chinese datasets.',\n",
       "  'Compared to the state-of-the-art methods, the learned network structures help improving the identification of concepts for entities based on the relations of entities on both datasets.'],\n",
       " ['We present a novel document-level model for finding argument spans that fill an event’s roles, connecting related ideas in sentence-level semantic role labeling and coreference resolution.',\n",
       "  'Because existing datasets for cross-sentence linking are small, development of our neural model is supported through the creation of a new resource, Roles Across Multiple Sentences (RAMS), which contains 9,124 annotated events across 139 types.',\n",
       "  'We demonstrate strong performance of our model on RAMS and other event-related datasets.'],\n",
       " ['Nowadays, the interpretability of machine learning models is becoming increasingly important, especially in the medical domain.',\n",
       "  'Aiming to shed some light on how to rationalize medical relation prediction, we present a new interpretable framework inspired by existing theories on how human memory works, e.g., theories of recall and recognition.',\n",
       "  'Given the corpus-level statistics, i.e., a global co-occurrence graph of a clinical text corpus, to predict the relations between two entities, we first recall rich contexts associated with the target entities, and then recognize relational interactions between these contexts to form model rationales, which will contribute to the final prediction.',\n",
       "  'We conduct experiments on a real-world public clinical dataset and show that our framework can not only achieve competitive predictive performance against a comprehensive list of neural baseline models, but also present rationales to justify its prediction.',\n",
       "  'We further collaborate with medical experts deeply to verify the usefulness of our model rationales for clinical decision making.'],\n",
       " ['Named-entities are inherently multilingual, and annotations in any given language may be limited.',\n",
       "  'This motivates us to consider polyglot named-entity recognition (NER), where one model is trained using annotated data drawn from more than one language.',\n",
       "  'However, a straightforward implementation of this simple idea does not always work in practice: naive training of NER models using annotated data drawn from multiple languages consistently underperforms models trained on monolingual data alone, despite having access to more training data.',\n",
       "  'The starting point of this paper is a simple solution to this problem, in which polyglot models are fine-tuned on monolingual data to consistently and significantly outperform their monolingual counterparts.',\n",
       "  'To explain this phenomena, we explore the sources of multilingual transfer in polyglot NER models and examine the weight structure of polyglot models compared to their monolingual counterparts.',\n",
       "  'We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters.'],\n",
       " ['In many documents, such as semi-structured webpages, textual semantics are augmented with additional information conveyed using visual elements including layout, font size, and color.',\n",
       "  'Prior work on information extraction from semi-structured websites has required learning an extraction model specific to a given template via either manually labeled or distantly supervised data from that template.',\n",
       "  'In this work, we propose a solution for “zero-shot” open-domain relation extraction from webpages with a previously unseen template, including from websites with little overlap with existing sources of knowledge for distant supervision and websites in entirely new subject verticals.',\n",
       "  'Our model uses a graph neural network-based approach to build a rich representation of text fields on a webpage and the relationships between them, enabling generalization to new templates.',\n",
       "  'Experiments show this approach provides a 31% F1 gain over a baseline for zero-shot extraction in a new subject vertical.'],\n",
       " ['Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance.',\n",
       "  'Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data.',\n",
       "  'However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages.',\n",
       "  'To address this problem, we propose a method of “soft gazetteers” that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking.',\n",
       "  'Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.'],\n",
       " ['We reframe suicide risk assessment from social media as a ranking problem whose goal is maximizing detection of severely at-risk individuals given the time available.',\n",
       "  'Building on measures developed for resource-bounded document retrieval, we introduce a well founded evaluation paradigm, and demonstrate using an expert-annotated test collection that meaningful improvements over plausible cascade model baselines can be achieved using an approach that jointly ranks individuals and their social media posts.'],\n",
       " ['Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration.',\n",
       "  'Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the “ideal” number of topics and depth of the hierarchy.',\n",
       "  'In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM.',\n",
       "  'CluHTM’s novel contributions include: (i) the exploration of richer text representation that encapsulates both, global (dataset level) and local semantic information – when combined, these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure; (ii) the exploitation of a stability analysis metric for defining the number of topics and the “shape” the hierarchical structure.',\n",
       "  'In our evaluation, considering twelve datasets and seven state-of-the-art baselines, CluHTM outperformed the baselines in the vast majority of the cases, with gains of around 500% over the strongest state-of-the-art baselines.',\n",
       "  'We also provide qualitative and quantitative statistical analyses of why our solution works so well.'],\n",
       " ['Entity set expansion, aiming at expanding a small seed entity set with new entities belonging to the same semantic class, is a critical task that benefits many downstream NLP and IR applications, such as question answering, query understanding, and taxonomy construction.',\n",
       "  'Existing set expansion methods bootstrap the seed entity set by adaptively selecting context features and extracting new entities.',\n",
       "  'A key challenge for entity set expansion is to avoid selecting ambiguous context features which will shift the class semantics and lead to accumulative errors in later iterations.',\n",
       "  'In this study, we propose a novel iterative set expansion framework that leverages automatically generated class names to address the semantic drift issue.',\n",
       "  'In each iteration, we select one positive and several negative class names by probing a pre-trained language model, and further score each candidate entity based on selected class names.',\n",
       "  'Experiments on two datasets show that our framework generates high-quality class names and outperforms previous state-of-the-art methods significantly.'],\n",
       " ['In classification, there are usually some good features that are indicative of class labels.',\n",
       "  'For example, in sentiment classification, words like good and nice are indicative of the positive sentiment and words like bad and terrible are indicative of the negative sentiment.',\n",
       "  'However, there are also many common features (e.g., words) that are not indicative of any specific class (e.g., voice and screen, which are common to both sentiment classes and are not discriminative for classification).',\n",
       "  'Although deep learning has made significant progresses in generating discriminative features through its powerful representation learning, we believe there is still room for improvement.',\n",
       "  'In this paper, we propose a novel angle to further improve this representation learning, i.e., feature projection.',\n",
       "  'This method projects existing features into the orthogonal space of the common features.',\n",
       "  'The resulting projection is thus perpendicular to the common features and more discriminative for classification.',\n",
       "  'We apply this new method to improve CNN, RNN, Transformer, and Bert based text classification and obtain markedly better results.'],\n",
       " ['Existing Visual Question Answering (VQA) methods tend to exploit dataset biases and spurious statistical correlations, instead of producing right answers for the right reasons.',\n",
       "  'To address this issue, recent bias mitigation methods for VQA propose to incorporate visual cues (e.g., human attention maps) to better ground the VQA models, showcasing impressive gains.',\n",
       "  'However, we show that the performance improvements are not a result of improved visual grounding, but a regularization effect which prevents over-fitting to linguistic priors.',\n",
       "  'For instance, we find that it is not actually necessary to provide proper, human-based cues; random, insensible cues also result in similar improvements.',\n",
       "  'Based on this observation, we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state-of-the-art performance on VQA-CPv2.'],\n",
       " ['Visual Dialogue involves “understanding” the dialogue history (what has been discussed previously) and the current question (what is asked), in addition to grounding information in the image, to accurately generate the correct response.',\n",
       "  'In this paper, we show that co-attention models which explicitly encode dialoh history outperform models that don’t, achieving state-of-the-art performance (72 % NDCG on val set).',\n",
       "  'However, we also expose shortcomings of the crowdsourcing dataset collection procedure, by showing that dialogue history is indeed only required for a small amount of the data, and that the current evaluation metric encourages generic replies.',\n",
       "  'To that end, we propose a challenging subset (VisdialConv) of the VisdialVal set and the benchmark NDCG of 63%.'],\n",
       " ['We present a new problem: grounding natural language instructions to mobile user interface actions, and create three new datasets for it.',\n",
       "  'For full task evaluation, we create PixelHelp, a corpus that pairs English instructions with actions performed by people on a mobile UI emulator.',\n",
       "  'To scale training, we decouple the language and action data by (a) annotating action phrase spans in How-To instructions and (b) synthesizing grounded descriptions of actions for mobile user interfaces.',\n",
       "  'We use a Transformer to extract action phrase tuples from long-range natural language instructions.',\n",
       "  'A grounding Transformer then contextually represents UI objects using both their content and screen position and connects them to object descriptions.',\n",
       "  'Given a starting screen and instruction, our model achieves 70.59% accuracy on predicting complete ground-truth action sequences in PixelHelp.'],\n",
       " ['We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos.',\n",
       "  'We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers.',\n",
       "  'We name this augmented version as TVQA+.',\n",
       "  'We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos.',\n",
       "  'Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task.',\n",
       "  'Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations.'],\n",
       " ['Unsupervised machine translation (MT) has recently achieved impressive results with monolingual corpora only.',\n",
       "  'However, it is still challenging to associate source-target sentences in the latent space.',\n",
       "  'As people speak different languages biologically share similar visual systems, the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT (MMT).',\n",
       "  'In this paper, we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT.',\n",
       "  'Our model employs multimodal back-translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visually-pivoted captioning as additional weak supervision.',\n",
       "  'The experimental results on the widely used Multi30K dataset show that the proposed model significantly improves over the state-of-the-art methods and generalizes well when images are not available at the testing time.'],\n",
       " ['In many languages like Arabic, diacritics are used to specify pronunciations as well as meanings.',\n",
       "  'Such diacritics are often omitted in written text, increasing the number of possible pronunciations and meanings for a word.',\n",
       "  'This results in a more ambiguous text making computational processing on such text more difficult.',\n",
       "  'Diacritic restoration is the task of restoring missing diacritics in the written text.',\n",
       "  'Most state-of-the-art diacritic restoration models are built on character level information which helps generalize the model to unseen data, but presumably lose useful information at the word level.',\n",
       "  'Thus, to compensate for this loss, we investigate the use of multi-task learning to jointly optimize diacritic restoration with related NLP problems namely word segmentation, part-of-speech tagging, and syntactic diacritization.',\n",
       "  'We use Arabic as a case study since it has sufficient data resources for tasks that we consider in our joint modeling.',\n",
       "  'Our joint models significantly outperform the baselines and are comparable to the state-of-the-art models that are more complex relying on morphological analyzers and/or a lot more data (e.g.',\n",
       "  'dialectal data).'],\n",
       " ['Lexica distinguishing all morphologically related forms of each lexeme are crucial to many language technologies, yet building them is expensive.',\n",
       "  'We propose a frugal paradigm completion approach that predicts all related forms in a morphological paradigm from as few manually provided forms as possible.',\n",
       "  'It induces typological information during training which it uses to determine the best sources at test time.',\n",
       "  'We evaluate our language-agnostic approach on 7 diverse languages.',\n",
       "  'Compared to popular alternative approaches, ours reduces manual labor by 16-63% and is the most robust to typological variation.'],\n",
       " ['Contextual features always play an important role in Chinese word segmentation (CWS).',\n",
       "  'Wordhood information, being one of the contextual features, is proved to be useful in many conventional character-based segmenters.',\n",
       "  'However, this feature receives less attention in recent neural models and it is also challenging to design a framework that can properly integrate wordhood information from different wordhood measures to existing neural frameworks.',\n",
       "  'In this paper, we therefore propose a neural framework, WMSeg, which uses memory networks to incorporate wordhood information with several popular encoder-decoder combinations for CWS.',\n",
       "  'Experimental results on five benchmark datasets indicate the memory mechanism successfully models wordhood information for neural segmenters and helps WMSeg achieve state-of-the-art performance on all those datasets.',\n",
       "  'Further experiments and analyses also demonstrate the robustness of our proposed framework with respect to different wordhood measures and the efficiency of wordhood information in cross-domain experiments.'],\n",
       " ['Chinese word segmentation (CWS) and part-of-speech (POS) tagging are important fundamental tasks for Chinese language processing, where joint learning of them is an effective one-step solution for both tasks.',\n",
       "  'Previous studies for joint CWS and POS tagging mainly follow the character-based tagging paradigm with introducing contextual information such as n-gram features or sentential representations from recurrent neural models.',\n",
       "  'However, for many cases, the joint tagging needs not only modeling from context features but also knowledge attached to them (e.g., syntactic relations among words); limited efforts have been made by existing research to meet such needs.',\n",
       "  'In this paper, we propose a neural model named TwASP for joint CWS and POS tagging following the character-based sequence labeling paradigm, where a two-way attention mechanism is used to incorporate both context feature and their corresponding syntactic knowledge for each input character.',\n",
       "  'Particularly, we use existing language processing toolkits to obtain the auto-analyzed syntactic knowledge for the context, and the proposed attention module can learn and benefit from them although their quality may not be perfect.',\n",
       "  'Our experiments illustrate the effectiveness of the two-way attentions for joint CWS and POS tagging, where state-of-the-art performance is achieved on five benchmark datasets.'],\n",
       " ['The written forms of Semitic languages are both highly ambiguous and morphologically rich: a word can have multiple interpretations and is one of many inflected forms of the same concept or lemma.',\n",
       "  'This is further exacerbated for dialectal content, which is more prone to noise and lacks a standard orthography.',\n",
       "  'The morphological features can be lexicalized, like lemmas and diacritized forms, or non-lexicalized, like gender, number, and part-of-speech tags, among others.',\n",
       "  'Joint modeling of the lexicalized and non-lexicalized features can identify more intricate morphological patterns, which provide better context modeling, and further disambiguate ambiguous lexical choices.',\n",
       "  'However, the different modeling granularity can make joint modeling more difficult.',\n",
       "  'Our approach models the different features jointly, whether lexicalized (on the character-level), or non-lexicalized (on the word-level).',\n",
       "  'We use Arabic as a test case, and achieve state-of-the-art results for Modern Standard Arabic with 20% relative error reduction, and Egyptian Arabic with 11% relative error reduction.'],\n",
       " ['Informal romanization is an idiosyncratic process used by humans in informal digital communication to encode non-Latin script languages into Latin character sets found on common keyboards.',\n",
       "  'Character substitution choices differ between users but have been shown to be governed by the same main principles observed across a variety of languages—namely, character pairs are often associated through phonetic or visual similarity.',\n",
       "  'We propose a noisy-channel WFST cascade model for deciphering the original non-Latin script from observed romanized text in an unsupervised fashion.',\n",
       "  'We train our model directly on romanized data from two languages: Egyptian Arabic and Russian.',\n",
       "  'We demonstrate that adding inductive bias through phonetic and visual priors on character mappings substantially improves the model’s performance on both languages, yielding results much closer to the supervised skyline.',\n",
       "  'Finally, we introduce a new dataset of romanized Russian, collected from a Russian social network website and partially annotated for our experiments.'],\n",
       " ['We improve upon pairwise annotation for active learning in coreference resolution, by asking annotators to identify mention antecedents if a presented mention pair is deemed not coreferent.',\n",
       "  'This simple modification, when combined with a novel mention clustering algorithm for selecting which examples to label, is much more efficient in terms of the performance obtained per annotation budget.',\n",
       "  'In experiments with existing benchmark coreference datasets, we show that the signal from this additional question leads to significant performance gains per human-annotation hour.',\n",
       "  'Future work can use our annotation protocol to effectively develop coreference models for new domains.',\n",
       "  'Our code is publicly available.'],\n",
       " ['This paper introduces two tasks: determining (a) the duration of possession relations and (b) co-possessions, i.e., whether multiple possessors possess a possessee at the same time.',\n",
       "  'We present new annotations on top of corpora annotating possession existence and experimental results.',\n",
       "  'Regarding possession duration, we derive the time spans we work with empirically from annotations indicating lower and upper bounds.',\n",
       "  'Regarding co-possessions, we use a binary label.',\n",
       "  'Cohen’s kappa coefficients indicate substantial agreement, and experimental results show that text is more useful than the image for solving these tasks.'],\n",
       " ['Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP.',\n",
       "  'In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task.',\n",
       "  'We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings.',\n",
       "  'Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining.',\n",
       "  'Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable.',\n",
       "  'Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.'],\n",
       " ['Word embedding-based similarity measures are currently among the top-performing methods on unsupervised semantic textual similarity (STS) tasks.',\n",
       "  'Recent work has increasingly adopted a statistical view on these embeddings, with some of the top approaches being essentially various correlations (which include the famous cosine similarity).',\n",
       "  'Another excellent candidate for a similarity measure is mutual information (MI), which can capture arbitrary dependencies between the variables and has a simple and intuitive expression.',\n",
       "  'Unfortunately, its use in the context of dense word embeddings has so far been avoided due to difficulties with estimating MI for continuous data.',\n",
       "  'In this work we go through a vast literature on estimating MI in such cases and single out the most promising methods, yielding a simple and elegant similarity measure for word embeddings.',\n",
       "  'We show that mutual information is a viable alternative to correlations, gives an excellent signal that correlates well with human judgements of similarity and rivals existing state-of-the-art unsupervised methods.'],\n",
       " ['We study the task of cross-database semantic parsing (XSP), where a system that maps natural language utterances to executable SQL queries is evaluated on databases unseen during training.',\n",
       "  'Recently, several datasets, including Spider, were proposed to support development of XSP systems.',\n",
       "  'We propose a challenging evaluation setup for cross-database semantic parsing, focusing on variation across database schemas and in-domain language use.',\n",
       "  'We re-purpose eight semantic parsing datasets that have been well-studied in the setting where in-domain training data is available, and instead use them as additional evaluation data for XSP systems instead.',\n",
       "  'We build a system that performs well on Spider, and find that it struggles to generalize to our re-purposed set.',\n",
       "  'Our setup uncovers several generalization challenges for cross-database semantic parsing, demonstrating the need to use and develop diverse training and evaluation datasets.'],\n",
       " ['The focus of a negation is the set of tokens intended to be negated, and a key component for revealing affirmative alternatives to negated utterances.',\n",
       "  'In this paper, we experiment with neural networks to predict the focus of negation.',\n",
       "  'Our main novelty is leveraging a scope detector to introduce the scope of negation as an additional input to the network.',\n",
       "  'Experimental results show that doing so obtains the best results to date.',\n",
       "  'Additionally, we perform a detailed error analysis providing insights into the main error categories, and analyze errors depending on whether the model takes into account scope and context information.'],\n",
       " ['Recent neural network-driven semantic role labeling (SRL) systems have shown impressive improvements in F1 scores.',\n",
       "  'These improvements are due to expressive input representations, which, at least at the surface, are orthogonal to knowledge-rich constrained decoding mechanisms that helped linear SRL models.',\n",
       "  'Introducing the benefits of structure to inform neural models presents a methodological challenge.',\n",
       "  'In this paper, we present a structured tuning framework to improve models using softened constraints only at training time.',\n",
       "  'Our framework leverages the expressiveness of neural networks and provides supervision with structured loss components.',\n",
       "  'We start with a strong baseline (RoBERTa) to validate the impact of our approach, and show that our framework outperforms the baseline by learning to comply with declarative constraints.',\n",
       "  'Additionally, our experiments with smaller training sizes show that we can achieve consistent improvements under low-resource scenarios.'],\n",
       " ['Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks.',\n",
       "  'Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables).',\n",
       "  'In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables.',\n",
       "  'TaBERT is trained on a large corpus of 26 million tables and their English contexts.',\n",
       "  'In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider.'],\n",
       " ['We introduce a transductive model for parsing into Universal Decompositional Semantics (UDS) representations, which jointly learns to map natural language utterances into UDS graph structures and annotate the graph with decompositional semantic attribute scores.',\n",
       "  'We also introduce a strong pipeline model for parsing into the UDS graph structure, and show that our transductive parser performs comparably while additionally performing attribute prediction.',\n",
       "  'By analyzing the attribute prediction errors, we find the model captures natural relationships between attribute groups.'],\n",
       " ['This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks.',\n",
       "  'We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data.',\n",
       "  'Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER.',\n",
       "  'XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.',\n",
       "  'We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale.',\n",
       "  'Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks.',\n",
       "  'We will make our code and models publicly available.'],\n",
       " ['Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is challenging because ontologies are large.',\n",
       "  'In most cases, annotated datasets cover only a small sample of the concepts, yet concept normalizers are expected to predict all concepts in the ontology.',\n",
       "  'In this paper, we propose an architecture consisting of a candidate generator and a list-wise ranker based on BERT.',\n",
       "  'The ranker considers pairings of concept mentions and candidate concepts, allowing it to make predictions for any concept, not just those seen during training.',\n",
       "  'We further enhance this list-wise approach with a semantic type regularizer that allows the model to incorporate semantic type information from the ontology during training.',\n",
       "  'Our proposed concept normalization framework achieves state-of-the-art performance on multiple datasets.'],\n",
       " ['We propose a novel method for hierarchical entity classification that embraces ontological structure at both training and during prediction.',\n",
       "  'At training, our novel multi-level learning-to-rank loss compares positive types against negative siblings according to the type tree.',\n",
       "  'During prediction, we define a coarse-to-fine decoder that restricts viable candidates at each level of the ontology based on already predicted parent type(s).',\n",
       "  'Our approach significantly outperform prior work on strict accuracy, demonstrating the effectiveness of our method.'],\n",
       " ['Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input.',\n",
       "  'However, domain transfer of NER models with data from multiple genres has not been widely studied.',\n",
       "  'To this end, we conduct NER experiments in three predictive setups on data from: a) multiple domains; b) multiple domains where the genre label is unknown at inference time; c) domains not encountered in training.',\n",
       "  'We introduce a new architecture tailored to this task by using shared and private domain parameters and multi-task learning.',\n",
       "  'This consistently outperforms all other baseline and competitive methods on all three experimental setups, with differences ranging between +1.95 to +3.11 average F1 across multiple genres when compared to standard approaches.',\n",
       "  'These results illustrate the challenges that need to be taken into account when building real-world NLP applications that are robust to various types of text and the methods that can help, at least partially, alleviate these issues.'],\n",
       " ['Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce.',\n",
       "  'State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories.',\n",
       "  'This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy.',\n",
       "  'Through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts category-specific attribute values.',\n",
       "  'Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10% in F1 and 15% in coverage across all categories.'],\n",
       " ['Training neural models for named entity recognition (NER) in a new domain often requires additional human annotations (e.g., tens of thousands of labeled instances) that are usually expensive and time-consuming to collect.',\n",
       "  'Thus, a crucial research question is how to obtain supervision in a cost-effective way.',\n",
       "  'In this paper, we introduce “entity triggers,” an effective proxy of human explanations for facilitating label-efficient learning of NER models.',\n",
       "  'An entity trigger is defined as a group of words in a sentence that helps to explain why humans would recognize an entity in the sentence.',\n",
       "  'We crowd-sourced 14k entity triggers for two well-studied NER datasets.',\n",
       "  'Our proposed model, Trigger Matching Network, jointly learns trigger representations and soft matching module with self-attention such that can generalize to unseen sentences easily for tagging.',\n",
       "  'Our framework is significantly more cost-effective than the traditional neural NER frameworks.',\n",
       "  'Experiments show that using only 20% of the trigger-annotated sentences results in a comparable performance as using 70% of conventional annotated sentences.'],\n",
       " ['This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs).',\n",
       "  'It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings.',\n",
       "  'Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task.',\n",
       "  'As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro↔En and De↔En.',\n",
       "  'With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU).'],\n",
       " ['When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others.',\n",
       "  'Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance.',\n",
       "  'In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages.',\n",
       "  'Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.'],\n",
       " ['Neural Machine Translation (NMT) models are sensitive to small perturbations in the input.',\n",
       "  'Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input.',\n",
       "  'This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input.',\n",
       "  'We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed.',\n",
       "  'Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.'],\n",
       " ['Web-crawled data provides a good source of parallel corpora for training machine translation models.',\n",
       "  'It is automatically obtained, but extremely noisy, and recent work shows that neural machine translation systems are more sensitive to noise than traditional statistical machine translation methods.',\n",
       "  'In this paper, we propose a novel approach to filter out noisy sentence pairs from web-crawled corpora via pre-trained language models.',\n",
       "  'We measure sentence parallelism by leveraging the multilingual capability of BERT and use the Generative Pre-training (GPT) language model as a domain filter to balance data domains.',\n",
       "  'We evaluate the proposed method on the WMT 2018 Parallel Corpus Filtering shared task, and on our own web-crawled Japanese-Chinese parallel corpus.',\n",
       "  'Our method significantly outperforms baselines and achieves a new state-of-the-art.',\n",
       "  'In an unsupervised setting, our method achieves comparable performance to the top-1 supervised method.',\n",
       "  'We also evaluate on a web-crawled Japanese-Chinese parallel corpus that we make publicly available.'],\n",
       " ['Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT).',\n",
       "  'However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN.',\n",
       "  'This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer.',\n",
       "  'In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information.',\n",
       "  'Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline.'],\n",
       " ['The ability to match pieces of code to their corresponding natural language descriptions and vice versa is fundamental for natural language search interfaces to software repositories.',\n",
       "  'In this paper, we propose a novel multi-perspective cross-lingual neural framework for code–text matching, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities.',\n",
       "  'Our experiments on the CoNaLa dataset show that our proposed model yields better performance on this cross-lingual text-to-code matching task than previous approaches that map code and text to a single joint embedding space.'],\n",
       " ['While automated essay scoring (AES) can reliably grade essays at scale, automated writing evaluation (AWE) additionally provides formative feedback to guide essay revision.',\n",
       "  'However, a neural AES typically does not provide useful feature representations for supporting AWE.',\n",
       "  'This paper presents a method for linking AWE and neural AES, by extracting Topical Components (TCs) representing evidence from a source text using the intermediate output of attention layers.',\n",
       "  'We evaluate performance using a feature-based AES requiring TCs.',\n",
       "  'Results show that performance is comparable whether using automatically or manually constructed TCs for 1) representing essays as rubric-based features, 2) grading essays.'],\n",
       " ['In traditional approaches to entity linking, linking decisions are based on three sources of information – the similarity of the mention string to an entity’s name, the similarity of the context of the document to the entity, and broader information about the knowledge base (KB).',\n",
       "  'In some domains, there is little contextual information present in the KB and thus we rely more heavily on mention string similarity.',\n",
       "  'We consider one example of this, concept linking, which seeks to link mentions of medical concepts to a medical concept ontology.',\n",
       "  'We propose an approach to concept linking that leverages recent work in contextualized neural models, such as ELMo (Peters et al.',\n",
       "  '2018), which create a token representation that integrates the surrounding context of the mention and concept name.',\n",
       "  'We find a neural ranking approach paired with contextualized embeddings provides gains over a competitive baseline (Leaman et al.',\n",
       "  '2013).',\n",
       "  'Additionally, we find that a pre-training step using synonyms from the ontology offers a useful initialization for the ranker.'],\n",
       " ['The increased focus on misinformation has spurred development of data and systems for detecting the veracity of a claim as well as retrieving authoritative evidence.',\n",
       "  'The Fact Extraction and VERification (FEVER) dataset provides such a resource for evaluating endto- end fact-checking, requiring retrieval of evidence from Wikipedia to validate a veracity prediction.',\n",
       "  'We show that current systems for FEVER are vulnerable to three categories of realistic challenges for fact-checking – multiple propositions, temporal reasoning, and ambiguity and lexical variation – and introduce a resource with these types of claims.',\n",
       "  'Then we present a system designed to be resilient to these “attacks” using multiple pointer networks for document selection and jointly modeling a sequence of evidence sentences and veracity relation predictions.',\n",
       "  'We find that in handling these attacks we obtain state-of-the-art results on FEVER, largely due to improved evidence retrieval.'],\n",
       " ['In this paper, we aim to learn associations between visual attributes of fonts and the verbal context of the texts they are typically applied to.',\n",
       "  'Compared to related work leveraging the surrounding visual context, we choose to focus only on the input text, which can enable new applications for which the text is the only visual element in the document.',\n",
       "  'We introduce a new dataset, containing examples of different topics in social media posts and ads, labeled through crowd-sourcing.',\n",
       "  'Due to the subjective nature of the task, multiple fonts might be perceived as acceptable for an input text, which makes this problem challenging.',\n",
       "  'To this end, we investigate different end-to-end models to learn label distributions on crowd-sourced data, to capture inter-subjectivity across all annotations.'],\n",
       " ['News framing refers to the practice in which aspects of specific issues are highlighted in the news to promote a particular interpretation.',\n",
       "  'In NLP, although recent works have studied framing in English news, few have studied how the analysis can be extended to other languages and in a multi-label setting.',\n",
       "  'In this work, we explore multilingual transfer learning to detect multiple frames from just the news headline in a genuinely low-resource context where there are few/no frame annotations in the target language.',\n",
       "  'We propose a novel method that can leverage elementary resources consisting of a dictionary and few annotations to detect frames in the target language.',\n",
       "  'Our method performs comparably or better than translating the entire target language headline to the source language for which we have annotated data.',\n",
       "  'This work opens up an exciting new capability of scaling up frame analysis to many languages, even those without existing translation technologies.',\n",
       "  'Lastly, we apply our method to detect frames on the issue of U.S.',\n",
       "  'gun violence in multiple languages and obtain exciting insights on the relationship between different frames of the same problem across different countries with different languages.'],\n",
       " ['Given the complexity of combinations of tasks, languages, and domains in natural language processing (NLP) research, it is computationally prohibitive to exhaustively test newly proposed models on each possible experimental setting.',\n",
       "  'In this work, we attempt to explore the possibility of gaining plausible judgments of how well an NLP model can perform under an experimental setting, without actually training or testing the model.',\n",
       "  'To do so, we build regression models to predict the evaluation score of an NLP experiment given the experimental settings as input.',\n",
       "  'Experimenting on~9 different NLP tasks, we find that our predictors can produce meaningful predictions over unseen languages and different modeling architectures, outperforming reasonable baselines as well as human experts.',\n",
       "  '%we represent experimental settings using an array of features.',\n",
       "  'Going further, we outline how our predictor can be used to find a small subset of representative experiments that should be run in order to obtain plausible predictions for all other experimental settings.'],\n",
       " ['It is appealing to have a system that generates a story or scripts automatically from a storyline, even though this is still out of our reach.',\n",
       "  'In dialogue systems, it would also be useful to drive dialogues by a dialogue plan.',\n",
       "  'In this paper, we address a key problem involved in these applications - guiding a dialogue by a narrative.',\n",
       "  'The proposed model ScriptWriter selects the best response among the candidates that fit the context as well as the given narrative.',\n",
       "  'It keeps track of what in the narrative has been said and what is to be said.',\n",
       "  'A narrative plays a different role than the context (i.e., previous utterances), which is generally used in current dialogue systems.',\n",
       "  'Due to the unavailability of data for this new application, we construct a new large-scale data collection GraphMovie from a movie website where end- users can upload their narratives freely when watching a movie.',\n",
       "  'Experimental results on the dataset show that our proposed approach based on narratives significantly outperforms the baselines that simply use the narrative as a kind of context.'],\n",
       " ['Most of recent work in cross-lingual word embeddings is severely Anglocentric.',\n",
       "  'The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting.',\n",
       "  'With this work, however, we challenge these practices.',\n",
       "  'First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance.',\n",
       "  'Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages.',\n",
       "  'Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field.',\n",
       "  'Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.'],\n",
       " ['Intelligent features in email service applications aim to increase productivity by helping people organize their folders, compose their emails and respond to pending tasks.',\n",
       "  'In this work, we explore a new application, Smart-To-Do, that helps users with task management over emails.',\n",
       "  'We introduce a new task and dataset for automatically generating To-Do items from emails where the sender has promised to perform an action.',\n",
       "  'We design a two-stage process leveraging recent advances in neural text generation and sequence-to-sequence learning, obtaining BLEU and ROUGE scores of 0.23 and 0.63 for this task.',\n",
       "  'To the best of our knowledge, this is the first work to address the problem of composing To-Do items from emails.'],\n",
       " ['Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another.',\n",
       "  'However, the ability of NLI models to make pragmatic inferences remains understudied.',\n",
       "  'We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types.',\n",
       "  'We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences.',\n",
       "  'Although MultiNLI appears to contain very few pairs illustrating these inference types, we find that BERT learns to draw pragmatic inferences.',\n",
       "  'It reliably treats scalar implicatures triggered by “some” as entailments.',\n",
       "  'For some presupposition triggers like “only”, BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation.',\n",
       "  'BOW and InferSent show weaker evidence of pragmatic reasoning.',\n",
       "  'We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.'],\n",
       " ['Several recent studies have shown that strong natural language understanding (NLU) models are prone to relying on unwanted dataset biases without learning the underlying task, resulting in models that fail to generalize to out-of-domain datasets and are likely to perform poorly in real-world scenarios.',\n",
       "  'We propose two learning strategies to train neural models, which are more robust to such biases and transfer better to out-of-domain datasets.',\n",
       "  'The biases are specified in terms of one or more bias-only models, which learn to leverage the dataset biases.',\n",
       "  'During training, the bias-only models’ predictions are used to adjust the loss of the base model to reduce its reliance on biases by down-weighting the biased examples and focusing the training on the hard examples.',\n",
       "  'We experiment on large-scale natural language inference and fact verification benchmarks, evaluating on out-of-domain datasets that are specifically designed to assess the robustness of models against known biases in the training data.',\n",
       "  'Results show that our debiasing methods greatly improve robustness in all settings and better transfer to other textual entailment datasets.',\n",
       "  'Our code and data are publicly available in https://github.com/rabeehk/robust-nli.'],\n",
       " ['Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution.',\n",
       "  'Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance.',\n",
       "  'However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity.',\n",
       "  'This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the out-of-distribution data.',\n",
       "  'In this paper, we address this trade-off by introducing a novel debiasing method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples.',\n",
       "  'We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy.'],\n",
       " ['The recent growth in the popularity and success of deep learning models on NLP classification tasks has accompanied the need for generating some form of natural language explanation of the predicted labels.',\n",
       "  'Such generated natural language (NL) explanations are expected to be faithful, i.e., they should correlate well with the model’s internal decision making.',\n",
       "  'In this work, we focus on the task of natural language inference (NLI) and address the following question: can we build NLI systems which produce labels with high accuracy, while also generating faithful explanations of its decisions? We propose Natural-language Inference over Label-specific Explanations (NILE), a novel NLI method which utilizes auto-generated label-specific NL explanations to produce labels along with its faithful explanation.',\n",
       "  'We demonstrate NILE’s effectiveness over previously reported methods through automated and human evaluation of the produced labels and explanations.',\n",
       "  'Our evaluation of NILE also supports the claim that accurate systems capable of providing testable explanations of their decisions can be designed.',\n",
       "  'We discuss the faithfulness of NILE’s explanations in terms of sensitivity of the decisions to the corresponding explanations.',\n",
       "  'We argue that explicit evaluation of faithfulness, in addition to label and explanation accuracy, is an important step in evaluating model’s explanations.',\n",
       "  'Further, we demonstrate that task-specific probes are necessary to establish such sensitivity.'],\n",
       " ['Question-answering (QA) data often encodes essential information in many facets.',\n",
       "  'This paper studies a natural question: Can we get supervision from QA data for other tasks (typically, non-QA ones)? For example, can we use QAMR (Michael et al., 2017) to improve named entity recognition? We suggest that simply further pre-training BERT is often not the best option, and propose the question-answer driven sentence encoding (QuASE) framework.',\n",
       "  'QuASE learns representations from QA data, using BERT or other state-of-the-art contextual language models.',\n",
       "  'In particular, we observe the need to distinguish between two types of sentence encodings, depending on whether the target task is a single- or multi-sentence input; in both cases, the resulting encoding is shown to be an easy-to-use plugin for many downstream tasks.',\n",
       "  'This work may point out an alternative way to supervise NLP tasks.'],\n",
       " ['While deep learning models are making fast progress on the task of Natural Language Inference, recent studies have also shown that these models achieve high accuracy by exploiting several dataset biases, and without deep understanding of the language semantics.',\n",
       "  'Using contradiction-word bias and word-overlapping bias as our two bias examples, this paper explores both data-level and model-level debiasing methods to robustify models against lexical dataset biases.',\n",
       "  'First, we debias the dataset through data augmentation and enhancement, but show that the model bias cannot be fully removed via this method.',\n",
       "  'Next, we also compare two ways of directly debiasing the model without knowing what the dataset biases are in advance.',\n",
       "  'The first approach aims to remove the label bias at the embedding level.',\n",
       "  'The second approach employs a bag-of-words sub-model to capture the features that are likely to exploit the bias and prevents the original model from learning these biased features by forcing orthogonality between these two sub-models.',\n",
       "  'We performed evaluations on new balanced datasets extracted from the original MNLI dataset as well as the NLI stress tests, and show that the orthogonality approach is better at debiasing the model while maintaining competitive overall accuracy.'],\n",
       " ['We introduce Uncertain Natural Language Inference (UNLI), a refinement of Natural Language Inference (NLI) that shifts away from categorical labels, targeting instead the direct prediction of subjective probability assessments.',\n",
       "  'We demonstrate the feasibility of collecting annotations for UNLI by relabeling a portion of the SNLI dataset under a probabilistic scale, where items even with the same categorical label differ in how likely people judge them to be true given a premise.',\n",
       "  'We describe a direct scalar regression modeling approach, and find that existing categorically-labeled NLI data can be used in pre-training.',\n",
       "  'Our best models correlate well with humans, demonstrating models are capable of more subtle inferences than the categorical bin assignment employed in current NLI tasks.'],\n",
       " ['An interesting and frequent type of multi-word expression (MWE) is the headless MWE, for which there are no true internal syntactic dominance relations; examples include many named entities (“Wells Fargo”) and dates (“July 5, 2020”) as well as certain productive constructions (“blow for blow”, “day after day”).',\n",
       "  'Despite their special status and prevalence, current dependency-annotation schemes require treating such flat structures as if they had internal syntactic heads, and most current parsers handle them in the same fashion as headed constructions.',\n",
       "  'Meanwhile, outside the context of parsing, taggers are typically used for identifying MWEs, but taggers might benefit from structural information.',\n",
       "  'We empirically compare these two common strategies—parsing and tagging—for predicting flat MWEs.',\n",
       "  'Additionally, we propose an efficient joint decoding algorithm that combines scores from both strategies.',\n",
       "  'Experimental results on the MWE-Aware English Dependency Corpus and on six non-English dependency treebanks with frequent flat structures show that: (1) tagging is more accurate than parsing for identifying flat-structure MWEs, (2) our joint decoder reconciles the two different views and, for non-BERT features, leads to higher accuracies, and (3) most of the gains result from feature sharing between the parsers and taggers.'],\n",
       " ['Neural encoders have allowed dependency parsers to shift from higher-order structured models to simpler first-order ones, making decoding faster and still achieving better accuracy than non-neural parsers.',\n",
       "  'This has led to a belief that neural encoders can implicitly encode structural constraints, such as siblings and grandparents in a tree.',\n",
       "  'We tested this hypothesis and found that neural parsers may benefit from higher-order features, even when employing a powerful pre-trained encoder, such as BERT.',\n",
       "  'While the gains of higher-order features are small in the presence of a powerful encoder, they are consistent for long-range dependencies and long sentences.',\n",
       "  'In particular, higher-order models are more accurate on full sentence parses and on the exact match of modifier lists, indicating that they deal better with larger, more complex structures.'],\n",
       " ['Virtual adversarial training (VAT) is a powerful technique to improve model robustness in both supervised and semi-supervised settings.',\n",
       "  'It is effective and can be easily adopted on lots of image classification and text classification tasks.',\n",
       "  'However, its benefits to sequence labeling tasks such as named entity recognition (NER) have not been shown as significant, mostly, because the previous approach can not combine VAT with the conditional random field (CRF).',\n",
       "  'CRF can significantly boost accuracy for sequence models by putting constraints on label transitions, which makes it an essential component in most state-of-the-art sequence labeling model architectures.',\n",
       "  'In this paper, we propose SeqVAT, a method which naturally applies VAT to sequence labeling models with CRF.',\n",
       "  'Empirical studies show that SeqVAT not only significantly improves the sequence labeling performance over baselines under supervised settings, but also outperforms state-of-the-art approaches under semi-supervised settings.'],\n",
       " ['A recent advance in monolingual dependency parsing is the idea of a treebank embedding vector, which allows all treebanks for a particular language to be used as training data while at the same time allowing the model to prefer training data from one treebank over others and to select the preferred treebank at test time.',\n",
       "  'We build on this idea by 1) introducing a method to predict a treebank vector for sentences that do not come from a treebank used in training, and 2) exploring what happens when we move away from predefined treebank embedding vectors during test time and instead devise tailored interpolations.',\n",
       "  'We show that 1) there are interpolated vectors that are superior to the predefined ones, and 2) treebank vectors can be predicted with sufficient accuracy, for nine out of ten test languages, to match the performance of an oracle approach that knows the most suitable predefined treebank embedding for the test set.'],\n",
       " ['This paper proposes the building of Xiaomingbot, an intelligent, multilingual and multimodal software robot equipped with four inte- gral capabilities: news generation, news translation, news reading and avatar animation.',\n",
       "  'Its system summarizes Chinese news that it automatically generates from data tables.',\n",
       "  'Next, it translates the summary or the full article into multiple languages, and reads the multi- lingual rendition through synthesized speech.',\n",
       "  'Notably, Xiaomingbot utilizes a voice cloning technology to synthesize the speech trained from a real person’s voice data in one input language.',\n",
       "  'The proposed system enjoys several merits: it has an animated avatar, and is able to generate and read multilingual news.',\n",
       "  'Since it was put into practice, Xiaomingbot has written over 600,000 articles, and gained over 150,000 followers on social media platforms.'],\n",
       " ['In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for natural language processing.',\n",
       "  'It works with different neural network models and supports various kinds of supervised learning tasks, such as text classification, reading comprehension, sequence labeling.',\n",
       "  'TextBrewer provides a simple and uniform workflow that enables quick setting up of distillation experiments with highly flexible configurations.',\n",
       "  'It offers a set of predefined distillation methods and can be extended with custom code.',\n",
       "  'As a case study, we use TextBrewer to distill BERT on several typical NLP tasks.',\n",
       "  'With simple configurations, we achieve results that are comparable with or even higher than the public distilled BERT models with similar numbers of parameters.'],\n",
       " ['We present a system that allows a user to search a large linguistically annotated corpus using syntactic patterns over dependency graphs.',\n",
       "  'In contrast to previous attempts to this effect, we introduce a light-weight query language that does not require the user to know the details of the underlying syntactic representations, and instead to query the corpus by providing an example sentence coupled with simple markup.',\n",
       "  'Search is performed at an interactive speed due to efficient linguistic graph-indexing and retrieval engine.',\n",
       "  'This allows for rapid exploration, development and refinement of syntax-based queries.',\n",
       "  'We demonstrate the system using queries over two corpora: the English wikipedia, and a collection of English pubmed abstracts.',\n",
       "  'A demo of the wikipedia system is available at https://allenai.github.io/spike/ .'],\n",
       " ['We present Tabouid, a word-guessing game automatically generated from Wikipedia.',\n",
       "  'Tabouid contains 10,000 (virtual) cards in English, and as many in French, covering not only words and linguistic expressions but also a variety of topics including artists, historical events or scientific concepts.',\n",
       "  'Each card corresponds to a Wikipedia article, and conversely, any article could be turned into a card.',\n",
       "  'A range of relatively simple NLP and machine-learning techniques are effectively integrated into a two-stage process.',\n",
       "  'First, a large subset of Wikipedia articles are scored - this score estimates the difficulty, or alternatively, the playability of the page.',\n",
       "  'Then, the best articles are turned into cards by selecting, for each of them, a list of banned words based on its content.',\n",
       "  'We believe that the game we present is more than mere entertainment and that, furthermore, this paper has pedagogical potential.'],\n",
       " ['We introduce Talk to Papers, which exploits the recent open-domain question answering (QA) techniques to improve the current experience of academic search.',\n",
       "  'It’s designed to enable researchers to use natural language queries to find precise answers and extract insights from a massive amount of academic papers.',\n",
       "  'We present a large improvement over classic search engine baseline on several standard QA datasets and provide the community a collaborative data collection tool to curate the first natural language processing research QA dataset via a community effort.'],\n",
       " ['Exploiting syntagmatic information is an encouraging research focus to be pursued in an effort to close the gap between knowledge-based and supervised Word Sense Disambiguation (WSD) performance.',\n",
       "  'We follow this direction in our next-generation knowledge-based WSD system, SyntagRank, which we make available via a Web interface and a RESTful API.',\n",
       "  'SyntagRank leverages the disambiguated pairs of co-occurring words included in SyntagNet, a lexical-semantic combination resource, to perform state-of-the-art knowledge-based WSD in a multilingual setting.',\n",
       "  'Our service provides both a user-friendly interface, available at http://syntagnet.org/, and a RESTful endpoint to query the system programmatically (accessible at http://api.syntagnet.org/).'],\n",
       " ['Syntactic dependencies can be predicted with high accuracy, and are useful for both machine-learned and pattern-based information extraction tasks.',\n",
       "  'However, their utility can be improved.',\n",
       "  'These syntactic dependencies are designed to accurately reflect syntactic relations, and they do not make semantic relations explicit.',\n",
       "  'Therefore, these representations lack many explicit connections between content words, that would be useful for downstream applications.',\n",
       "  'Proposals like English Enhanced UD improve the situation by extending universal dependency trees with additional explicit arcs.',\n",
       "  'However, they are not available to Python users, and are also limited in coverage.',\n",
       "  'We introduce a broad-coverage, data-driven and linguistically sound set of transformations, that makes event-structure and many lexical relations explicit.',\n",
       "  'We present pyBART, an easy-to-use open-source Python library for converting English UD trees either to Enhanced UD graphs or to our representation.',\n",
       "  'The library can work as a standalone package or be integrated within a spaCy NLP pipeline.',\n",
       "  'When evaluated in a pattern-based relation extraction scenario, our representation results in higher extraction scores than Enhanced UD, while requiring fewer patterns.'],\n",
       " ['Traditional search engines for life sciences (e.g., PubMed) are designed for document retrieval and do not allow direct retrieval of specific statements.',\n",
       "  'Some of these statements may serve as textual evidence that is key to tasks such as hypothesis generation and new finding validation.',\n",
       "  'We present EVIDENCEMINER, a web-based system that lets users query a natural language statement and automatically retrieves textual evidence from a background corpora for life sciences.',\n",
       "  'EVIDENCEMINER is constructed in a completely automated way without any human effort for training data annotation.',\n",
       "  'It is supported by novel data-driven methods for distantly supervised named entity recognition and open information extraction.',\n",
       "  'The entities and patterns are pre-computed and indexed offline to support fast online evidence retrieval.',\n",
       "  'The annotation results are also highlighted in the original document for better visualization.',\n",
       "  'EVIDENCEMINER also includes analytic functionalities such as the most frequent entity and relation summarization.',\n",
       "  'EVIDENCEMINER can help scientists uncover important research issues, leading to more effective research and more in-depth quantitative analysis.',\n",
       "  'The system of EVIDENCEMINER is available at https://evidenceminer.firebaseapp.com/.'],\n",
       " ['We introduce Trialstreamer, a living database of clinical trial reports.',\n",
       "  'Here we mainly describe the evidence extraction component; this extracts from biomedical abstracts key pieces of information that clinicians need when appraising the literature, and also the relations between these.',\n",
       "  'Specifically, the system extracts descriptions of trial participants, the treatments compared in each arm (the interventions), and which outcomes were measured.',\n",
       "  'The system then attempts to infer which interventions were reported to work best by determining their relationship with identified trial outcome measures.',\n",
       "  'In addition to summarizing individual trials, these extracted data elements allow automatic synthesis of results across many trials on the same topic.',\n",
       "  'We apply the system at scale to all reports of randomized controlled trials indexed in MEDLINE, powering the automatic generation of evidence maps, which provide a global view of the efficacy of different interventions combining data from all relevant clinical trials on a topic.',\n",
       "  'We make all code and models freely available alongside a demonstration of the web interface.'],\n",
       " ['Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models.',\n",
       "  'However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models.',\n",
       "  'We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design.',\n",
       "  'This paper releases two tools of independent value for the computational linguistics community: 1.',\n",
       "  'A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2.',\n",
       "  'Two command-line tools, ‘syntaxgym‘ and ‘lm-zoo‘, which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine.'],\n",
       " ['We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology.',\n",
       "  'Our system, GAIA, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos.',\n",
       "  'GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation.',\n",
       "  'The system is publicly available at GitHub and DockerHub, with a narrated video that documents the system.'],\n",
       " ['We present easy-to-use retrieval focused multilingual sentence embedding models, made available on TensorFlow Hub.',\n",
       "  'The models embed text from 16 languages into a shared semantic space using a multi-task trained dual-encoder that learns tied cross-lingual representations via translation bridge tasks (Chidambaram et al., 2018).',\n",
       "  'The models achieve a new state-of-the-art in performance on monolingual and cross-lingual semantic retrieval (SR).',\n",
       "  'Competitive performance is obtained on the related tasks of translation pair bitext retrieval (BR) and retrieval question answering (ReQA).',\n",
       "  'On transfer learning tasks, our multilingual embeddings approach, and in some cases exceed, the performance of English only sentence embeddings.'],\n",
       " ['CodaLab is an open-source web-based platform for collaborative computational research.',\n",
       "  'Although CodaLab has gained popularity in the research community, its interface has limited support for creating reusable tools that can be easily applied to new datasets and composed into pipelines.',\n",
       "  'In clinical domain, natural language processing (NLP) on medical notes generally involves multiple steps, like tokenization, named entity recognition, etc.',\n",
       "  'Since these steps require different tools which are usually scattered in different publications, it is not easy for researchers to use them to process their own datasets.',\n",
       "  'In this paper, we present BENTO, a workflow management platform with a graphic user interface (GUI) that is built on top of CodaLab, to facilitate the process of building clinical NLP pipelines.',\n",
       "  'BENTO comes with a number of clinical NLP tools that have been pre-trained using medical notes and expert annotations and can be readily used for various clinical NLP tasks.',\n",
       "  'It also allows researchers and developers to create their custom tools (e.g., pre-trained NLP models) and use them in a controlled and reproducible way.',\n",
       "  'In addition, the GUI interface enables researchers with limited computer background to compose tools into NLP pipelines and then apply the pipelines on their own datasets in a “what you see is what you get” (WYSIWYG) way.',\n",
       "  'Although BENTO is designed for clinical NLP applications, the underlying architecture is flexible to be tailored to any other domains.'],\n",
       " ['We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages.',\n",
       "  'Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition.',\n",
       "  'We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested.',\n",
       "  'Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction.',\n",
       "  'Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.'],\n",
       " ['We introduce jiant, an open source toolkit for conducting multitask and transfer learning experiments on English NLU tasks.',\n",
       "  'jiant enables modular and configuration driven experimentation with state-of-the-art models and a broad set of tasks for probing, transfer learning, and multitask training experiments.',\n",
       "  'jiant implements over 50 NLU tasks, including all GLUE and SuperGLUE benchmark tasks.',\n",
       "  'We demonstrate that jiant reproduces published performance on a variety of tasks and models, e.g., RoBERTa and BERT.'],\n",
       " ['We present MT-DNN, an open-source natural language understanding (NLU) toolkit that makes it easy for researchers and developers to train customized deep learning models.',\n",
       "  'Built upon PyTorch and Transformers, MT-DNN is designed to facilitate rapid customization for a broad spectrum of NLU tasks, using a variety of objectives (classification, regression, structured prediction) and text encoders (e.g., RNNs, BERT, RoBERTa, UniLM).',\n",
       "  'A unique feature of MT-DNN is its built-in support for robust and transferable learning using the adversarial multi-task learning paradigm.',\n",
       "  'To enable efficient production deployment, MT-DNN supports multi-task knowledge distillation, which can substantially compress a deep neural model without significant performance drop.',\n",
       "  'We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains.',\n",
       "  'The software and pre-trained models will be publicly available at https://github.com/namisan/mt-dnn.'],\n",
       " ['This paper presents LinggleWrite, a writing coach that provides writing suggestions, assesses writing proficiency levels, detects grammatical errors, and offers corrective feedback in response to user’s essay.',\n",
       "  'The method involves extracting grammar patterns, training models for automated essay scoring (AES) and grammatical error detection (GED), and finally retrieving plausible corrections from a n-gram search engine.',\n",
       "  'Experiments on public test sets indicate that both AES and GED models achieve state-of-the-art performance.',\n",
       "  'These results show that LinggleWrite is potentially useful in helping learners improve their writing skills.'],\n",
       " ['We present CLIReval, an easy-to-use toolkit for evaluating machine translation (MT) with the proxy task of cross-lingual information retrieval (CLIR).',\n",
       "  'Contrary to what the project name might suggest, CLIReval does not actually require any annotated CLIR dataset.',\n",
       "  'Instead, it automatically transforms translations and references used in MT evaluations into a synthetic CLIR dataset; it then sets up a standard search engine (Elasticsearch) and computes various information retrieval metrics (e.g., mean average precision) by treating the translations as documents to be retrieved.',\n",
       "  'The idea is to gauge the quality of MT by its impact on the document translation approach to CLIR.',\n",
       "  'As a case study, we run CLIReval on the “metrics shared task” of WMT2019; while this extrinsic metric is not intended to replace popular intrinsic metrics such as BLEU, results suggest CLIReval is competitive in many language pairs in terms of correlation to human judgments of quality.',\n",
       "  'CLIReval is publicly available at https://github.com/ssun32/CLIReval.'],\n",
       " ['We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems.',\n",
       "  'As the successor of ConvLab, ConvLab-2 inherits ConvLab’s framework but integrates more powerful dialogue models and supports more datasets.',\n",
       "  'Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems.',\n",
       "  'The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement.',\n",
       "  'The interactive tool provides an user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component.'],\n",
       " ['This paper introduces OpusFilter, a flexible and modular toolbox for filtering parallel corpora.',\n",
       "  'It implements a number of components based on heuristic filters, language identification libraries, character-based language models, and word alignment tools, and it can easily be extended with custom filters.',\n",
       "  'Bitext segments can be ranked according to their quality or domain match using single features or a logistic regression model that can be trained without manually labeled training data.',\n",
       "  'We demonstrate the effectiveness of OpusFilter on the example of a Finnish-English news translation task based on noisy web-crawled training data.',\n",
       "  'Applying our tool leads to improved translation quality while significantly reducing the size of the training data, also clearly outperforming an alternative ranking given in the crawled data set.',\n",
       "  'Furthermore, we show the ability of OpusFilter to perform data selection for domain adaptation.'],\n",
       " ['Label noise—incorrectly or ambiguously labeled training examples—can negatively impact model performance.',\n",
       "  'Although noise detection techniques have been around for decades, practitioners rarely apply them, as manual noise remediation is a tedious process.',\n",
       "  'Examples incorrectly flagged as noise waste reviewers’ time, and correcting label noise without guidance can be difficult.',\n",
       "  'We propose LNIC, a noise-detection method that uses an example’s neighborhood within the training set to (a) reduce false positives and (b) provide an explanation as to why the ex- ample was flagged as noise.',\n",
       "  'We demonstrate on several short-text classification datasets that LNIC outperforms the state of the art on measures of precision and F0.5-score.',\n",
       "  'We also show how LNIC’s training set context helps a reviewer to understand and correct label noise in a dataset.',\n",
       "  'The LNIC tool lowers the barriers to label noise remediation, increasing its utility for NLP practitioners.'],\n",
       " ['Large Transformer-based language models can route and reshape complex information via their multi-headed attention mechanism.',\n",
       "  'Although the attention never receives explicit supervision, it can exhibit recognizable patterns following linguistic or positional information.',\n",
       "  'Analyzing the learned representations and attentions is paramount to furthering our understanding of the inner workings of these models.',\n",
       "  'However, analyses have to catch up with the rapid release of new models and the growing diversity of investigation techniques.',\n",
       "  'To support analysis for a wide variety of models, we introduce exBERT, a tool to help humans conduct flexible, interactive investigations and formulate hypotheses for the model-internal reasoning process.',\n",
       "  'exBERT provides insights into the meaning of the contextual representations and attention by matching a human-specified input to similar contexts in large annotated datasets.',\n",
       "  'By aggregating the annotations of the matched contexts, exBERT can quickly replicate findings from literature and extend them to previously not analyzed models.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ls in enumerate(list):\n",
    "    for l in range(len(ls)):\n",
    "        tt_s.append([i+381,ls[l],l])\n",
    "tt.save('C:/Users/a0102/Desktop/tt.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

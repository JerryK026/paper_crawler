{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=Workbook()\n",
    "tt_s=tt.active\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>381</td>\n",
       "      <td>Given the fast development of analysis techniq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>382</td>\n",
       "      <td>To increase trust in artificial intelligence s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>383</td>\n",
       "      <td>By introducing a small set of additional param...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>384</td>\n",
       "      <td>Language models keep track of complex informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>385</td>\n",
       "      <td>In the Transformer model, “self-attention” com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>386</td>\n",
       "      <td>With the growing popularity of deep-learning b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     a                                                  b\n",
       "0  381  Given the fast development of analysis techniq...\n",
       "1  382  To increase trust in artificial intelligence s...\n",
       "2  383  By introducing a small set of additional param...\n",
       "3  384  Language models keep track of complex informat...\n",
       "4  385  In the Transformer model, “self-attention” com...\n",
       "5  386  With the growing popularity of deep-learning b..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_excel('t.xlsx')\n",
    "data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 420 entries, 0 to 419\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   a       420 non-null    int64 \n",
      " 1   b       420 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 6.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[]\n",
    "for i in range(len(data['b'])):\n",
    "    list.append(data['b'][i].split('. ')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method', 'As a step in this direction we study the case of representations of phonology in neural network models of spoken language', 'We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences', 'We manipulate two factors that can affect the outcome of analysis', 'First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models', 'Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance', 'We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.']\n",
      "['To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions', 'In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations', 'We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations', 'Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks', 'Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions', 'Our framework shows that this model is capable of generating a significant number of inconsistent explanations.']\n",
      "['By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings)', 'The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge', 'However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself', 'Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT)', 'Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process', 'Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines', 'We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ls in list:\n",
    "    for i in range(len(ls)-1):\n",
    "        ls[i]=ls[i]+'.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method.', 'As a step in this direction we study the case of representations of phonology in neural network models of spoken language.', 'We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences.', 'We manipulate two factors that can affect the outcome of analysis.', 'First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models.', 'Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance.', 'We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.']\n",
      "['To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions.', 'In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations.', 'We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations.', 'Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks.', 'Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions.', 'Our framework shows that this model is capable of generating a significant number of inconsistent explanations.']\n",
      "['By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings).', 'The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge.', 'However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself.', 'Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT).', 'Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process.', 'Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines.', 'We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the fast development of analysis techniques for NLP and speech processing systems, few systematic studies have been conducted to compare the strengths and weaknesses of each method.\n",
      "As a step in this direction we study the case of representations of phonology in neural network models of spoken language.\n",
      "We use two commonly applied analytical techniques, diagnostic classifiers and representational similarity analysis, to quantify to what extent neural activation patterns encode phonemes and phoneme sequences.\n",
      "We manipulate two factors that can affect the outcome of analysis.\n",
      "First, we investigate the role of learning by comparing neural activations extracted from trained versus randomly-initialized models.\n",
      "Second, we examine the temporal scope of the activations by probing both local activations corresponding to a few milliseconds of the speech signal, and global activations pooled over the whole utterance.\n",
      "We conclude that reporting analysis results with randomly initialized models is crucial, and that global-scope methods tend to yield more consistent and interpretable results and we recommend their use as a complement to local-scope diagnostic methods.\n"
     ]
    }
   ],
   "source": [
    "for i in list[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,ls in enumerate(list):\n",
    "    for l in range(len(ls)):\n",
    "        tt_s.append([i+381,ls[l],l])\n",
    "tt.save('C:/Users/a0102/Desktop/tt.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
